{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dcf06fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Structure\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dba165b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'example.txt', 'author': 'Renju Rajagopalan', 'page_length': 1}, page_content='This is the page content I am adding to the document')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = Document(\n",
    "    page_content = \"This is the page content I am adding to the document\",\n",
    "    metadata = {\n",
    "        \"source\":\"example.txt\",\n",
    "        \"author\":\"Renju Rajagopalan\",\n",
    "        \"page_length\":1\n",
    "    }\n",
    ")\n",
    "\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "748913df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"../data/sample_text_files\", exist_ok=True)\n",
    "\n",
    "sample_texts = {\n",
    " \"../data/sample_text_files/python_intro.txt\":\n",
    " \"Python is a popular programming language. It was created by Guido van Rossum, and released in 1991.Python was designed for readability, and has some similarities to the English language with influence from mathematics.Python uses new lines to complete a command, as opposed to other programming languages which often use semicolons or parentheses.ython relies on indentation, using whitespace, to define scope; such as the scope of loops, functions and classes. Other programming languages often use curly-brackets for this purpose.\" ,\n",
    " \n",
    "\"../data/sample_text_files/deep_learning.txt\":\n",
    "\"Deep Learning is transforming the way machines understand, learn and interact with complex data. Deep learning mimics neural networks of the human brain, it enables computers to autonomously uncover patterns and make informed decisions from vast amounts of unstructured data\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f69e5896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text files created\n"
     ]
    }
   ],
   "source": [
    "for file_path,content in sample_texts.items():\n",
    "    with open(file_path,'w',encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"Sample text files created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29858f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\renju\\Documents\\Renju\\AI ML\\Projects\\UV Projects\\RAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/sample_text_files/python_intro.txt'}, page_content='Python is a popular programming language. It was created by Guido van Rossum, and released in 1991.Python was designed for readability, and has some similarities to the English language with influence from mathematics.Python uses new lines to complete a command, as opposed to other programming languages which often use semicolons or parentheses.ython relies on indentation, using whitespace, to define scope; such as the scope of loops, functions and classes. Other programming languages often use curly-brackets for this purpose.')]\n"
     ]
    }
   ],
   "source": [
    "### TextLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"../data/sample_text_files/python_intro.txt\", encoding = \"utf-8\")\n",
    "doc = loader.load()\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa1b83d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Directory Loader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "dir_loader = DirectoryLoader(\n",
    "    \"../data/sample_text_files\",\n",
    "    glob=\"**/*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={\"encoding\":\"utf-8\"},\n",
    "    show_progress=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a45113c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\data\\\\sample_text_files\\\\deep_learning.txt'}, page_content='Deep Learning is transforming the way machines understand, learn and interact with complex data. Deep learning mimics neural networks of the human brain, it enables computers to autonomously uncover patterns and make informed decisions from vast amounts of unstructured data'),\n",
       " Document(metadata={'source': '..\\\\data\\\\sample_text_files\\\\python_intro.txt'}, page_content='Python is a popular programming language. It was created by Guido van Rossum, and released in 1991.Python was designed for readability, and has some similarities to the English language with influence from mathematics.Python uses new lines to complete a command, as opposed to other programming languages which often use semicolons or parentheses.ython relies on indentation, using whitespace, to define scope; such as the scope of loops, functions and classes. Other programming languages often use curly-brackets for this purpose.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8ed14c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "## load all the PDF files from the directory\n",
    "pdf_loader = DirectoryLoader(\n",
    "    \"../data/PDFs\",\n",
    "    glob = '**/*.pdf',\n",
    "    loader_cls = PyMuPDFLoader,\n",
    "    show_progress = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e648476f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 0}, page_content='MIT Class 6.S184: Generative AI With Stochastic Differential Equations, 2025\\nAn Introduction to Flow Matching and Diffusion Models\\nPeter Holderrieth and Ezra Erives\\nWebsite: https://diffusion.csail.mit.edu/\\n1\\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n2\\n1.1\\nOverview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n2\\n1.2\\nCourse Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n3\\n1.3\\nGenerative Modeling As Sampling\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n3\\n2\\nFlow and Diffusion Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6\\n2.1\\nFlow Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6\\n2.2\\nDiffusion Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n9\\n3\\nConstructing the Training Target . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n13\\n3.1\\nConditional and Marginal Probability Path . . . . . . . . . . . . . . . . . . . . . . . . . .\\n14\\n3.2\\nConditional and Marginal Vector Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n15\\n3.3\\nConditional and Marginal Score Functions . . . . . . . . . . . . . . . . . . . . . . . . . .\\n18\\n4\\nTraining the Generative Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n24\\n4.1\\nFlow Matching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n24\\n4.2\\nScore Matching\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n28\\n4.3\\nA Guide to the Diffusion Model Literature . . . . . . . . . . . . . . . . . . . . . . . . . .\\n32\\n5\\nBuilding an Image Generator\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n36\\n5.1\\nGuidance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n36\\n5.2\\nNeural network architectures\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n43\\n5.3\\nA Survey of Large-Scale Image and Video Models . . . . . . . . . . . . . . . . . . . . . .\\n46\\n6\\nAcknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n49\\n7\\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n49\\nA\\nA Reminder on Probability Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n52\\nA.1\\nRandom vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n52\\nA.2\\nConditional densities and expectations . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n52\\nB\\nA Proof of the Fokker-Planck equation\\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\\n54\\n1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 1}, page_content='1\\nIntroduction\\nCreating noise from data is easy; creating data\\nfrom noise is generative modeling.\\nSong et al. [30]\\n1.1\\nOverview\\nIn recent years, we all have witnessed a tremendous revolution in artificial intelligence (AI). Image generators like\\nStable Diffusion 3 can generate photorealistic and artistic images across a diverse range of styles, video models\\nlike Meta’s Movie Gen Video can generate highly realistic movie clips, and large language models like ChatGPT\\ncan generate seemingly human-level responses to text prompts. At the heart of this revolution lies a new ability\\nof AI systems: the ability to generate objects. While previous generations of AI systems were mainly used for\\nprediction, these new AI system are creative: they dream or come up with new objects based on user-specified\\ninput. Such generative AI systems are at the core of this recent AI revolution.\\nThe goal of this class is to teach you two of the most widely used generative AI algorithms: denoising diffusion\\nmodels [32] and flow matching [14, 16, 1, 15]. These models are the backbone of the best image, audio, and video\\ngeneration models (e.g., Stable Diffusion 3 and Movie Gen Video), and have most recently became the state-of-the-\\nart in scientific applications such as protein structures (e.g., AlphaFold3 is a diffusion model). Without a doubt,\\nunderstanding these models is truly an extremely useful skill to have.\\nAll of these generative models generate objects by iteratively converting noise into data. This evolution from\\nnoise to data is facilitated by the simulation of ordinary or stochastic differential equations (ODEs/SDEs). Flow\\nmatching and denoising diffusion models are a family of techniques that allow us to construct, train, and simulate,\\nsuch ODEs/SDEs at large scale with deep neural networks. While these models are rather simple to implement,\\nthe technical nature of SDEs can make these models difficult to understand. In this course, our goal is to provide a\\nself-contained introduction to the necessary mathematical toolbox regarding differential equations to enable you to\\nsystematically understand these models. Beyond being widely applicable, we believe that the theory behind flow\\nand diffusion models is elegant in its own right. Therefore, most importantly, we hope that this course will be a\\nlot of fun to you.\\nRemark 1 (Additional Resources)\\nWhile these lecture notes are self-contained, there are two additional resources that we encourage you to use:\\n1. Lecture recordings: These guide you through each section in a lecture format.\\n2. Labs: These guide you in implementing your own diffusion model from scratch. We highly recommend\\nthat you “get your hands dirty” and code.\\nYou can find these on our course website: https://diffusion.csail.mit.edu/.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 2}, page_content='1.2\\nCourse Structure\\n1.2\\nCourse Structure\\nWe give a brief overview over of this document.\\n• Section 1, Generative Modeling as Sampling: We formalize what it means to “generate” an image, video,\\nprotein, etc. We will translate the problem of e.g., “how to generate an image of a dog?” into the more precise\\nproblem of sampling from a probability distribution.\\n• Section 2, Flow and Diffusion Models: Next, we explain the machinery of generation. As you can guess by\\nthe name of this class, this machinery consists of simulating ordinary and stochastic differential equations.\\nWe provide an introduction to differential equations and explain how to construct them with neural networks.\\n• Section 3, Constructing a Training Target: To train our generative model, we must first pin down precisely\\nwhat it is that our model is supposed to approximate. In other words, what’s the ground truth? We will\\nintroduce the celebrated Fokker-Planck equation, which will allow us to formalize the notion of ground truth.\\n• Section 4, Training: This section formulates a training objective, allowing us to approximate the training\\ntarget, or ground truth, of the previous section. With this, we are ready to provide a minimal implementation\\nof flow matching and denoising diffusion models.\\n• Section 5, Conditional Image Generation: We learn how to build a conditional image generator. To do so,\\nwe formulate how to condition our samples on a prompt (e.g. “an image of a cat”). We then discuss common\\nneural network architectures and survey state-of-the-art models for both image and video generation.\\nRequired background.\\nDue to the technical nature of this subject, we recommend some base level of mathematical\\nmaturity, and in particular some familiarity with probability theory. For this reason, we included a brief reminder\\nsection on probability theory in appendix A. Don’t worry if some of the concepts there are unfamiliar to you.\\n1.3\\nGenerative Modeling As Sampling\\nLet’s begin by thinking about various data types, or modalities, that we might encounter, and how we will go\\nabout representing them numerically:\\n1. Image: Consider images with H × W pixels where H describes the height and W the width of the image,\\neach with three color channels (RGB). For every pixel and every color channel, we are given an intensity value\\nin R. Therefore, an image can be represented by an element z ∈RH×W ×3.\\n2. Video: A video is simply a series of images in time. If we have T time points or frames, a video would\\ntherefore be represented by an element z ∈RT ×H×W ×3.\\n3. Molecular structure: A naive way would be to represent the structure of a molecule by a matrix\\nz = (z1, . . . , zN) ∈R3×N where N is the number of atoms in the molecule and each zi ∈R3 describes the\\nlocation of that atom. Of course, there are other, more sophisticated ways of representing such a molecule.\\nIn all of the above examples, the object that we want to generate can be mathematically represented as a vector\\n(potentially after flattening). Therefore, throughout this document, we will have:\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 3}, page_content='1.3\\nGenerative Modeling As Sampling\\nKey Idea 1 (Objects as Vectors)\\nWe identify the objects being generated as vectors z ∈Rd.\\nA notable exception to the above is text data, which is typically modeled as a discrete object via autoregressive\\nlanguage models (such as ChatGPT). While flow and diffusion models for discrete data have been developed, this\\ncourse focuses exclusively on applications to continuous data.\\nGeneration as Sampling.\\nLet us define what it means to “generate” something. For example, let’s say we want\\nto generate an image of a dog. Naturally, there are many possible images of dogs that we would be happy with. In\\nparticular, there is no one single “best” image of a dog. Rather, there is a spectrum of images that fit better or worse.\\nIn machine learning, it is common to think of this diversity of possible images as a probability distribution. We call\\nit the data distribution and denote it as pdata. In the example of dog images, this distribution would therefore\\ngive higher likelihood to images that look more like a dog. Therefore, how \"good\" an image/video/molecule fits -\\na rather subjective statement - is replaced by how \"likely\" it is under the data distribution pdata. With this, we\\ncan mathematically express the task of generation as sampling from the (unknown) distribution pdata:\\nKey Idea 2 (Generation as Sampling)\\nGenerating an object z is modeled as sampling from the data distribution z ∼pdata.\\nA generative model is a machine learning model that allows us to generate samples from pdata. In machine learning,\\nwe require data to train models. In generative modeling, we usually assume access to a finite number of examples\\nsampled independently from pdata, which together serve as a proxy for the true distribution.\\nKey Idea 3 (Dataset)\\nA dataset consists of a finite number of samples z1, . . . , zN ∼pdata.\\nFor images, we might construct a dataset by compiling publicly available images from the internet. For videos,\\nwe might similarly use YouTube as a database. For protein structures, we can use experimental data bases from\\nsources such as the Protein Data Bank (PDB) that collected scientific measurements over decades. As the size of\\nour dataset grows very large, it becomes an increasingly better representation of the underlying distribution pdata.\\nConditional Generation.\\nIn many cases, we want to generate an object conditioned on some data y. For example,\\nwe might want to generate an image conditioned on y =“a dog running down a hill covered with snow with mountains\\nin the background”. We can rephrase this as sampling from a conditional distribution:\\nKey Idea 4 (Conditional Generation)\\nConditional generation involves sampling from z ∼pdata(·|y), where y is a conditioning variable.\\nWe call pdata(·|y) the conditional data distribution. The conditional generative modeling task typically involves\\nlearning to condition on an arbitrary, rather than fixed, choice of y.\\nUsing our previous example, we might\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 4}, page_content='1.3\\nGenerative Modeling As Sampling\\nalternatively want to condition on a different text prompt, such as y =“a photorealistic image of a cat blowing out\\nbirthday candles”. We therefore seek a single model which may be conditioned on any such choice of y. It turns out\\nthat techniques for unconditional generation are readily generalized to the conditional case. Therefore, for the first\\n3 sections, we will focus almost exclusively on the unconditional case (keeping in mind that conditional generation\\nis what we’re building towards).\\nFrom Noise to Data.\\nSo far, we have discussed the what of generative modeling: generating samples from pdata.\\nHere, we will briefly discuss the how. For this, we assume that we have access to some initial distribution pinit\\nthat we can easily sample from, such as the Gaussian pinit = N(0, Id). The goal of generative modeling is then to\\ntransform samples from x ∼pinit into samples from pdata. We note that pinit does not have to be so simple as a\\nGaussian. As we shall see, there are interesting use cases for leveraging this flexibility. Despite this, in the majority\\nof applications we take it to be a simple Gaussian and it is important to keep that in mind.\\nSummary\\nWe summarize our discussion so far as follows.\\nSummary 2 (Generation as Sampling)\\nWe summarize the findings of this section:\\n1. In this class, we consider the task of generating objects that are represented as vectors z ∈Rd such as\\nimages, videos, or molecular structures.\\n2. Generation is the task of generating samples from a probability distribution pdata having access to a\\ndataset of samples z1, . . . , zN ∼pdata during training.\\n3. Conditional generation assumes that we condition the distribution on a label y and we want to sample\\nfrom pdata(·|y) having access to data set of pairs (z1, y) . . . , (zN, y) during training.\\n4. Our goal is to train a generative model to transform samples from a simple distribution pinit (e.g. a\\nGaussian) into samples from pdata.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 5}, page_content='2\\nFlow and Diffusion Models\\nIn the previous section, we formalized generative modeling as sampling from a data distribution pdata. Further,\\nwe saw that sampling could be achieved via the transformation of samples from a simple distribution pinit, such as\\nthe Gaussian N(0, Id), to samples from the target distribution pdata. In this section, we describe how the desired\\ntransformation can be obtained as the simulation of a suitably constructed differential equation. For example,\\nflow matching and diffusion models involve simulating ordinary differential equations (ODEs) and stochastic\\ndifferential equations (SDEs), respectively. The goal of this section is therefore to define and construct these\\ngenerative models as they will be used throughout the remainder of the notes. Specifically, we first define ODEs\\nand SDEs, and discuss their simulation. Second, we describe how to parameterize an ODE/SDE using a deep neural\\nnetwork. This leads to the definition of a flow and diffusion model and the fundamental algorithms to sample from\\nsuch models. In later sections, we then explore how to train these models.\\n2.1\\nFlow Models\\nWe start by defining ordinary differential equations (ODEs). A solution to an ODE is defined by a trajectory, i.e.\\na function of the form\\nX : [0, 1] →Rd,\\nt 7→Xt,\\nthat maps from time t to some location in space Rd. Every ODE is defined by a vector field u, i.e. a function of\\nthe form\\nu : Rd × [0, 1] →Rd,\\n(x, t) 7→ut(x),\\ni.e. for every time t and location x we get a vector ut(x) ∈Rd specifying a velocity in space (see fig. 1). An ODE\\nimposes a condition on a trajectory: we want a trajectory X that “follows along the lines” of the vector field ut,\\nstarting at the point x0. We may formalize such a trajectory as being the solution to the equation:\\nd\\ndtXt = ut(Xt)\\n▶ODE\\n(1a)\\nX0 = x0\\n▶initial conditions\\n(1b)\\nEquation (1a) requires that the derivative of Xt is specified by the direction given by ut. Equation (1b) requires\\nthat we start at x0 at time t = 0. We may now ask: if we start at X0 = x0 at t = 0, where are we at time t (what\\nis Xt)? This question is answered by a function called the flow, which is a solution to the ODE\\nψ : Rd × [0, 1] 7→Rd,\\n(x0, t) 7→ψt(x0)\\n(2a)\\nd\\ndtψt(x0) = ut(ψt(x0))\\n▶flow ODE\\n(2b)\\nψ0(x0) = x0\\n▶flow initial conditions\\n(2c)\\nFor a given initial condition X0 = x0, a trajectory of the ODE is recovered via Xt = ψt(X0). Therefore, vector\\nfields, ODEs, and flows are, intuitively, three descriptions of the same object: vector fields define ODEs whose\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 6}, page_content='2.1\\nFlow Models\\nFigure 1: A flow ψt : Rd →Rd (red square grid) is defined by a velocity field ut : Rd →Rd (visualized with blue\\narrows) that prescribes its instantaneous movements at all locations (here, d = 2). We show three different times\\nt. As one can see, a flow is a diffeomorphism that \"warps\" space. Figure from [15].\\nsolutions are flows. As with every equation, we should ask ourselves about an ODE: Does a solution exist and if\\nso, is it unique? A fundamental result in mathematics is \"yes!\" to both, as long we impose weak assumptions on\\nut:\\nTheorem 3 (Flow existence and uniqueness)\\nIf u : Rd×[0, 1] →Rd is continuously differentiable with a bounded derivative, then the ODE in (2) has a unique\\nsolution given by a flow ψt. In this case, ψt is a diffeomorphism for all t, i.e. ψt is continuously differentiable\\nwith a continuously differentiable inverse ψ−1\\nt\\n.\\nNote that the assumptions required for the existence and uniqueness of a flow are almost always fulfilled in machine\\nlearning, as we use neural networks to parameterize ut(x) and they always have bounded derivatives. Therefore,\\ntheorem 3 should not be a concern for you but rather good news: flows exist and are unique solutions to ODEs\\nin our cases of interest. A proof can be found in [20, 4].\\nExample 4 (Linear Vector Fields)\\nLet us consider a simple example of a vector field ut(x) that is a simple linear function in x, i.e. ut(x) = −θx\\nfor θ > 0. Then the function\\nψt(x0) = exp (−θt) x0\\n(3)\\ndefines a flow ψ solving the ODE in eq. (2). You can check this yourself by checking that ψ0(x0) = x0 and\\ncomputing\\nd\\ndtψt(x0)\\n(3)\\n= d\\ndt (exp (−θt) x0)\\n(i)\\n= −θ exp (−θt) x0\\n(3)\\n= −θψt(x0) = ut(ψt(x0)),\\nwhere in (i) we used the chain rule. In fig. 3, we visualize a flow of this form converging to 0 exponentially.\\nSimulating an ODE.\\nIn general, it is not possible to compute the flow ψt explicitly if ut is not as simple as a\\nlinear function. In these cases, one uses numerical methods to simulate ODEs. Fortunately, this is a classical and\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 7}, page_content='2.1\\nFlow Models\\nwell researched topic in numerical analysis, and a myriad of powerful methods exist [11]. One of the simplest and\\nmost intuitive methods is the Euler method. In the Euler method, we initialize with X0 = x0 and update via\\nXt+h = Xt + hut(Xt)\\n(t = 0, h, 2h, 3h, . . . , 1 −h)\\n(4)\\nwhere h = n−1 > 0 is a step size hyperparameter with n ∈N. For this class, the Euler method will be good enough.\\nTo give you a taste of a more complex method, let us consider Heun’s method defined via the update rule\\nX′\\nt+h = Xt + hut(Xt)\\n▶initial guess of new state\\nXt+h = Xt + h\\n2 (ut(Xt) + ut+h(X′\\nt+h))\\n▶update with average u at current and guessed state\\nIntuitively, the Heun’s method is as follows: it takes a first guess X′\\nt+h of what the next step could be but corrects\\nthe direction initially taken via an updated guess.\\nFlow models.\\nWe can now construct a generative model via an ODE. Remember that our goal was to convert a\\nsimple distribution pinit into a complex distribution pdata. The simulation of an ODE is thus a natural choice for\\nthis transformation. A flow model is described by the ODE\\nX0 ∼pinit\\n▶random initialization\\nd\\ndtXt = uθ\\nt (Xt)\\n▶ODE\\nwhere the vector field uθ\\nt is a neural network uθ\\nt with parameters θ. For now, we will speak of uθ\\nt as being a generic\\nneural network; i.e. a continuous function uθ\\nt : Rd ×[0, 1] →Rd with parameters θ. Later, we will discuss particular\\nchoices of neural network architectures. Our goal is to make the endpoint X1 of the trajectory have distribution\\npdata, i.e.\\nX1 ∼pdata\\n⇔\\nψθ\\n1(X0) ∼pdata\\nwhere ψθ\\nt describes the flow induced by uθ\\nt . Note however: although it is called flow model, the neural network\\nparameterizes the vector field, not the flow. In order to compute the flow, we need to simulate the ODE. In\\nalgorithm 1, we summarize the procedure how to sample from a flow model.\\nAlgorithm 1 Sampling from a Flow Model with Euler method\\nRequire: Neural network vector field uθ\\nt , number of steps n\\n1: Set t = 0\\n2: Set step size h = 1\\nn\\n3: Draw a sample X0 ∼pinit\\n4: for i = 1, . . . , n do\\n5:\\nXt+h = Xt + huθ\\nt(Xt)\\n6:\\nUpdate t ←t + h\\n7: end for\\n8: return X1\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 8}, page_content='2.2\\nDiffusion Models\\n2.2\\nDiffusion Models\\nStochastic differential equations (SDEs) extend the deterministic trajectories from ODEs with stochastic trajecto-\\nries. A stochastic trajectory is commonly called a stochastic process (Xt)0≤t≤1 and is given by\\nXt is a random variable for every 0 ≤t ≤1\\nX : [0, 1] →Rd,\\nt 7→Xt is a random trajectory for every draw of X\\nIn particular, when we simulate the same stochastic process twice, we might get different outcomes because the\\ndynamics are designed to be random.\\nBrownian Motion.\\nSDEs are constructed via a Brownian motion - a fundamental stochastic process that came\\nout of the study physical diffusion processes. You can think of a Brownian motion as a continuous random walk.\\nFigure 2: Sample trajectories of a Brownian\\nmotion Wt in dimension d = 1 simulated using\\neq. (5).\\nLet us define it: A Brownian motion W = (Wt)0≤t≤1 is a stochastic\\nprocess such that W0 = 0, the trajectories t 7→Wt are continuous,\\nand the following two conditions hold:\\n1. Normal increments: Wt−Ws ∼N(0, (t−s)Id) for all 0 ≤s <\\nt, i.e. increments have a Gaussian distribution with variance\\nincreasing linearly in time (Id is the identity matrix).\\n2. Independent increments: For any 0 ≤t0 < t1 < · · · < tn =\\n1, the increments Wt1−Wt0, . . . , Wtn−Wtn−1 are independent\\nrandom variables.\\nBrownian motion is also called a Wiener process, which is why\\nwe denote it with a \"W\".1\\nWe can easily simulate a Brownian\\nmotion approximately with step size h > 0 by setting W0 = 0 and\\nupdating\\nWt+h =Wt +\\n√\\nhϵt,\\nϵt ∼N(0, Id)\\n(t = 0, h, 2h, . . . , 1 −h) (5)\\nIn fig. 2, we plot a few example trajectories of a Brownian motion.\\nBrownian motion is as central to the study of stochastic processes as the Gaussian distribution is to the study of\\nprobability distributions. From finance to statistical physics to epidemiology, the study of Brownian motion has\\nfar reaching applications beyond machine learning. In finance, for example, Brownian motion is used to model the\\nprice of complex financial instruments. Also just as a mathematical construction, Brownian motion is fascinating:\\nFor example, while the paths of a Brownian motion are continuous (so that you could draw it without ever lifting\\na pen), they are infinitely long (so that you would never stop drawing).\\nFrom ODEs to SDEs.\\nThe idea of an SDE is to extend the deterministic dynamics of an ODE by adding stochastic\\ndynamics driven by a Brownian motion. Because everything is stochastic, we may no longer take the derivative as\\n1Nobert Wiener was a famous mathematician who taught at MIT. You can still see his portraits hanging at the MIT math department.\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 9}, page_content='2.2\\nDiffusion Models\\nFigure 3: Illustration of Ornstein-Uhlenbeck processes (eq. (8)) in dimension d = 1 for θ = 0.25 and various choices\\nof σ (increasing from left to right). For σ = 0, we recover a flow (smooth, deterministic trajectories) that converges\\nto the origin as t →∞. For σ > 0 we have random paths which converge towards the Gaussian N(0, σ2\\n2θ ) as t →∞.\\nin eq. (1a). Hence, we need to find an equivalent formulation of ODEs that does not use derivatives. For this,\\nlet us therefore rewrite trajectories (Xt)0≤t≤1 of an ODE as follows:\\nd\\ndtXt = ut(Xt)\\n▶expression via derivatives\\n(i)\\n⇔\\n1\\nh (Xt+h −Xt) = ut(Xt) + Rt(h)\\n⇔\\nXt+h = Xt + hut(Xt) + hRt(h)\\n▶expression via infinitesimal updates\\nwhere Rt(h) describes a negligible function for small h, i.e. such that lim\\nh→0 Rt(h) = 0, and in (i) we simply use the\\ndefinition of derivatives. The derivation above simply restates what we already know: A trajectory (Xt)0≤t≤1 of\\nan ODE takes, at every timestep, a small step in the direction ut(Xt). We may now amend the last equation to\\nmake it stochastic: A trajectory (Xt)0≤t≤1 of an SDE takes, at every timestep, a small step in the direction ut(Xt)\\nplus some contribution from a Brownian motion:\\nXt+h = Xt + hut(Xt)\\n|\\n{z\\n}\\ndeterministic\\n+σt (Wt+h −Wt)\\n|\\n{z\\n}\\nstochastic\\n+ hRt(h)\\n| {z }\\nerror term\\n(6)\\nwhere σt ≥0 describes the diffusion coefficient and Rt(h) describes a stochastic error term such that the standard\\ndeviation E[∥Rt(h)∥2]1/2 →0 goes to zero for h →0. The above describes a stochastic differential equation\\n(SDE). It is common to denote it in the following symbolic notation:\\ndXt = ut(Xt)dt + σtdWt\\n▶SDE\\n(7a)\\nX0 = x0\\n▶initial condition\\n(7b)\\nHowever, always keep in mind that the \"dXt\"-notation above is a purely informal notation of eq. (6). Unfortunately,\\nSDEs do not have a flow map ϕt anymore. This is because the value Xt is not fully determined by X0 ∼pinit\\nanymore as the evolution itself is stochastic. Still, in the same way as for ODEs, we have:\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 10}, page_content='2.2\\nDiffusion Models\\nTheorem 5 (SDE Solution Existence and Uniqueness)\\nIf u : Rd × [0, 1] →Rd is continuously differentiable with a bounded derivative and σt is continuous, then the\\nSDE in (7) has a solution given by the unique stochastic process (Xt)0≤t≤1 satisfying eq. (6).\\nIf this was a stochastic calculus class, we would spend several lectures proving this theorem and constructing SDEs\\nwith full mathematical rigor, i.e. constructing a Brownian motion from first principles and constructing the process\\nXt via stochastic integration. As we focus on machine learning in this class, we refer to [18] for a more technical\\ntreatment. Finally, note that every ODE is also an SDE - simply with a vanishing diffusion coefficient σt = 0.\\nTherefore, for the remainder of this class, when we speak about SDEs, we consider ODEs as a special case.\\nExample 6 (Ornstein-Uhlenbeck Process)\\nLet us consider a constant diffusion coefficient σt = σ ≥0 and a constant linear drift ut(x) = −θx for θ > 0,\\nyielding the SDE\\ndXt = −θXtdt + σdWt.\\n(8)\\nA solution (Xt)0≤t≤1 to the above SDE is known as an Ornstein-Uhlenbeck (OU) process. We visualize it in\\nfig. 3. The vector field −θx pushes the process back to its center 0 (as I always go the inverse direction of where\\nI am), while the diffusion coefficient σ always adds more noise. This process converges towards a Gaussian\\ndistribution N(0, σ2) if we simulate it for t →∞. Note that for σ = 0, we have a flow with linear vector field\\nthat we have studied in eq. (3).\\nSimulating an SDE.\\nIf you struggle with the abstract definition of an SDE so far, then don’t worry about it. A\\nmore intuitive way of thinking about SDEs is given by answering the question: How might we simulate an SDE?\\nThe simplest such scheme is known as the Euler-Maruyama method, and is essentially to SDEs what the Euler\\nmethod is to ODEs. Using the Euler-Maruyama method, we initialize X0 = x0 and update iteratively via\\nXt+h = Xt + hut(Xt) +\\n√\\nhσtϵt,\\nϵt ∼N(0, Id)\\n(9)\\nwhere h = n−1 > 0 is a step size hyperparameter for n ∈N. In other words, to simulate using the Euler-Maruyama\\nmethod, we take a small step in the direction of ut(Xt) as well as add a little bit of Gaussian noise scaled by\\n√\\nhσt.\\nWhen simulating SDEs in this class (such as in the accompanying labs), we will usually stick to the Euler-Maruyama\\nmethod.\\nDiffusion Models.\\nWe can now construct a generative model via an SDE in the same way as we did for ODEs.\\nRemember that our goal was to convert a simple distribution pinit into a complex distribution pdata. Like for\\nODEs, the simulation of an SDE randomly initialized with X0 ∼pinit is a natural choice for this transformation.\\nTo parameterize this SDE, we can simply parameterize its central ingredient - the vector field ut - a neural network\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 11}, page_content='2.2\\nDiffusion Models\\nAlgorithm 2 Sampling from a Diffusion Model (Euler-Maruyama method)\\nRequire: Neural network uθ\\nt, number of steps n, diffusion coefficient σt\\n1: Set t = 0\\n2: Set step size h = 1\\nn\\n3: Draw a sample X0 ∼pinit\\n4: for i = 1, . . . , n do\\n5:\\nDraw a sample ϵ ∼N(0, Id)\\n6:\\nXt+h = Xt + huθ\\nt (Xt) + σt\\n√\\nhϵ\\n7:\\nUpdate t ←t + h\\n8: end for\\n9: return X1\\nuθ\\nt . A diffusion model is thus given by\\ndXt = uθ\\nt (Xt)dt + σtdWt\\n▶SDE\\nX0 ∼pinit\\n▶random initialization\\nIn algorithm 2, we describe the procedure by which to sample from a diffusion model with the Euler-Maruyama\\nmethod. We summarize the results of this section as follows.\\nSummary 7 (SDE generative model)\\nThroughout this document, a diffusion model consists of a neural network uθ\\nt with parameters θ that parame-\\nterize a vector field and a fixed diffusion coefficient σt:\\nNeural network: uθ : Rd × [0, 1] →Rd, (x, t) 7→uθ\\nt(x) with parameters θ\\nFixed: σt : [0, 1] →[0, ∞), t 7→σt\\nTo obtain samples from our SDE model (i.e. generate objects), the procedure is as follows:\\nInitialization:\\nX0 ∼pinit\\n▶Initialize with simple distribution, e.g. a Gaussian\\nSimulation:\\ndXt = uθ\\nt(Xt)dt + σtdWt\\n▶Simulate SDE from 0 to 1\\nGoal:\\nX1 ∼pdata\\n▶Goal is to make X1 have distribution pdata\\nA diffusion model with σt = 0 is a flow model.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 12}, page_content='3\\nConstructing the Training Target\\nIn the previous section, we constructed flow and diffusion models where we obtain trajectories (Xt)0≤t≤1 by\\nsimulating the ODE/SDE\\nX0 ∼pinit,\\ndXt = uθ\\nt(Xt)dt\\n(Flow model)\\n(10)\\nX0 ∼pinit,\\ndXt = uθ\\nt(Xt)dt + σtdWt\\n(Diffusion model)\\n(11)\\nwhere uθ\\nt is a neural network and σt is a fixed diffusion coefficient. Naturally, if we just randomly initialize the\\nparameters θ of our neural network uθ\\nt, simulating the ODE/SDE will just produce nonsense. As always in machine\\nlearning, we need to train the neural network. We accomplish this by minimizing a loss function L(θ), such as the\\nmean-squared error\\nL(θ) = ∥uθ\\nt (x) −utarget\\nt\\n(x)\\n|\\n{z\\n}\\ntraining target\\n∥2,\\nwhere utarget\\nt\\n(x) is the training target that we would like to approximate. To derive a training algorithm, we\\nproceed in two steps: In this chapter, our goal is to find an equation for the training target utarget\\nt\\n. In the next\\nchapter, we will describe a training algorithm that approximates the training target utarget\\nt\\n. Naturally, like the\\nneural network uθ\\nt , the training target should itself be a vector field utarget\\nt\\n: Rd × [0, 1] →Rd. Further, utarget\\nt\\nshould do what we want uθ\\nt to do: convert noise into data. Therefore, the goal of this chapter is to derive a\\nformula for the training target uref\\nt\\nsuch that the corresponding ODE/SDE converts pinit into pdata. Along the\\nway we will encounter two fundamental results from physics and stochastic calculus: the continuity equation and\\nthe Fokker-Planck equation. As before, we will first describe the key ideas for ODEs before generalizing them to\\nSDEs.\\nRemark 8\\nThere are a number of different approaches to deriving a training target for flow and diffusion models. The\\napproach we present here is both the most general and arguably most simple and is in line with recent state-\\nof-the-art models. However, it might well differ from other, older presentations of diffusion models you have\\nseen. Later, we will discuss alternative formulations.\\nFigure 4: Gradual interpolation from noise to data via a Gaussian conditional probability path for a collection of\\nimages.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 13}, page_content='3.1\\nConditional and Marginal Probability Path\\n3.1\\nConditional and Marginal Probability Path\\nThe first step of constructing the training target utarget\\nt\\nis by specifying a probability path. Intuitively, a probability\\npath specifies a gradual interpolation between noise pinit and data pdata (see fig. 4). We explain the construction\\nin this section. In the following, for a data point z ∈Rd, we denote with δz the Dirac delta “distribution”. This\\nis the simplest distribution that one can imagine: sampling from δz always returns z (i.e. it is deterministic). A\\nconditional (interpolating) probability path is a set of distribution pt(x|z) over Rd such that:\\np0(·|z) = pinit,\\np1(·|z) = δz\\nfor all z ∈Rd.\\n(12)\\nIn other words, a conditional probability path gradually converts a single data point into the distribution pinit (see\\ne.g. fig. 4). You can think of a probability path as a trajectory in the space of distributions. Every conditional\\nprobability path pt(x|z) induces a marginal probability path pt(x) defined as the distribution that we obtain by\\nfirst sampling a data point z ∼pdata from the data distribution and then sampling from pt(·|z):\\nz ∼pdata,\\nx ∼pt(·|z)\\n⇒x ∼pt\\n▶sampling from marginal path\\n(13)\\npt(x) =\\nZ\\npt(x|z)pdata(z)dz\\n▶density of marginal path\\n(14)\\nNote that we know how to sample from pt but we don’t know the density values pt(x) as the integral is intractable.\\nCheck for yourself that because of the conditions on pt(·|z) in eq. (12), the marginal probability path pt interpolates\\nbetween pinit and pdata:\\np0 = pinit\\nand\\np1 = pdata.\\n▶noise-data interpolation\\n(15)\\nExample 9 (Gaussian Conditional Probability Path)\\nOne particularly popular probability path is the Gaussian probability path. This is the probability path used\\nby denoising diffusion models. Let αt, βt be noise schedulers: two continuously differentiable, monotonic\\nfunctions with α0 = β1 = 0 and α1 = β0 = 1. We then define the conditional probability path\\npt(·|z) = N(αtz, β2\\nt Id)\\n▶Gaussian conditional path\\n(16)\\nwhich, by the conditions we imposed on αt and βt, fulfills\\np0(·|z) = N(α0z, β2\\n0Id) = N(0, Id),\\nand\\np1(·|z) = N(α1z, β2\\n1Id) = δz,\\nwhere we have used the fact that a normal distribution with zero variance and mean z is just δz. Therefore,\\nthis choice of pt(x|z) fulfills eq. (12) for pinit = N(0, Id) and is therefore a valid conditional interpolating path.\\nThe Gaussian conditional probability path has several useful properties which makes it especially amenable to\\nour goals, and because of this we will use it as our prototypical example of a conditional probability path for\\nthe rest of the section. In fig. 4, we illustrate its application to an image. We can express sampling from the\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 14}, page_content='3.2\\nConditional and Marginal Vector Fields\\nFigure 5: Illustration of a conditional (top) and marginal (bottom) probability path. Here, we plot a Gaussian\\nprobability path with αt = t, βt = 1 −t. The conditional probability path interpolates a Gaussian pinit = N(0, Id)\\nand pdata = δz for single data point z. The marginal probability path interpolates a Gaussian and a data distribution\\npdata (Here, pdata is a toy distribution in dimension d = 2 represented by a chess board pattern.)\\nmarginal path pt as:\\nz ∼pdata, ϵ ∼pinit = N(0, Id) ⇒x = αtz + βtϵ ∼pt\\n▶sampling from marginal Gaussian path\\n(17)\\nIntuitively, the above procedure adds more noise for lower t until time t = 0, at which point there is only\\nnoise. In fig. 5, we plot an example of such an interpolating path between Gaussian noise and a simple data\\ndistribution.\\n3.2\\nConditional and Marginal Vector Fields\\nWe proceed now by constructing a training target utarget\\nt\\nfor a flow model using the recently defined notion of a\\nprobability path pt. The idea is to construct utarget\\nt\\nfrom simple components that we can derive analytically by\\nhand.\\nTheorem 10 (Marginalization trick)\\nFor every data point z ∈Rd, let utarget\\nt\\n(·|z) denote a conditional vector field, defined so that the corresponding\\nODE yields the conditional probability path pt(·|z), viz.,\\nX0 ∼pinit,\\nd\\ndtXt = utarget\\nt\\n(Xt|z)\\n⇒\\nXt ∼pt(·|z)\\n(0 ≤t ≤1).\\n(18)\\nThen the marginal vector field utarget\\nt\\n(x), defined by\\nutarget\\nt\\n(x) =\\nZ\\nutarget\\nt\\n(x|z)pt(x|z)pdata(z)\\npt(x)\\ndz,\\n(19)\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 15}, page_content='3.2\\nConditional and Marginal Vector Fields\\nFigure 6: Illustration of theorem 10. Simulating a probability path with ODEs. Data distribution pdata in blue\\nbackground.\\nGaussian pinit in red background.\\nTop row: Conditional probability path.\\nLeft: Ground truth\\nsamples from conditional path pt(·|z). Middle: ODE samples over time. Right: Trajectories by simulating ODE\\nwith utarget\\nt\\n(x|z) in eq. (21). Bottom row: Simulating a marginal probability path. Left: Ground truth samples\\nfrom pt. Middle: ODE samples over time. Right: Trajectories by simulating ODE with marginal vector field\\nuflow\\nt\\n(x). As one can see, the conditional vector field follows the conditional probability path and the marginal\\nvector field follows the marginal probability path.\\nfollows the marginal probability path, i.e.\\nX0 ∼pinit,\\nd\\ndtXt = utarget\\nt\\n(Xt)\\n⇒\\nXt ∼pt\\n(0 ≤t ≤1).\\n(20)\\nIn particular, X1 ∼pdata for this ODE, so that we might say \"utarget\\nt\\nconverts noise pinit into data pdata\".\\nSee fig. 6 for an illustration. Before we prove the marginalization trick, let us first explain why it is useful: The\\nmarginalization trick from theorem 10 allows us to construct the marginal vector field from a conditional vector field.\\nThis simplifies the problem of finding a formula for a training target significantly as we can often find a conditional\\nvector field utarget\\nt\\n(·|z) satisfying eq. (18) analytically by hand (i.e. by just doing some algebra ourselves). Let us\\nillustrate this by deriving a conditional vector field ut(x|z) for our running example of a Gaussian probability path.\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 16}, page_content='3.2\\nConditional and Marginal Vector Fields\\nExample 11 (Target ODE for Gaussian probability paths)\\nAs before, let pt(·|z) = N(αtz, β2\\nt Id) for noise schedulers αt, βt (see eq. (16)). Let ˙αt = ∂tαt and ˙βt = ∂tβt\\ndenote respective time derivatives of αt and βt. Here, we want to show that the conditional Gaussian vector\\nfield given by\\nutarget\\nt\\n(x|z) =\\n \\n˙αt −\\n˙βt\\nβt\\nαt\\n!\\nz +\\n˙βt\\nβt\\nx\\n(21)\\nis a valid conditional vector field model in the sense of theorem 10: its ODE trajectories Xt satisfy Xt ∼\\npt(·|z) = N(αtz, β2\\nt Id) if X0 ∼N(0, Id). In fig. 6, we confirm this visually by comparing samples from the\\nconditional probability path (ground truth) to samples from simulated ODE trajectories of this flow. As you\\ncan see, the distribution match. We will now prove this.\\nProof. Let us construct a conditional flow model ψtarget\\nt\\n(x|z) first by defining\\nψtarget\\nt\\n(x|z) = αtz + βtx.\\n(22)\\nIf Xt is the ODE trajectory of ψtarget\\nt\\n(·|z) with X0 ∼pinit = N(0, Id), then by definition\\nXt = ψtarget\\nt\\n(X0|z) = αtz + βtX0 ∼N(αtz, β2Id) = pt(·|z).\\nWe conclude that the trajectories are distributed like the conditional probability path (i.e, eq. (18) is fulfilled).\\nIt remains to extract the vector field utarget\\nt\\n(x|z) from ψtarget\\nt\\n(x|z). By the definition of a flow (eq. (2b)), it\\nholds\\nd\\ndtψtarget\\nt\\n(x|z) = utarget\\nt\\n(ψtarget\\nt\\n(x|z)|z)\\nfor all x, z ∈Rd\\n(i)\\n⇔\\n˙αtz + ˙βtx = utarget\\nt\\n(αtz + βtx|z)\\nfor all x, z ∈Rd\\n(ii)\\n⇔\\n˙αtz + ˙βt\\n\\x12x −αtz\\nβt\\n\\x13\\n= utarget\\nt\\n(x|z)\\nfor all x, z ∈Rd\\n(iii)\\n⇔\\n \\n˙αt −\\n˙βt\\nβt\\nαt\\n!\\nz +\\n˙βt\\nβt\\nx = utarget\\nt\\n(x|z)\\nfor all x, z ∈Rd\\nwhere in (i) we used the definition of ψtarget\\nt\\n(x|z) (eq. (22)), in (ii) we reparameterized x →(x −αtz)/βt, and\\nin (iii) we just did some algebra. Note that the last equation is the conditional Gaussian vector field as we\\ndefined in eq. (21). This proves the statement.a\\naOne can also double check this by plugging it into the continuity equation introduced later in this section.\\nThe remainder of this section will now prove theorem 10 via the continuity equation, a fundamental result in\\nmathematics and physics. To explain it, we will require the use of the divergence operator div, which we define as\\ndiv(vt)(x) =\\nd\\nX\\ni=1\\n∂\\n∂xi\\nvt(x)\\n(23)\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 17}, page_content='3.3\\nConditional and Marginal Score Functions\\nTheorem 12 (Continuity Equation)\\nLet us consider an flow model with vector field utarget\\nt\\nwith X0 ∼pinit. Then Xt ∼pt for all 0 ≤t ≤1 if and\\nonly if\\n∂tpt(x) = −div(ptutarget\\nt\\n)(x)\\nfor all x ∈Rd, 0 ≤t ≤1,\\n(24)\\nwhere ∂tpt(x) = d\\ndtpt(x) denotes the time-derivative of pt(x). Equation 24 is known as the continuity equation.\\nFor the mathematically-inclined reader, we present a self-contained proof of the Continuity Equation in appendix B.\\nBefore we move on, let us try and understand intuitively the continuity equation.\\nThe left-hand side ∂tpt(x)\\ndescribes how much the probability pt(x) at x changes over time. Intuitively, the change should correspond to the\\nnet inflow of probability mass. For a flow model, a particle Xt follows along the vector field utarget\\nt\\n. As you might\\nrecall from physics, the divergence measures a sort of net outflow from the vector field. Therefore, the negative\\ndivergence measures the net inflow. Scaling this by the total probability mass currently residing at x, we get that\\nthe net −div(ptut) measures the total inflow of probability mass. Since probability mass is conserved, the left-hand\\nand right-hand side of the equation should be the same! We now proceed with a proof of the marginalization trick\\nfrom theorem 10.\\nProof. By theorem 12, we have to show that the marginal vector field utarget\\nt\\n, as defined as in eq. (19), satisfies the\\ncontinuity equation. We can do this by direct calculation:\\n∂tpt(x)\\n(i)\\n= ∂t\\nZ\\npt(x|z)pdata(z)dz\\n=\\nZ\\n∂tpt(x|z)pdata(z)dz\\n(ii)\\n=\\nZ\\n−div(pt(·|z)utarget\\nt\\n(·|z))(x)pdata(z)dz\\n(iii)\\n= −div\\n\\x12Z\\npt(x|z)utarget\\nt\\n(x|z)pdata(z)dz\\n\\x13\\n(iv)\\n= −div\\n\\x12\\npt(x)\\nZ\\nutarget\\nt\\n(x|z)pt(x|z)pdata(z)\\npt(x)\\ndz\\n\\x13\\n(x)\\n(v)\\n= −div\\n\\x00ptutarget\\nt\\n\\x01\\n(x),\\nwhere in (i) we used the definition of pt(x) in eq. (13), in (ii) we used the continuity equation for the conditional\\nprobability path pt(·|z), in (iii) we swapped the integral and divergence operator using eq. (23), in (iv) we multiplied\\nand divided by pt(x), and in (v) we used eq. (19). The beginning and end of the above chain of equations show\\nthat the continuity equation is fulfilled for utarget\\nt\\n. By theorem 12, this is enough to imply eq. (20), and we are\\ndone.\\n3.3\\nConditional and Marginal Score Functions\\nWe just successfully constructed a training target for a flow model. We now extend this reasoning to SDEs. To\\ndo so, let us define the marginal score function of pt as ∇log pt(x). We can use this to extend the ODE from the\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 18}, page_content='3.3\\nConditional and Marginal Score Functions\\nFigure 7: Illustration of theorem 13. Simulating a probability path with SDEs. This repeats the plots from fig. 6\\nwith SDE sampling using eq. (25). Data distribution pdata in blue background. Gaussian pinit in red background.\\nTop row: Conditional path. Bottom row: Marginal probability path. As one can see, the SDE transports samples\\nfrom pinit into samples from δz (for the conditional path) and to pdata (for the marginal path).\\nprevious section to an SDE, as the following result demonstrates.\\nTheorem 13 (SDE extension trick)\\nDefine the conditional and marginal vector fields utarget\\nt\\n(x|z) and utarget\\nt\\n(x) as before.\\nThen, for diffusion\\ncoefficient σt ≥0, we may construct an SDE which follows the same probability path:\\nX0 ∼pinit,\\ndXt =\\n\\x14\\nutarget\\nt\\n(Xt) + σ2\\nt\\n2 ∇log pt(Xt)\\n\\x15\\ndt + σtdWt\\n(25)\\n⇒Xt ∼pt\\n(0 ≤t ≤1)\\n(26)\\nIn particular, X1 ∼pdata for this SDE. The same identity holds if we replace the marginal probability pt(x)\\nand vector field utarget\\nt\\n(x) with the conditional probability path pt(x|z) and vector field utarget\\nt\\n(x|z).\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 19}, page_content='3.3\\nConditional and Marginal Score Functions\\nWe illustrate the theorem in fig. 7. The formula in theorem 13 is useful because, similar to before, we can express\\nthe marginal score function via the conditional score function ∇log pt(x|z)\\n∇log pt(x) = ∇pt(x)\\npt(x)\\n= ∇\\nR\\npt(x|z)pdata(z)dz\\npt(x)\\n=\\nR\\n∇pt(x|z)pdata(z)dz\\npt(x)\\n=\\nZ\\n∇log pt(x|z)pt(x|z)pdata(z)\\npt(x)\\ndz\\n(27)\\nand the conditional score function ∇log pt(x|z) is something we usually know analytically, as illustrated by the\\nfollowing example.\\nExample 14 (Score Function for Gaussian Probability Paths.)\\nFor the Gaussian path pt(x|z) = N(x; αtz, β2\\nt Id), we can use the form of the Gaussian probability density (see\\neq. (81)) to get\\n∇log pt(x|z) = ∇log N(x; αtz, β2\\nt Id) = −x −αtz\\nβ2\\nt\\n.\\n(28)\\nNote that the score is a linear function of x. This is a unique feature of Gaussian distributions.\\nIn the remainder of this section, we will prove theorem 13 via the Fokker-Planck equation, which extends the\\ncontinuity equation from ODEs to SDEs. To do so, let us first define the Laplacian operator ∆via\\n∆wt(x) =\\nd\\nX\\ni=1\\n∂2\\n∂2xi\\nwt(x) = div(∇wt)(x).\\n(29)\\nTheorem 15 (Fokker-Planck Equation)\\nLet pt be a probability path and let us consider the SDE\\nX0 ∼pinit,\\ndXt = ut(Xt)dt + σtdWt.\\nThen Xt has distribution pt for all 0 ≤t ≤1 if and only if the Fokker-Planck equation holds:\\n∂tpt(x) = −div(ptut)(x) + σ2\\nt\\n2 ∆pt(x)\\nfor all x ∈Rd, 0 ≤t ≤1.\\n(30)\\nA self-contained proof of the Fokker-Planck equation can be found in appendix B. Note that the continuity equation\\nis recovered from the Fokker-Planck equation when σt = 0. The additional Laplacian term ∆pt might be hard to\\nrationalize at first. Those familiar with physics will note that the same term also appears in the heat equation\\n(which is in fact a special case of the Fokker-Planck equation). Heat diffuses through a medium. We also add a\\ndiffusion process (not a physical but a mathematical one) and hence we add this additional Laplacian term. Let\\nus now use the Fokker-Planck equation to help us prove theorem 13.\\nProof of Theorem 13. By theorem 15, we need to show that that the SDE defined in eq. (25) satisfies the Fokker-\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 20}, page_content='3.3\\nConditional and Marginal Score Functions\\nFigure 8: Top row: Particles evolving under the Langevin dynamics given by eq. (31), with p(x) taken to be a\\nGaussian mixture with 5 modes. Bottom row: A kernel density estimate of the same samples shown in the top row.\\nAs one can see, the distribution of samples converges to the equilibrium distribution p (blue background colour).\\nPlanck equation for pt. We can do this by direction calculation:\\n∂tpt(x)\\n(i)\\n= −div(ptutarget\\nt\\n)(x)\\n(ii)\\n= −div(ptutarget\\nt\\n)(x) −σ2\\nt\\n2 ∆pt(x) + σ2\\nt\\n2 ∆pt(x)\\n(iii)\\n= −div(ptutarget\\nt\\n)(x) −div(σ2\\nt\\n2 ∇pt)(x) + σ2\\nt\\n2 ∆pt(x)\\n(iv)\\n= −div(ptutarget\\nt\\n)(x) −div(pt\\n\\x14σ2\\nt\\n2 ∇log pt\\n\\x15\\n)(x) + σ2\\nt\\n2 ∆pt(x)\\n(v)\\n= −div\\n\\x12\\npt\\n\\x14\\nutarget\\nt\\n+ σ2\\nt\\n2 ∇log pt\\n\\x15\\x13\\n(x) + σ2\\nt\\n2 ∆pt(x),\\nwhere in (i) we used the Contuity Equation, in (ii) we added and subtracted the same term, in (iii) we used the\\ndefinition of the Laplacian (eq. (29)), in (iv) we used that ∇log pt =\\n∇pt\\npt , and in (v) we used the linearity of\\nthe divergence operator. The above derivation shows that the SDE defined in eq. (25) satisfies the Fokker-Planck\\nequation for pt. By theorem 15, this implies Xt ∼pt for 0 ≤t ≤1, as desired.\\nRemark 16 (Langevin dynamics.)\\nThe above construction has a famous special case when the probability path is static, i.e. pt = p for a fixed\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 21}, page_content='3.3\\nConditional and Marginal Score Functions\\ndistribution p. In this case, we set utarget\\nt\\n= 0 and obtain the SDE\\ndXt = σ2\\nt\\n2 ∇log p(Xt)dt + σtdWt,\\n(31)\\nwhich is commonly known as Langevin dynamics. The fact that pt is static implies that ∂tpt(x) = 0. It follows\\nimmediately from theorem 13 that these dynamics satisfy the Fokker-Planck equation for the static path pt = p\\nin theorem 13. Therefore, we may conclude that p is a stationary distribution of Langevin dynamics, so that\\nX0 ∼p\\n⇒\\nXt ∼p\\n(t ≥0)\\nAs with many Markov chains, these dynamics converge to the stationary distribution p under rather general\\nconditions (see section 3.3). That is, if we instead we take X0 ∼p′ ̸= p, so that Xt ∼p′\\nt, then under mild\\nconditions pt →p. This fact makes Langevin dynamics extremely useful, and it accordingly serves as the basis\\nfor e.g., molecular dynamics simulations, and many other Markov chain Monte Carlo (MCMC) methods across\\nBayesian statistics and the natural sciences.\\nLet us summarize the results of this section.\\nSummary 17 (Derivation of the Training Target)\\nThe flow training target is the marginal vector field utarget\\nt\\n. To construct it, we choose a conditional probability\\npath pt(x|z) that fulfils p0(·|z) = pinit, p1(·|z) = δz. Next, we find a conditional vector field uflow\\nt\\n(x|z) such\\nthat its corresponding flow ψtarget\\nt\\n(x|z) fulfills\\nX0 ∼pinit\\n⇒\\nXt = ψtarget\\nt\\n(X0|z) ∼pt(·|z),\\nor, equivalently, that utarget\\nt\\nsatisfies the continuity equation. Then the marginal vector field defined by\\nutarget\\nt\\n(x) =\\nZ\\nutarget\\nt\\n(x|z)pt(x|z)pdata(z)\\npt(x)\\ndz,\\n(32)\\nfollows the marginal probability path, i.e.,\\nX0 ∼pinit,\\ndXt = utarget\\nt\\n(Xt)dt ⇒Xt ∼pt\\n(0 ≤t ≤1).\\n(33)\\nIn particular, X1 ∼pdata for this ODE, so that utarget\\nt\\n\"converts noise into data\", as desired.\\nExtending to SDEs. For a time-dependent diffusion coefficient σt ≥0, we can extend the above ODE to\\nan SDE with the same marginal probability path:\\nX0 ∼pinit,\\ndXt =\\n\\x14\\nutarget\\nt\\n(Xt) + σ2\\nt\\n2 ∇log pt(Xt)\\n\\x15\\ndt + σtdWt\\n(34)\\n⇒Xt ∼pt\\n(0 ≤t ≤1),\\n(35)\\nwhere ∇log pt(x) is the marginal score function\\n∇log pt(x) =\\nZ\\n∇log pt(x|z)pt(x|z)pdata(z)\\npt(x)\\ndz.\\n(36)\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 22}, page_content='3.3\\nConditional and Marginal Score Functions\\nIn particular, for the trajectories Xt of the above SDE, it holds that X1 ∼pdata, so that the SDE \"converts\\nnoise into data\", as desired. An important example is the Gaussian probability path, yielding the formulae:\\npt(x|z) =N(x; αtz, β2\\nt Id)\\n(37)\\nuflow\\nt\\n(x|z) =\\n \\n˙αt −\\n˙βt\\nβt\\nαt\\n!\\nz +\\n˙βt\\nβt\\nx\\n(38)\\n∇log pt(x|z) = −x −αtz\\nβ2\\nt\\n,\\n(39)\\nfor noise schedulers αt, βt ∈R: continuously differentiable, monotonic functions such that α0 = β1 = 0\\nα1 = β0 = 1.\\n23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 23}, page_content='4\\nTraining the Generative Model\\nIn the last two sections, we showed how to construct a generative model with a vector field uθ\\nt given by a neural\\nnetwork, and we derived a formula for the training target utarget\\nt\\n. In this section, we will describe how to train the\\nneural network uθ\\nt to approximate the training target utarget\\nt\\n. First, we restrict ourselves to ODEs again, in doing\\nso recovering flow matching. Second, we explain how to extend the approach to SDEs via score matching. Finally,\\nwe consider the special case of Gaussian probability paths, in doing so recovering denoising diffusion models. With\\nthese tools, we will at last have an end-to-end procedure to train and sample from a generative model with ODEs\\nand SDEs.\\n4.1\\nFlow Matching\\nAs before, let us consider a flow model given by\\nX0 ∼pinit,\\ndXt = uθ\\nt(Xt) dt.\\n▶flow model\\n(40)\\nAs we learned, we want the neural network uθ\\nt to equal the marginal vector field utarget\\nt\\n.\\nIn other words, we\\nwould like to find parameters θ so that uθ\\nt ≈utarget\\nt\\n. In the following, we denote by Unif = Unif[0,1] the uniform\\ndistribution on the interval [0, 1], and by E the expected value of a random variable. An intuitive way of obtaining\\nuθ\\nt ≈utarget\\nt\\nis to use a mean-squared error, i.e. to use the flow matching loss defined as\\nLFM(θ) = Et∼Unif,x∼pt[∥uθ\\nt(x) −utarget\\nt\\n(x)∥2]\\n(41)\\n(i)\\n= Et∼Unif,z∼pdata,x∼pt(·|z)[∥uθ\\nt(x) −utarget\\nt\\n(x)∥2],\\n(42)\\nwhere pt(x) =\\nR\\npt(x|z)pdata(z)dz is the marginal probability path and in (i) we used the sampling procedure given\\nby eq. (13). Intuitively, this loss says: First, draw a random time t ∈[0, 1]. Second, draw a random point z from our\\ndata set, sample from pt(·|z) (e.g., by adding some noise), and compute uθ\\nt (x). Finally, compute the mean-squared\\nerror between the output of our neural network and the marginal vector field utarget\\nt\\n(x). Unfortunately, we are not\\ndone here. While we do know the formula for utarget\\nt\\nby theorem 10\\nutarget\\nt\\n(x) =\\nZ\\nutarget\\nt\\n(x|z)pt(x|z)pdata(z)\\npt(x)\\ndz,\\n(43)\\nwe cannot compute it efficiently because the above integral is intractable. Instead, we will exploit the fact that the\\nconditional velocity field utarget\\nt\\n(x|z) is tractable. To do so, let us define the conditional flow matching loss\\nLCFM(θ) = Et∼Unif,z∼pdata,x∼pt(·|z)[∥uθ\\nt(x) −utarget\\nt\\n(x|z)∥2].\\n(44)\\nNote the difference to eq. (41): we use the conditional vector field utarget\\nt\\n(x|z) instead of the marginal vector\\nutarget\\nt\\n(x). As we have an analytical formula for utarget\\nt\\n(x|z), we can minimize the above loss easily. But wait, what\\nsense does it make to regress against the conditional vector field if it’s the marginal vector field we care about?\\nAs it turns out, by explicitly regressing against the tractable, conditional vector field, we are implicitly regressing\\nagainst the intractable, marginal vector field. The next result makes this intuition precise.\\n24'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 24}, page_content='4.1\\nFlow Matching\\nTheorem 18\\nThe marginal flow matching loss equals the conditional flow matching loss up to a constant. That is,\\nLFM(θ) = LCFM(θ) + C,\\nwhere C is independent of θ. Therefore, their gradients coincide:\\n∇θLFM(θ) = ∇θLCFM(θ).\\nHence, minimizing LCFM(θ) with e.g., stochastic gradient descent (SGD) is equivalent to minimizing LFM(θ)\\nwith in the same fashion.\\nIn particular, for the minimizer θ∗of LCFM(θ), it will hold that uθ∗\\nt\\n= utarget\\nt\\n(assuming an infintely expressive parameterization).\\nProof. The proof works by expanding the mean-squared error into three components and removing constants:\\nLFM(θ)\\n(i)\\n= Et∼Unif,x∼pt[∥uθ\\nt (x) −utarget\\nt\\n(x)∥2]\\n(ii)\\n= Et∼Unif,x∼pt[∥uθ\\nt(x)∥2 −2uθ\\nt(x)T utarget\\nt\\n(x) + ∥utarget\\nt\\n(x)∥2]\\n(iii)\\n= Et∼Unif,x∼pt\\n\\x02\\n∥uθ\\nt(x)∥2\\x03\\n−2Et∼Unif,x∼pt[uθ\\nt (x)T utarget\\nt\\n(x)] + Et∼Unif[0,1],x∼pt[∥utarget\\nt\\n(x)∥2]\\n|\\n{z\\n}\\n=:C1\\n(iv)\\n= Et∼Unif,z∼pdata,x∼pt(·|z)[∥uθ\\nt(x)∥2] −2Et∼Unif,x∼pt[uθ\\nt (x)T utarget\\nt\\n(x)] + C1\\nwhere (i) holds by definition, in (ii) we used the formula ∥a−b∥2 = ∥a∥2 −2aT b+∥b∥2, in (iii) we define a constant\\nC1 and in (iv) we used the sampling procedure of pt given by eq. (13). Let us reexpress the second summand:\\nEt∼Unif,x∼pt[uθ\\nt(x)T utarget\\nt\\n(x)]\\n(i)\\n=\\n1\\nZ\\n0\\nZ\\npt(x)uθ\\nt(x)T utarget\\nt\\n(x) dx dt\\n(ii)\\n=\\n1\\nZ\\n0\\nZ\\npt(x)uθ\\nt(x)T\\n\\x14Z\\nutarget\\nt\\n(x|z)pt(x|z)pdata(z)\\npt(x)\\ndz\\n\\x15\\ndx dt\\n(iii)\\n=\\n1\\nZ\\n0\\nZ Z\\nuθ\\nt(x)T utarget\\nt\\n(x|z)pt(x|z)pdata(z) dz dx dt\\n(iv)\\n= Et∼Unif,z∼pdata,x∼pt(·|z)[uθ\\nt(x)T utarget\\nt\\n(x|z)]\\nwhere in (i) we expressed the expected value as an integral, in (ii) we use eq. (43), in (iii) we use the fact that\\nintegrals are linear, in (iv) we express the integral as an expected value. Note that this was really the crucial\\nstep of the proof: The beginning of the equality used the marginal vector field utarget\\nt\\n(x), while the end uses the\\n25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 25}, page_content='4.1\\nFlow Matching\\nconditional vector field utarget\\nt\\n(x|z). We plug is into the equation for LFM to get:\\nLFM(θ)\\n(i)\\n= Et∼Unif,z∼pdata,x∼pt(·|z)[∥uθ\\nt(x)∥2] −2Et∼Unif,z∼pdata,x∼pt(·|z)[uθ\\nt (x)T utarget\\nt\\n(x|z)] + C1\\n(ii)\\n= Et∼Unif,z∼pdata,x∼pt(·|z)[∥uθ\\nt (x)∥2 −2uθ\\nt(x)T utarget\\nt\\n(x|z) + ∥utarget\\nt\\n(x|z)∥2 −∥utarget\\nt\\n(x|z)∥2] + C1\\n(iii)\\n= Et∼Unif,z∼pdata,x∼pt(·|z)[∥uθ\\nt(x) −utarget\\nt\\n(x|z)∥2] + Et∼Unif,z∼pdata,x∼pt(·|z)[−∥utarget\\nt\\n(x|z)∥2]\\n|\\n{z\\n}\\nC2\\n+C1\\n(iv)\\n= LCFM(θ) + C2 + C1\\n|\\n{z\\n}\\n=:C\\nwhere in (i) we plugged in the derived equation, in (ii) we added and subtracted the same value, in (iii) we used\\nthe formula ∥a−b∥2 = ∥a∥2 −2aT b+∥b∥2 again, and in (iv) we defined a constant in θ. This finishes the proof.\\nOnce uθ\\nt has been trained, we may simulate the flow model\\ndXt = uθ\\nt (Xt) dt,\\nX0 ∼pinit\\n(45)\\nvia e.g., algorithm 1 to obtain samples X1 ∼pdata. This whole pipeline is called flow matching in the literature\\n[14, 16, 1, 15]. The training procedure is summarized in algorithm 5 and visualized in fig. 9. Let us now instantiate\\nthe conditional flow matching loss for the choice of Gaussian probability paths:\\nExample 19 (Flow Matching for Gaussian Conditional Probability Paths)\\nLet us return to the example of Gaussian probability paths pt(·|z) = N(αtz; β2\\nt Id), where we may sample from\\nthe conditional path via\\nϵ ∼N(0, Id)\\n⇒\\nxt = αtz + βtϵ ∼N(αtz, β2\\nt Id) = pt(·|z).\\n(46)\\nAs we derived in eq. (21), the conditional vector field utarget\\nt\\n(x|z) is given by\\nutarget\\nt\\n(x|z) =\\n \\n˙αt −\\n˙βt\\nβt\\nαt\\n!\\nz +\\n˙βt\\nβt\\nx,\\n(47)\\nwhere ˙αt = ∂tαt and ˙βt = ∂tβt are the respective time derivatives. Plugging in this formula, the conditional\\nflow matching loss reads\\nLCFM(θ) = Et∼Unif,z∼pdata,x∼N(αtz,β2\\nt Id)[∥uθ\\nt (x) −\\n \\n˙αt −\\n˙βt\\nβt\\nαt\\n!\\nz −\\n˙βt\\nβt\\nx∥2]\\n(i)\\n= Et∼Unif,z∼pdata,ϵ∼N(0,Id)[∥uθ\\nt (αtz + βtϵ) −( ˙αtz + ˙βtϵ)∥2]\\nwhere in (i) we plugged in eq. (46) and replaced x by αtz+βtϵ. Note the simplicity of LCFM : We sample a data\\npoint z, sample some noise ϵ and then we take a mean squared error. Let us make this even more concrete for\\nthe special case of αt = t, and βt = 1 −t. The corresponding probability pt(x|z) = N(tz, (1 −t)2) is sometimes\\n26'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 26}, page_content='4.1\\nFlow Matching\\nFigure 9: Illustration of theorem 18 with a Gaussian CondOT probability path: simulating an ODE from a trained\\nflow matching model. The data distribution is the chess board pattern (top right). Top row: Histogram from\\nground truth marginal probability path pt(x). Bottom row: Histogram of samples from flow matching model. As\\none can see, the top row and bottom row match after training (up to training error). The model was trained using\\nalgorithm 5.\\nreferred to as the (Gaussian) CondOT probability path. Then we have ˙αt = 1, ˙βt = −1, so that\\nLcfm(θ) =Et∼Unif,z∼pdata,ϵ∼N(0,Id)[∥uθ\\nt (tz + (1 −t)ϵ) −(z −ϵ)∥2]\\nMany famous state-of-the-art models have been trained using this simple yet effective procedure, e.g. Stable\\nDiffusion 3, Meta’s Movie Gen Video, and probably many more proprietary models. In fig. 9, we visualize it\\nin a simple example and in algorithm 5 we summarize the training procedure.\\nAlgorithm 3 Flow Matching Training Procedure (here for Gaussian CondOT path pt(x|z) = N(tz, (1 −t)2))\\nRequire: A dataset of samples z ∼pdata, neural network uθ\\nt\\n1: for each mini-batch of data do\\n2:\\nSample a data example z from the dataset.\\n3:\\nSample a random time t ∼Unif[0,1].\\n4:\\nSample noise ϵ ∼N(0, Id)\\n5:\\nSet x = tz + (1 −t)ϵ\\n(General case: x ∼pt(·|z))\\n6:\\nCompute loss\\nL(θ) =∥uθ\\nt(x) −(z −ϵ)∥2\\n(General case: = ∥uθ\\nt(x) −utarget\\nt\\n(x|z)∥2)\\n7:\\nUpdate the model parameters θ via gradient descent on L(θ).\\n8: end for\\n27'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 27}, page_content='4.2\\nScore Matching\\n4.2\\nScore Matching\\nLet us extend the algorithm we just found from ODEs to SDEs. Remember we can extend the target ODE to an\\nSDE with the same marginal distribution given by\\ndXt =\\n\\x14\\nutarget\\nt\\n(Xt) + σ2\\nt\\n2 ∇log pt(Xt)\\n\\x15\\ndt + σtdWt\\n(48)\\nX0 ∼pinit,\\n(49)\\n⇒Xt ∼pt\\n(0 ≤t ≤1)\\n(50)\\nwhere utarget\\nt\\nis the marginal vector field and ∇log pt(x) is the marginal score function represented via the formula\\n∇log pt(x) =\\nZ\\n∇log pt(x|z)pt(x|z)pdata(z)\\npt(x)\\ndz.\\n(51)\\nTo approximate the marginal score ∇log pt, we can use a neural network that we call score network sθ\\nt : Rd×[0, 1] →\\nRd. In the same way as before, we can design a score matching loss and a conditional score matching loss:\\nLSM(θ) = Et∼Unif,z∼pdata,x∼pt(·|z)[∥sθ\\nt(x) −∇log pt(x)∥2]\\n▶score matching loss\\nLCSM(θ) = Et∼Unif,z∼pdata,x∼pt(·|z)[∥sθ\\nt(x) −∇log pt(x|z)∥2]\\n▶conditional score matching loss\\nwhere again the difference is using the marginal score ∇log pt(x) vs. using the conditional score ∇log pt(x|z). As\\nbefore, we ideally would want to minimize the score matching loss but can’t because we don’t know ∇log pt(x).\\nBut similarly as before, the conditional score matching loss is a tractable alternative:\\nTheorem 20\\nThe score matching loss equals the conditional score matching loss up to a constant:\\nLSM(θ) = LCSM(θ) + C,\\nwhere C is independent of parameters θ. Therefore, their gradients coincide:\\n∇θLSM(θ) = ∇θLCSM(θ).\\nIn particular, for the minimizer θ∗, it will hold that sθ∗\\nt\\n= ∇log pt.\\nProof. Note that the formula for ∇log pt (eq. (51)) looks the same as the formula for utarget\\nt\\n(eq. (43)). Therefore,\\nthe proof is identical to the proof of theorem 18 replacing utarget\\nt\\nwith ∇log pt.\\nThe above procedure describes the vanilla procedure of training a diffusion model. After training, we can choose\\nan arbitrary diffusion coefficient σt ≥0 and then simulate the SDE\\nX0 ∼pinit,\\ndXt =\\n\\x14\\nuθ\\nt(Xt) + σ2\\nt\\n2 sθ\\nt(Xt)\\n\\x15\\ndt + σtdWt,\\n(52)\\nto generate samples X1 ∼pdata. In theory, every σt should give samples X1 ∼pdata at perfect training. In practice,\\nwe encounter two types of errors: (1) numerical errors by simulating the SDE imperfectly and (2) training errors\\n28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 28}, page_content='4.2\\nScore Matching\\n(i.e., the model uθ\\nt is not exactly equal to utarget\\nt\\n). Therefore, there is an optimal unknown noise level σt - this can\\nbe determined empirically by just testing our different values of empirically (see e.g. [1, 12, 17]). At first sight, it\\nmight seem to be a disadvantage that we have to learn both sθ\\nt and uθ\\nt if we wanted to use diffusion model now\\nas opposed to a flow model. However, note we can often directly sθ\\nt and uθ\\nt in a single network with two outputs,\\nso that the additional computational effort is usually minimal. Further, as we will see now for the special case of\\nthe Gaussian probability path, sθ\\nt and uθ\\nt may be converted into one another so that we don’t have to train them\\nseparately.\\nRemark 21 (Denoising Diffusion Models)\\nIf you are familiar with diffusion models, you have probably encountered the term denoising diffusion model.\\nThis model has become so popular that most people nowadays drop the word \"denoising\" and simply use the\\nterm \"diffusion model\" to describe it. In the language of this document, these are simply diffusion models with\\nGaussian probability paths pt(·|z) = N(αtz; β2\\nt Id). However, it is important to note that this might not be\\nimmediately obvious if you read some of the first diffusion model papers: they use a different time convention\\n(time is inverted) - so you need apply an appropriate time re-scaling - and they construct their probability path\\nvia so-called forward processes (we will discuss this in section 4.3).\\nExample 22 (Denoising Diffusion Models: Score Matching for Gaussian Probability Paths)\\nFirst, let us instantiate the denoising score matching loss for the case of pt(x|z) = N(αtz, β2\\nt Id). As we derived\\nin eq. (28), the conditional score ∇log pt(x|z) has the formula\\n∇log pt(x|z) = −x −αtz\\nβ2\\nt\\n.\\n(53)\\nPlugging in this formula, the conditional score matching loss becomes:\\nLCSM(θ) = Et∼Unif,z∼pdata,x∼pt(·|z)[∥sθ\\nt(x) + x −αtz\\nβ2\\nt\\n∥2]\\n(i)\\n= Et∼Unif,z∼pdata,ϵ∼N(0,Id)[∥sθ\\nt(αtz + βtϵ) + ϵ\\nβt\\n∥2]\\n= Et∼Unif,z∼pdata,ϵ∼N(0,Id)[ 1\\nβ2\\nt\\n∥βtsθ\\nt(αtz + βtϵ) + ϵ∥2]\\nwhere in (i) we plugged in eq. (46) and replaced x by αtz + βtϵ. Note that the network sθ\\nt essentially learns\\nto predict the noise that was used to corrupt a data sample z. Therefore, the above training loss is also called\\ndenoising score matching and it was the one of the first procedures used to learn diffusion models. It was soon\\nrealized that the above loss is numerically unstable for βt ≈0 close to zero (i.e. denoising score matching only\\nworks if you add a sufficient amount of noise). In some of the first works on denoising diffusion models (see\\nDenoising Diffusion Probabilitic Models, [9]) it was therefore proprosed to drop the constant\\n1\\nβ2\\nt in the loss\\nand reparameterize sθ\\nt into a noise predictor network ϵθ\\nt : Rd × [0, 1] →Rd via:\\n−βtsθ\\nt(x) = ϵθ\\nt(x)\\n⇒\\nLDDPM(θ) =Et∼Unif,z∼pdata,ϵ∼N(0,Id)\\n\\x02\\n∥ϵθ\\nt (αtz + βtϵ) −ϵ∥2\\x03\\n29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 29}, page_content='4.2\\nScore Matching\\nAs before, the network ϵθ\\nt essentially learns to predict the noise that was used to corrupt a data sample z. In\\nalgorithm 4, we summarize the training procedure.\\nAlgorithm 4 Score Matching Training Procedure for Gaussian probability path\\nRequire: A dataset of samples z ∼pdata, score network sθ\\nt or noise predictor ϵθ\\nt\\n1: for each mini-batch of data do\\n2:\\nSample a data example z from the dataset.\\n3:\\nSample a random time t ∼Unif[0,1].\\n4:\\nSample noise ϵ ∼N(0, Id)\\n5:\\nSet xt = αtz + βtϵ\\n(General case: xt ∼pt(·|z))\\n6:\\nCompute loss\\nL(θ) =∥sθ\\nt (xt) + ϵ\\nβt\\n∥2\\n(General case: = ∥sθ\\nt(xt) −∇log pt(xt|z)∥2)\\nAlternatively: L(θ) =∥ϵθ\\nt (xt) −ϵ∥2\\n7:\\nUpdate the model parameters θ via gradient descent on L(θ).\\n8: end for\\nBeyond its simplicity, there is another useful property of the Gaussian probability path: By learning sθ\\nt or ϵθ\\nt,\\nwe also learn uθ\\nt automatically and the other way around:\\nProposition 1 (Conversion formula for Gaussian probability path)\\nFor the Gaussian probability path pt(x|z) = N(αtz, β2\\nt Id), it holds that that the conditional (resp. marginal)\\nvector field can be converted into the conditional (resp. marginal) score:\\nutarget\\nt\\n(x|z) =\\n\\x12\\nβ2\\nt\\n˙αt\\nαt\\n−˙βtβt\\n\\x13\\n∇log pt(x|z) + ˙αt\\nαt\\nx\\nutarget\\nt\\n(x) =\\n\\x12\\nβ2\\nt\\n˙αt\\nαt\\n−˙βtβt\\n\\x13\\n∇log pt(x) + ˙αt\\nαt\\nx\\nwhere the formula for the above marginal vector field utarget\\nt\\nis called probability flow ODE in the literature\\n(more correctly, the corresponding ODE).\\nProof. For the conditional vector field and conditional score, we can derive:\\nutarget\\nt\\n(x|z) =\\n \\n˙αt −\\n˙βt\\nβt\\nαt\\n!\\nz +\\n˙βt\\nβt\\nx\\n(i)\\n=\\n\\x12\\nβ2\\nt\\n˙αt\\nαt\\n−˙βtβt\\n\\x13 \\x12αtz −x\\nβ2\\nt\\n\\x13\\n+ ˙αt\\nαt\\nx =\\n\\x12\\nβ2\\nt\\n˙αt\\nαt\\n−˙βtβt\\n\\x13\\n∇log pt(x|z) + ˙αt\\nαt\\nx\\nwhere in (i) we just did some algebra. By taking integrals, the same identity holds for the marginal flow vector\\nfield and the marginal score function:\\nutarget(x) =\\nZ\\nutarget\\nt\\n(x|z)pt(x|z)pdata(z)\\npt(x)\\ndz =\\nZ \\x14\\x12\\nβ2\\nt\\n˙αt\\nαt\\n−˙βtβt\\n\\x13\\n∇log pt(x|z) + ˙αt\\nαt\\nx\\n\\x15 pt(x|z)pdata(z)\\npt(x)\\ndz\\n(i)\\n=\\n\\x12\\nβ2\\nt\\n˙αt\\nαt\\n−˙βtβt\\n\\x13\\n∇log pt(x) + ˙αt\\nαt\\nx\\n30'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 30}, page_content='4.2\\nScore Matching\\nwhere in (i) we used eq. (51).\\nFigure 10: A comparison of the score, as obtained in two different ways. Top: A visualization of the score field\\nsθ\\nt (x) learned independently with score matching (see algorithm 4). Bottom: A visualization of the score field ˜sθ\\nt(x)\\nparameterized using uθ\\nt(x) as in eq. (55).\\nWe can use the conversion formula to parameterize the score network sθ\\nt and the vector field network uθ\\nt into one\\nanother via\\nuθ\\nt =\\n\\x12\\nβ2\\nt\\n˙αt\\nαt\\n−˙βtβt\\n\\x13\\nsθ\\nt(x) + ˙αt\\nαt\\nx.\\n(54)\\nSimilarly, so long as β2\\nt ˙αt −αt ˙βtβt ̸= 0 (always true for t ∈[0, 1)), it follows that\\nsθ\\nt (x) = αtuθ\\nt (x) −˙αtx\\nβ2\\nt ˙αt −αt ˙βtβt\\n.\\n(55)\\nUsing this parameterization, it can be shown that the denoising score matching and the conditional flow matching\\nlosses are the same up to a constant.\\nWe conclude that for Gaussian probability paths there is no need to\\nseparately train both the marginal score and the marginal vector field, as knowledge of one is sufficient to\\ncompute the other. In particular, we can choose whether we want to use flow matching or score matching\\nto train it. In fig. 10, we compare visually the score as approximated using score matching and the parameterized\\nscore using eq. (55). If we have trained a score network sθ\\nt , we know by eq. (52) that we can use arbitrary σt ≥0\\nto sample from the SDE\\nX0 ∼pinit,\\ndXt =\\n\\x14\\x12\\nβ2\\nt\\n˙αt\\nαt\\n−˙βtβt + σ2\\nt\\n2\\n\\x13\\nsθ\\nt(x) + ˙αt\\nαt\\nx\\n\\x15\\ndt + σtdWt\\n(56)\\n31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 31}, page_content='4.3\\nA Guide to the Diffusion Model Literature\\nto obtain samples X1 ∼pdata (up to training and simulation error). This corresponds to stochastic sampling from\\na denoising diffusion model.\\n4.3\\nA Guide to the Diffusion Model Literature\\nThere is a whole family of models around diffusion models and flow matching in the literature. When you read\\nthese papers, you will likely find a different (but equivalent) way of presenting the material from this class. This\\nmakes it sometimes a little confusing to read these papers. For this reason, we want to give a brief overview over\\nvarious frameworks and their differences and put them also in their historical context. This is not necessary to\\nunderstand the remainder of this document but rather intended to be a support for you in case you read the\\nliterature.\\nDiscrete time vs. continuous time.\\nThe first denoising diffusion model papers [28, 29, 9] did not use SDEs but\\nconstructed Markov chains in discrete time, i.e. with time steps t = 0, 1, 2, 3, . . . . To this date, you will find a lot\\nof works in the literature working with this discrete-time formulation. While this construction is appealing due to\\nits simplicity, the disadvantage of the time-discrete approach is that it forces you to choose a time discretization\\nbefore training. Further, the loss function needs to be approximated via an evidence lower bound (ELBO) - which\\nis, as the name suggests, only a lower bound to the loss we actually want to minimize. Later, Song et al. [32]\\nshowed that these constructions were essentially an approximation of a time-continuous SDEs. Further, the ELBO\\nloss becomes tight (i.e. it is not a lower bound anymore) in the continuous time case (e.g. note that theorem 18\\nand theorem 20 are equalities and not lower bounds - this would be different in the discrete time case). This made\\nthe SDE construction popular because it was considered mathematically \"cleaner\" and that one could control the\\nsimulation error via ODE/SDE samplers post training. It is important to note however that both models employ\\nthe same loss and are not fundamentally different.\\n\"Forward process\" vs probability paths.\\nThe first wave of denoising diffusion models [28, 29, 9, 32] did not use\\nthe term probability path but constructed a noising procedure of a data point z ∈Rd via a so-called forward\\nprocess. This is an SDE of the form\\n¯X0 = z,\\nd ¯Xt = uforw\\nt\\n( ¯Xt)dt + σforw\\nt\\nd ¯Wt\\n(57)\\nThe idea is that after drawing a data point z ∼pdata one simulates the forward process and thereby corrupts or\\n\"noises\" the data. The forward process is designed such that for t →∞its distribution converges to a Gaussian\\nN(0, Id).\\nIn other words, for T ≫0 it holds that ¯XT ∼N(0, Id) approximately.\\nNote that this essentially\\ncorresponds to a probability path: the conditional distribution of ¯Xt given ¯X0 = z is a conditional probability\\npath ¯pt(·|z) and the distribution of ¯Xt marginalized over z ∼pdata corresponds to a marginal probability path ¯pt.2\\nHowever, note that with this construction, we need to know the distribution of Xt|X0 = z in closed form in order\\nto train our models to avoid simulating the SDE. This essentially restrict the vector field uforw\\nt\\nto ones such that\\nwe know the distribution ¯Xt| ¯X0 = z in closed form. Therefore, throughout the diffusion model literature, vector\\nfields in forward processes are always of the affine form, i.e. uforw\\nt\\n(x) = atx for some continuous function at. For\\n2Note however that they use an inverted time convention: ¯p0(·|z) = pdata here.\\n32'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 32}, page_content='4.3\\nA Guide to the Diffusion Model Literature\\nthis choice, we can use known formulas of the conditional distribution [27, 31, 12]:\\n¯Xt| ¯X0 = z ∼N\\n\\x00αtz, β2\\nt I\\n\\x01\\n,\\nαt = exp\\n\\uf8eb\\n\\uf8ed\\nt\\nZ\\n0\\nardr\\n\\uf8f6\\n\\uf8f8,\\nβ2\\nt = α2\\nt\\nt\\nZ\\n0\\n(σforw\\nr\\n)2\\nα2r\\ndr\\nNote that these are simply Gaussian probability paths. Therefore, one can say that a forward process is a specific\\nway of constructing a (Gaussian) probability path. The term probability path was introduced by flow matching\\n[14] to both simplify the construction and make it more general at the same time: First, the \"forward process\"\\nof diffusion models is never actually simulated (only samples from ¯pt(·|z) are drawn during training). Second, a\\nforward process only converges for t →∞(i.e. we will never arrive at pinit in finite time). Therefore, we choose to\\nuse probability paths in this document.\\nTime-Reversals vs Solving the Fokker-Planck equation.\\nThe original description of diffusion models did not\\nconstruct the training target utarget\\nt\\nor ∇log pt via the Fokker-Planck equation (or Continuity equation) but via\\na time-reversal of the forward process [2]. A time-reversal (Xt)0≤t≤T is an SDE with the same distribution over\\ntrajectories inverted in time, i.e.\\nP[ ¯Xt1 ∈A1, . . . , ¯Xtn ∈An] = P[XT −t1 ∈A1, . . . , XT −tn ∈An]\\n(58)\\nfor all 0 ≤t1, . . . , tn ≤T, and A1, . . . , An ⊂S\\n(59)\\nAs shown in Anderson [2], one can obtain a time-reversal satisfying the above condition by the SDE:\\ndXt =\\n\\x02\\n−ut(Xt) + σ2\\nt ∇log pt(Xt)\\n\\x03\\ndt + σtdWt,\\nut(x) = uforw\\nT −t(x), σt = ¯σT −t\\nAs ut(Xt) = atXt, the above corresponds to a specific instance of training target we derived in proposition 1\\n(this is not immediately trivial as different time conventions are used. See e.g. [15] for a derivation). However,\\nfor the purposes of generative modeling, we often only use the final point X1 of the Markov process (e.g., as a\\ngenerated image) and discard earlier time points. Therefore, whether a Markov process is a “true” time-reversal\\nor follows along a probability path does not matter for many applications. Therefore, using a time-reversal is not\\nnecessary and often leads to suboptimal results, e.g. the probability flow ODE is often better [12, 17]. All ways of\\nsampling from a diffusion models that are different from the time-reversal rely again on using the Fokker-Planck\\nequation. We hope that this illustrates why nowadays many people construct the training targets directly via the\\nFokker-Planck equation - as pioneered by [14, 16, 1] and done in this class.\\nFlow Matching [14] and Stochastic Interpolants [1].\\nThe framework that we present is most closely related to\\nthe frameworks of flow matching and stochastic interpolants (SIs). As we learnt, flow matching restricts itself to\\nflows. In fact, one of the key innovations of flow matching was to show that one does not need a construction via a\\nforward process and SDEs but flow models alone can be trained in a scalable manner. Due to this restriction, you\\nshould keep in mind that sampling from a flow matching model will be deterministic (only the initial X0 ∼pinit will\\nbe random). Stochastic interpolants included both the pure flow and the SDE extension via \"Langevin dynamics\"\\nthat we use here (see theorem 13). Stochastic interpolants get their name from a interpolant function I(t, x, z)\\nintended to interpolate between two distributions. In the terminology we use here, this corresponds to a different\\n33'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 33}, page_content='4.3\\nA Guide to the Diffusion Model Literature\\nyet (mainly) equivalent way of constructing a conditional and marginal probability path. The advantage of flow\\nmatching and stochastic interpolants over diffusion models is both their simplicity and their generality: their\\ntraining framework is very simple but at the same time they allow you to go from an arbitrary distribution pinit to\\nan arbitrary distribution pdata - while denoising diffusion models only work for Gaussian initial distributions and\\nGaussian probability path. This opens up new possibilities for generative modeling that we will touch upon briefly\\nlater in this class.\\nLet us summarize the results of this section:\\nSummary 23 (Training the Generative Model)\\nFlow matching consists of training a neural network uθ\\nt via minimizing the conditional flow matching loss\\nLCFM(θ) = Ez∼pdata,t∼Unif, x∼pt(·|z)[∥uθ\\nt(x) −utarget\\nt\\n(x|z)∥2]\\n(conditional flow matching loss)\\n(60)\\nwhere utarget\\nt\\n(x|z) is the conditional vector field (see algorithm 5). After training, one generates samples by\\nsimulating the corresponding ODE (see algorithm 1). To extend this to a diffusion model, we can use a score\\nnetwork sθ\\nt and train it via conditional score matching\\nLCSM(θ) = Ez∼pdata, t∼Unif, x∼pt(·|z)[∥sθ\\nt (x) −∇log pt(x|z)∥2]\\n(denoising score matching loss)\\n(61)\\nFor every diffusion coefficient σt ≥0, simulating the SDE (e.g. via algorithm 2)\\nX0 ∼pinit,\\ndXt =\\n\\x14\\nuθ\\nt(Xt) + σ2\\nt\\n2 sθ\\nt (Xt)\\n\\x15\\ndt + σtdWt\\n(62)\\nwill result in generating approximate samples from pdata. One can empirically find the optimal σt ≥0.\\nGaussian probability paths.\\nFor the special case of a Gaussian probability path pt(x|z) = N(x; αtz, β2\\nt Id), the\\nconditional score matching is also called denoising score matching. This loss and conditional flow matching\\nloss are then given by:\\nLCFM(θ) =Et∼Unif,z∼pdata,ϵ∼N(0,Id)[∥uθ\\nt(αtz + βtϵ) −( ˙αtz + ˙βtϵ)∥2]\\nLCSM(θ) =Et∼Unif,z∼pdata,ϵ∼N(0,Id)[∥sθ\\nt(αtz + βtϵ) + ϵ\\nβt\\n∥2]\\nIn this case, there is no need to train sθ\\nt and uθ\\nt separately as we can convert them post training via the formula:\\nuθ\\nt (x) =\\n\\x12\\nβ2\\nt\\n˙αt\\nαt\\n−˙βtβt\\n\\x13\\nsθ\\nt(x) + ˙αt\\nαt\\nx\\nAlso here, after training we can simulate the SDE in eq. (62) via algorithm 2 to obtain samples X1.\\nDenoising diffusion models.\\nDenoising diffusion models are diffusion models with Gaussian probability paths.\\nFor this reason, it is sufficient for them to learn either uθ\\nt or sθ\\nt as they can be converted into one another.\\n34'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 34}, page_content='4.3\\nA Guide to the Diffusion Model Literature\\nWhile flow matching only allows for a simulation procedure that is deterministic via ODE, they allow for a\\nsimulation that is deterministic (probability flow ODE) or stochastic (SDE sampling). However, unlike flow\\nmatching or stochastic interpolants that allow to convert arbitrary distributions pinit into arbitrary distributions\\npdata via arbitrary probability paths pt, denoising diffusion models only works for Gaussian initial distributions\\npinit = N(0, Id) and a Gaussian probability path.\\nLiterature\\nAlternative formulations for diffusion models that are popular in the literature are:\\nDiscrete-time: Approximations of SDEs via discrete-time Markov chains are often used.\\n1.2. Inverted time convention: It is popular to use an inverted time convention where t = 0 corresponds to\\npdata (as opposed to here where t = 0 corresponds to pinit).\\n3. Forward process: Forward processes (or noising processes) are ways of constructing (Gaussian) probability\\npaths.\\n4. Training target via time-reversal: A training target can also be constructed via the time-reversal of\\nSDEs. This is a specific instance of the construction presented here (with an inverted time convention).\\n35'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 35}, page_content='5\\nBuilding an Image Generator\\nIn the previous sections, we learned how to train a flow matching or diffusion model to sample from a distribution\\npdata(x). This recipe is general and can be applied to a variety of different data types and applications. In this\\nsection, we learn how to apply this framework to build an image or video generator, such as e.g., Stable Diffusion\\n3 and Meta Movie Gen Video. To build such a model, there are two main ingredients that we are missing: First,\\nwe will need to formulate conditional generation (guidance), e.g. how do we generate an image that fits a specific\\ntext prompt, and how our existing objectives may be suitably adapted to this end.\\nWe will also learn about\\nclassifier-free guidance, a popular technique used to enhance the quality of conditional generation. Second, we will\\ndiscuss common neural network architectures, again focusing on those designed for images and videos. Finally,\\nwe will examine in depth the two state-of-the-art image and video models mentioned above - Stable Diffusion and\\nMeta MovieGen - to give you a taste of how things are done at scale.\\n5.1\\nGuidance\\nSo far, the generative models we considered were unconditional, e.g. an image model would simply generate some\\nimage. However, the task is not merely to generate an arbitrary object, but to generate an object conditioned on\\nsome additional information. For example, one might imagine a generative model for images which takes in a text\\nprompt y, and then generates an image x conditioned on y. For fixed prompt y, we would thus like to sample from\\npdata(x|y), that is, the data distribution conditioned on y. Formally, we think of y to live in a space Y. When y\\ncorresponds to a text-prompt, for example, Y would likely be some continuous space like Rdy. When y corresponds\\nto some discrete class label, Y would be discrete. In the lab, we will work with the MNIST dataset, in which case\\nwe will take Y = {0, 1, . . . , 9} to correspond to the identities of handwritten digits.\\nTo avoid a notation and terminology clash with the use of the word \"conditional\" to refer to conditioning on\\nz ∼pdata (conditional probability path/vector field), we will make use of the term guided to refer specifically to\\nconditioning on y.\\nRemark 24 (Guided vs. Conditional Terminology)\\nIn these notes, we opt to use the term guided in place of conditional to refer to the act of conditioning on y.\\nHere, we will refer to e.g., a guided vector field utarget\\nt\\n(x|y) and a conditional vector field utarget\\nt\\n(x|z). This\\nterminology is consistent with other works such as [15].\\nThe goal of guided generative modeling is thus to be able to sample from pdata(x|y) for any such y. In the\\nlanguage of flow and score matching, and in which our generative models correspond to the simulation of ordinary\\nand stochastic differential equations, this can be phrased as follows.\\nKey Idea 5 (Guided Generative Model)\\nWe define a guided diffusion model to consist of a guided vector field uθ\\nt (·|y), parameterized by some neural\\n36'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 36}, page_content='5.1\\nGuidance\\nnetwork, and a time-dependent diffusion coefficient σt, together given by\\nNeural network: uθ : Rd × Y × [0, 1] →Rd, (x, y, t) 7→uθ\\nt(x|y)\\nFixed: σt : [0, 1] →[0, ∞), t 7→σt\\nNotice the difference from summary 7: we are additionally guiding uθ\\nt with the input y ∈Y. For any such\\ny ∈Rdy, samples may then be generated from such a model as follows:\\nInitialization:\\nX0 ∼pinit\\n▶Initialize with simple distribution (such as a Gaussian)\\nSimulation:\\ndXt = uθ\\nt(Xt|y)dt + σtdWt\\n▶Simulate SDE from t = 0 to t = 1.\\nGoal:\\nX1 ∼pdata(·|y)\\n▶Goal is for X1 to be distributed like pdata(·|y).\\nWhen σt = 0, we say that such a model is a guided flow model.\\n5.1.1\\nGuidance for Flow Models\\nIf we imagine fixing our choice of y, and take our data distribution as pdata(x|y), then we have recovered the unguided\\ngenerative problem, and can accordingly construct a generative model using the conditional flow matching objective,\\nviz.,\\nEz∼pdata(·|y),x∼pt(·|z)∥uθ\\nt(x|y) −utarget\\nt\\n(x|z)∥2.\\n(63)\\nNote that the label y does not affect the conditional probability path pt(·|z) or the conditional vector field utarget\\nt\\n(x|z)\\n(although in principle, we could make it dependent). Expanding the expectation over all such choices of y, and\\nover all times t ∈Unif[0, 1), we thus obtain a guided conditional flow matching objective\\nLguided\\nCFM (θ) = E(z,y)∼pdata(z,y), t∼Unif[0,1), x∼pt(·|z)∥uθ\\nt(x|y) −utarget\\nt\\n(x|z)∥2.\\n(64)\\nOne of the main differences between the guided objective in eq. (64) and the unguided objective from eq. (44) is\\nthat here we are sampling (z, y) ∼pdata rather than just z ∼pdata. The reason is that our data distribution is\\nnow, in principle, a joint distribution over e.g., both images z and text prompts y. In practice, this means that\\na PyTorch implementation of eq. (64) would involve a dataloader which returned batches of both z and y. The\\nabove procedure leads to a faithful generation procedure of pdata(·|y).\\nClassifier-Free Guidance.\\nWhile the above conditional training procedure is theoretically valid, it was soon em-\\npirically realized that images samples with this procedure did not fit well enough to the desired label y. It was\\ndiscovered that perceptual quality is increased when the effect of the guidance variable y is artificially reinforced.\\nThis insight was distilled into a technique known as classifier-free guidance that is widely used in the context\\nof state-of-the-art diffusion models, and which we discuss next. For simplicity, we will focus here on the case of\\nGaussian probability paths. Recall from eq. (16) that a Gaussian conditional probability path is given by\\npt(·|z) = N(αtz, β2\\nt Id)\\n37'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 37}, page_content='5.1\\nGuidance\\nwhere the noise schedulers αt and βt are continuously differentiable, monotonic, and satisfy α0 = β1 = 0 and\\nα1 = β0 = 1. To gain intuition for classifier-free guidance, we can use proposition 1 to rewrite the guided vector\\nfield utarget\\nt\\n(x|y) in the following form using the guided score function ∇log pt(x|y)\\nutarget\\nt\\n(x|y) = atx + bt∇log pt(x|y),\\n(65)\\nwhere\\n(at, bt) =\\n \\n˙αt\\nαt\\n, ˙αtβ2\\nt −˙βtβtαt\\nαt\\n!\\n.\\n(66)\\nHowever, notice that by Bayes’ rule, we can rewrite the guided score as\\n∇log pt(x|y) = ∇log\\n\\x12pt(x)pt(y|x)\\npt(y)\\n\\x13\\n= ∇log pt(x) + ∇log pt(y|x),\\n(67)\\nwhere we used that the gradient ∇is taken with respect to the variable x, so that ∇log pt(y) = 0. We may thus\\nrewrite\\nutarget\\nt\\n(x|y) = atx + bt(∇log pt(x) + ∇log pt(y|x)) = utarget\\nt\\n(x) + bt∇log pt(x|y).\\nNotice the shape of the above equation: The guided vector field utarget\\nt\\n(x|y) is a sum of the unguided vector field\\nplus a guided score ∇log pt(x|y). As people observed that their image x did not fit their prompt y well enough, it\\nwas a natural idea to scale up the contribution of the ∇log pt(y|x) term, yielding\\n˜ut(x|y) = utarget\\nt\\n(x) + wbt∇log pt(y|x),\\nwhere w > 1 is known as the guidance scale. Note that this is a heuristic: for w ̸= 1, it holds that ˜ut(x|y) ̸=\\nutarget\\nt\\n(x|y), i.e. therefore not the true, guided vector field. However, empirical results have shown to yield preferable\\nresults (when w > 1).\\nRemark 25 (Where is the classifier?)\\nThe term log pt(y|x) can be considered as a sort of classifier of noised data (i.e. it gives the likelihoods of y\\ngiven x). In fact, early works in diffusion trained actual classifiers and used them to the guide via the above\\nprocedure. This leads to classifier guidance [5, 30]. As it has been largely superseded by classifier-free guidance,\\nwe do not consider it here.\\nWe may again apply the equality\\n∇log pt(x|y) = ∇log pt(x) + ∇log pt(y|x)\\n38'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 38}, page_content='5.1\\nGuidance\\nto obtain\\n˜ut(x|y) = utarget\\nt\\n(x) + wbt∇log pt(y|x)\\n= utarget\\nt\\n(x) + wbt(∇log pt(x|y) −∇log pt(x))\\n= utarget\\nt\\n(x) −(watx + wbt∇log pt(x)) + (watx + wbt∇log pt(x|y))\\n= (1 −w)utarget\\nt\\n(x) + wutarget\\nt\\n(x|y).\\nWe may therefore express the scaled guided vector field ˜ut(x|y) as the linear combination of the unguided vector\\nfield utarget\\nt\\n(x) with the guided vector field utarget\\nt\\n(x|y). The idea might then to to train both an unguided utarget\\nt\\n(x)\\n(using e.g., eq. (44)) as well as a guided utarget\\nt\\n(x|y) (using e.g., eq. (64)), and then combine them at inference time\\nto obtain ˜ut(x|y). \"But wait!\", you might ask, \"wouldn’t we need to train two models then !?\". It turns out\\nwe do can train both in model: we may thus augment our label set with a new, additional ∅label that denotes\\nthe absence of conditioning. We can then treat utarget\\nt\\n(x) = utarget\\nt\\n(x|∅). With that, we do not need to train\\na separate model to reinforce the effect of a hypothetical classifier. This approach of training a conditional and\\nunconditional model in one (and subsequently reinforcing the conditioning) is known as classifier-free guidance\\n(CFG) [10].\\nRemark 26 (Derivation for general probability paths)\\nNote that the construction\\n˜ut(x|y) = (1 −w)utarget\\nt\\n(x) + wutarget\\nt\\n(x|y),\\nis equally valid for any choice probability path, not just a Gaussian one. When w = 1, it is straightforward to\\nverify that ˜ut(x|y) = utarget\\nt\\n(x|y). Our derivation using Gaussian paths was simply to illustrate the intuition\\nbehind the construction, and in particular of amplifying the contribution of a “classifier” ∇log pt(y|x).\\nTraining and Context-Free Guidance.\\nWe must now amend the guided conditional flow matching objective from\\neq. (64) to account for the possibility of y = ∅. The challenge is that when sampling (z, y) ∼pdata, we will never\\nobtain y = ∅. It follows that we must introduce the possibility of y = ∅artificially. To do so, we will define some\\nhyperparameter η to be the probability that we discard the original label y, and replace it with ∅. We thus arrive\\nat our CFG conditional flow matching training objective\\nLCFG\\nCFM(θ) = E□∥uθ\\nt(x|y) −utarget\\nt\\n(x|z)∥2\\n(68)\\n□= (z, y) ∼pdata(z, y), t ∼Unif[0, 1), x ∼pt(·|z), replace y = ∅with prob. η\\n(69)\\nWe summarize our findings below.\\nSummary 27 (Classifier-Free Guidance for Flow Models)\\nGiven the unguided marginal vector field utarget\\nt\\n(x|∅), the guided marginal vector field utarget\\nt\\n(x|y), and a\\nguidance scale w > 1, we define the classifier-free guided vector field ˜ut(x|y) by\\n˜ut(x|y) = (1 −w)utarget\\nt\\n(x|∅) + wutarget\\nt\\n(x|y).\\n(70)\\n39'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 39}, page_content='5.1\\nGuidance\\nFigure 11: The effect of classifier guidance. The prompt here is the \"class\" chosen to be \"Corgi\" (a specific type\\nof dog). Left: samples generated with no guidance (i.e., w = 1). Right: samples generated with classifier guidance\\nand w = 4. As shown, classifier-free guidance improves the similarity to the prompt. Figure taken from [10].\\nBy approximating utarget\\nt\\n(x|∅) and utarget\\nt\\n(x|y) using the same neural network, we may leverage the following\\nclassifier-free guidance CFM (CFG-CFM) objective, given by\\nLCFG\\nCFM(θ) = E□∥uθ\\nt(x|y) −utarget\\nt\\n(x|z)∥2\\n(71)\\n□= (z, y) ∼pdata(z, y), t ∼Unif[0, 1), x ∼pt(·|z), replace y = ∅with prob. η\\n(72)\\nIn plain English, LCFG\\nCFM might be approximated by\\n(z, y) ∼pdata(z, y)\\n▶\\nSample (z, y) from data distribution.\\nt ∼Unif[0, 1)\\n▶\\nSample t uniformly on [0, 1).\\nx ∼pt(x|z)\\n▶\\nSample x from the conditional probability path pt(x|z).\\nwith prob. η, y ←∅\\n▶\\nReplace y with ∅with probability η.\\n\\\\\\nLCFG\\nCFM(θ) = ∥uθ\\nt (x|y) −utarget\\nt\\n(x|z)∥2\\n▶\\nRegress model against conditional vector field.\\nAbove, we made use multiple times of the fact that utarget\\nt\\n(x|z) = utarget\\nt\\n(x|z, y). At inference time, for a fixed\\nchoice of y, we may sample via\\nInitialization:\\nX0 ∼pinit(x)\\n▶Initialize with simple distribution (such as a Gaussian)\\nSimulation:\\ndXt = ˜uθ\\nt(Xt|y)dt\\n▶Simulate ODE from t = 0 to t = 1.\\nSamples:\\nX1\\n▶Goal is for X1 to adhere to the guiding variable y.\\nNote that the distribution of X1 is not necessarily aligned with X1 ∼pdata(·|y) anymore if we use a weight w > 1.\\nHowever, empirically, this shows better alignment with conditioning.\\nIn fig. 11, we illustrate class-based classifier-\\nfree guidance on 128x128 ImageNet, as in [10]. Similarly, in fig. 12, we visualize the affect of various guidance scales\\nw when applying classifier-free guidance to sampling from the MNIST dataset of handwritten digits.\\n40'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 40}, page_content='5.1\\nGuidance\\nFigure 12: The effect of classifier-free guidance applied at various guidance scales for the MNIST dataset of hand-\\nwritten digits. Left: Guidance scale set to w = 1.0. Middle: Guidance scale set to w = 2.0. Right: Guidance scale\\nset to w = 4.0. You will generate a similar image yourself in the lab three!\\nAlgorithm 5 Classifier-free guidance training for Gaussian probability path pt(x|z) = N(x; αtz, β2\\nt Id)\\nRequire: Paired dataset (z, y) ∼pdata, neural network uθ\\nt\\n1: for each mini-batch of data do\\n2:\\nSample a data example (z, y) from the dataset.\\n3:\\nSample a random time t ∼Unif[0,1].\\n4:\\nSample noise ϵ ∼N(0, Id)\\n5:\\nSet x = αtz + βtϵ\\n6:\\nWith probability p drop label: y ←∅\\n7:\\nCompute loss\\nL(θ) =∥uθ\\nt (x|y) −( ˙αtϵ + ˙βtz)∥2\\n8:\\nUpdate the model parameters θ via gradient descent on L(θ).\\n9: end for\\n5.1.2\\nGuidance for Diffusion Models\\nIn this section we extend the reasoning of the previous section to diffusion models. First, in the same way that we\\nobtained eq. (64), we may generalize the conditional score matching loss eq. (61) to obtain the guided conditional\\nscore matching objective\\nLguided\\nCSM (θ) = E□[∥sθ\\nt(x|y) −∇log pt(x|z)∥2]\\n(73)\\n□= (z, y) ∼pdata(z, y), t ∼Unif, x ∼pt(·|z).\\n(74)\\nA guided score network sθ\\nt (x|y) trained with eq. (73) might then be combined with the guided vector field uθ\\nt (x|y)\\nto simulate the SDE\\nX0 ∼pinit,\\ndXt =\\n\\x14\\nuθ\\nt (Xt|y) + σ2\\nt\\n2 sθ\\nt (Xt|y)\\n\\x15\\ndt + σtdWt.\\n41'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 41}, page_content='5.1\\nGuidance\\nClassifier-Free Guidance.\\nWe now extend the classifier-free guidance construction to the diffusion setting. By\\nBayes’ rule (see eq. (67)),\\n∇log pt(x|y) = ∇log pt(x) + ∇log pt(y|x),\\nso that for guidance scale w > 1 we may define\\n˜st(x|y) = ∇log pt(x) + w∇log pt(y|x)\\n= ∇log pt(x) + w(∇log pt(x|y) −∇log pt(x))\\n= (1 −w)∇log pt(x) + w∇log pt(x|y)\\n= (1 −w)∇log pt(x|∅) + w∇log pt(x|y)\\nWe thus arrive at the CFG-compatible (that is, accounting for the possibility of ∅) objective\\nLCFG\\nDSM(θ) = E□∥sθ\\nt (x|y) −∇log pt(x|z)∥2\\n(75)\\n□= (z, y) ∼pdata(z, y), t ∼Unif[0, 1), x ∼pt(·|z), replace y = ∅with prob. η,\\n(76)\\nwhere η is a hyperparameter (the probability of replacing y with ∅). We will refer LCFG\\nCSM(θ) as the guided conditional\\nscore matching objective. We recap as follows\\nSummary 28 (Classifier-Free Guidance for Diffusions)\\nGiven the unguided marginal score ∇log pt(x|∅), the guided marginal score field ∇log pt(x|y), and a guidance\\nscale w > 1, we define the classifier-free guided score ˜st(x|y) by\\n˜st(x|y) = (1 −w)∇log pt(x|∅) + w∇log pt(x|y).\\n(77)\\nBy approximating ∇log pt(x|∅) and ∇log pt(x|y) using the same neural network sθ\\nt(x|y), we may leverage the\\nfollowing classifier-free guidance CSM (CFG-CSM) objective, given by\\nLCFG\\nCSM(θ) = E□∥sθ\\nt(x|(1 −ξ)y + ξ∅) −∇log pt(x|z)∥2\\n(78)\\n□= (z, y) ∼pdata(z, y), t ∼Unif[0, 1), x ∼pt(·|z), replace y = ∅with prob. η\\n(79)\\nIn plain English, LCFG\\nDSM might be approximated by\\n(z, y) ∼pdata(z, y)\\n▶\\nSample (z, y) from data distribution.\\nt ∼Unif[0, 1)\\n▶\\nSample t uniformly on [0, 1).\\nx ∼pt(x|z, y)\\n▶\\nSample x from cond. path pt(x|z).\\nwith prob. η, y ←∅\\n▶\\nReplace y with ∅with probability η.\\n\\\\\\nLCFG\\nDSM(θ) = ∥sθ\\nt (x|y) −∇log pt(x|z)∥2\\n▶\\nRegress model against conditional score.\\nAt inference time, for a fixed choice of w > 1, we may combine sθ\\nt(x|y) with a guided vector field uθ\\nt(x|y) and\\n42'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 42}, page_content='5.2\\nNeural network architectures\\ndefine\\n˜sθ\\nt (x|y) = (1 −w)sθ\\nt (x|∅) + wsθ\\nt (x|y),\\n˜uθ\\nt (x|y) = (1 −w)uθ\\nt (x|∅) + wuθ\\nt(x|y).\\nThen we may sample via\\nInitialization:\\nX0 ∼pinit(x)\\n▶Initialize with simple distribution (such as a Gaussian)\\nSimulation:\\ndXt =\\n\\x14\\n˜uθ\\nt (Xt|y) + σ2\\nt\\n2 ˜sθ\\nt(Xt|y)\\n\\x15\\ndt + σtdWt\\n▶Simulate SDE from t = 0 to t = 1.\\nSamples:\\nX1\\n▶Goal is for X1 to adhere to the guiding variable y.\\n5.2\\nNeural network architectures\\nWe next discuss the design of neural networks for flow and diffusion models. Specifically, we answer the question of\\nhow to construct a neural network architecture that represents the (guided) vector field uθ\\nt(x|y) with parameters θ.\\nNote that the neural network must have 3 inputs - a vector x ∈Rd, a conditioning variable y ∈Y, and a time value\\nt ∈[0, 1] - and one output - a vector uθ\\nt(x|y) ∈Rd. For low-dimensional distributions (e.g. the toy distributions\\nwe have seen in previous sections), it is sufficient to parameterize uθ\\nt (x|y) as a multi-layer perceptron (MLP), oth-\\nerwise known as a fully connected neural network. That is, in this simple setting, a forward pass through uθ\\nt (x|y)\\nwould involve concatenating our input x, y, and t, and passing them through an MLP. However, for complex,\\nhigh-dimensional distributions, such as those over images, videos, and proteins, an MLP is rarely sufficient, and\\nit is common to use special, application-specific architectures. For the remainder of this section, we will consider\\nthe case of images (and by extension, videos), and discuss two common architectures: the U-Net [25], and the\\ndiffusion transformer (DiT).\\n5.2.1\\nU-Nets and Diffusion Transformers\\nBefore we dive into the specifics of these architectures, let us recall from the introduction that an image is simply\\na vector x ∈RCimage×H×W . Here Cimage denotes the number of channels (an RGB image typically would have\\nCinput = 3 color channels), H denotes the height of the image in pixels, and W denotes the width of the image in\\npictures.\\nU-Nets.\\nThe U-Net architecture [25] is a specific type of convolutional neural network. Originally designed for\\nimage segmentation, its crucial feature is that both its input and its output have the shape of images (possibly with\\na different number of channels). This makes it ideal to parameterize a vector field x 7→uθ\\nt (x|y) as for fixed y, t its\\ninput has the shape of an image and its output does, too. Therefore, U-Net were widely used in the development of\\ndiffusion models. A U-Net consists of a series of encoders Ei, and a corresponding sequence of decoders Di, along\\nwith a latent processing block in between, which we shall refer to as a midcoder (midcoder is a term is not used in\\nthe literature usually). For sake of example, let us walk through the path taken by an image xt ∈R3×256×256 (we\\n43'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 43}, page_content='5.2\\nNeural network architectures\\nFigure 13: The simplified U-Net architecture used in lab three.\\nhave taken (Cinput, H, W) = (3, 256, 256)) as it is processed by the U-Net:\\nxinput\\nt\\n∈R3×256×256\\n▶Input to the U-Net.\\nxlatent\\nt\\n= E(xinput\\nt\\n) ∈R512×32×32\\n▶Pass through encoders to obtain latent.\\nxlatent\\nt\\n= M(xlatent\\nt\\n) ∈R512×32×32\\n▶Pass latent through midcoder.\\nxoutput\\nt\\n= D(xlatent\\nt\\n) ∈R3×256×256\\n▶Pass through decoders to obtain output.\\nNotice that as the input passes through the encoders, the number of channels in its representation increases, while\\nthe height and width of the images are decreased. Both the encoder and the decoder usually consist of a series\\nof convolutional layers (with activation functions, pooling operations, etc. in between). Not shown above are two\\npoints: First, the input xinput\\nt\\n∈R3×256×256 is often fed into an initial pre-encoding block to increase the number\\nof channels before being fed into the first encoder block. Second, the encoders and decoders are often connected\\nby residual connections. The complete picture is shown in fig. 13. At a high level, most U-Nets involve some\\nvariant of what is described above. However, certain of the design choices described above may well differ from\\nvarious implementations in practice. In particular, we opt above for a purely-convolutional architecture whereas it\\nis common to include attention layers as well throughout the encoders and decoders. The U-Net derives its name\\nfrom the “U”-like shape formed by its encoders and decoders (see fig. 13).\\nDiffusion Transformers.\\nOne alternative to U-Nets are diffusion transformers (DiTs), which dispense with con-\\nvolutions and purely use attention [35, 19]. Diffusion transformers are based on vision transformers (ViTs), in\\nwhich the big idea is essentially to divide up an image into patches, embed each of these patches, and then attend\\nbetween the patches [6]. Stable Diffusion 3, trained with conditional flow matching, parameterizes the velocity field\\n44'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 44}, page_content='5.2\\nNeural network architectures\\nuθ\\nt (x) as a modified DiT, as we discuss later in section 5.3 [7].\\nRemark 29 (Working in Latent Space)\\nA common problem for large-scale applications is that the data is so high-dimensional that it consumes too\\nmuch memory. For example, we might want to generate a high resolution image of 1000 × 10000 pixels leading\\nto 1 million (!) dimensions. To reduce memory usage, a common design pattern is to work in a latent space\\nthat can be considered a compressed version of our data at lower resolution. Specifically, the usual approach\\nis to combine a flow or diffusion model with a (variational) autoencoder [24]. In this case, one first encodes\\nthe training dataset in the latent space via an autoencoder, and then training the flow or diffusion model\\nin the latent space. Sampling is performed by first sampling in the latent space using the trained flow or\\ndiffusion model, and then decoding of the output via the decoder. Intuitively, a well-trained autoencoder can\\nbe thought of as filtering out semantically meaningless details, allowing the generative model to “focus” on\\nimportant, perceptually relevant features [24]. By now, nearly all state-of-the-art approaches to image and\\nvideo generation involve training a flow or diffusion model in the latent space of an autoencoder - so called\\nlatent diffusion models [24, 34]. However, it is important to note: one also needs to train the autoencoder\\nbefore training the diffusion models. Crucially, performance now depends also on how good the autoencoder\\ncompresses images into latent space and recovers aesthetically pleasing images.\\n5.2.2\\nEncoding the Guiding Variable.\\nUp until this point, we have glossed over how exactly the guiding (conditioning) variable y is fed into the neural\\nnetwork uθ\\nt (x|y). Broadly, this process can be decomposed into two steps: embedding the raw input yraw (e.g., the\\ntext prompt “a cat playing a trumpet, photorealistic”) into some vector-valued input y, and feeding the resulting y\\ninto the actual model. We now proceed to describe each step in greater detail.\\nEmbedding Raw Input.\\nHere, we’ll consider two cases: (1) where yraw is a discrete class-label, and (2) where\\nyraw is a text-prompt. When yraw ∈Y ≜{0, . . . , N} is just a class label, then it is often easiest to simply learn\\na separate embedding vector for each of the N + 1 possible values of yraw, and set y to this embedding vector.\\nOne would consider the parameters of these embeddings to be included in the parameters of uθ\\nt(x|y), and would\\ntherefore learn these during training. When yraw is a text-prompt, the situation is more complex, and approaches\\nlargely rely on frozen, pre-trained models. Such models are trained to embed a discrete text input into a continuous\\nvector that captures the relevant information. One such model is known as CLIP (Contrastive Language-Image Pre-\\ntraining). CLIP is trained to learn a shared embedding space for both images and text-prompts, using a training\\nloss designed to encourage image embeddings to be close to their corresponding prompts, while being farther from\\nthe embeddings of other images and prompts [22]. We might therefore take y = CLIP(yraw) ∈RdCLIP to be the\\nembedding produced by a frozen, pre-trained CLIP model. In certain cases, it may be undesirable to compress the\\nentire sequence into a single representation. In this case, one might additionally consider embedding the prompt\\nusing a pre-trained transformer so as to obtain a sequence of embeddings. It is also common to combine multiple\\nsuch pretrained embeddings when conditioning so as to simultaneously reap the benefits of each model [7, 21].\\n45'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 45}, page_content='5.3\\nA Survey of Large-Scale Image and Video Models\\nFigure 14: Left: An overview of the diffusion transformer architecture, taken from [19]. Right: A schematic of the\\ncontrastive CLIP loss, in which a shared image-text embedding space is learned, taken from [22].\\nFeeding in the Embedding.\\nSuppose now that we have obtained our embedding vector y ∈Rdy. Now what? The\\nanswer varies, but usually it is some variant of the following: feed it individually into every sub-component of the\\narchitecture for images. Let us briefly describe how this is accomplished in the U-Net implementation used in lab\\nthree, as depicted in fig. 13. At some intermediate point within the network, we would like to inject information\\nfrom y ∈Rdy into the current activation xintermediate\\nt\\n∈RC×H×W . We might do so using the procedure below,\\ngiven in PyTorch-esque pseudocode.\\ny = MLP(y) ∈RC\\n▶Map y from Rdy to RC.\\ny = reshape(y) ∈RC×1×1\\n▶Reshape y to “look” like an image.\\nxintermediate\\nt\\n= broadcast_add(xintermediate\\nt\\n, y) ∈RC×H×W\\n▶Add y to xintermediate\\nt\\npointwise.\\nOne exception to this simple-pointwise conditioning scheme is when we have a sequence of embeddings as\\nproduced by some pretrained language model. In this case, we might consider using some sort of cross-attention\\nscheme between our image (suitably patchified) and the tokens of the embedded sequence. We will see multiple\\nexamples of this in section 5.3.\\n5.3\\nA Survey of Large-Scale Image and Video Models\\nWe conclude this section by briefly examining two large-scale generative models: Stable Diffusion 3 for image\\ngeneration and Meta’s Movie Gen Video for video generation [7, 21].\\nAs you will see, these models use the\\ntechniques we have described in this work along with additional architectural enhancements to both scale and\\naccommodate richly structured conditioning modalities, such as text-based input.\\n46'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 46}, page_content='5.3\\nA Survey of Large-Scale Image and Video Models\\n5.3.1\\nStable Diffusion 3\\nStable Diffusion is a series of state-of-the-art image generation models. These models were among the first to use\\nlarge-scale latent diffusion models for image generation. If you have not done so, we highly recommend testing it\\nfor yourself online (https://stability.ai/news/stable-diffusion-3).\\nStable Diffusion 3 uses the same conditional flow matching objective that we study in this work (see algo-\\nrithm 5).3 As outlined in their paper, they extensively tested various flow and diffusion alternatives and found\\nflow matching to perform best. For training, it uses classifier-free guidance training (with dropping class labels)\\nas outlined above. Further, Stable Diffusion 3 follows the approach outlined in section 5.2 by training within the\\nlatent space of a pre-trained autoencoder. Training a good autoencoder was a big contribution of the first stable\\ndiffusion papers.\\nTo enhance text conditioning, Stable Diffusion 3 makes use of both 3 different types of text embeddings (including\\nCLIP embeddings as well as the sequential outputs produced by a pretrained instance of the encoder of Google’s\\nT5-XXL [23], and similar to approaches taken in [3, 26]). Whereas CLIP embeddings provide a coarse, overarching\\nembedding of the input text, the T5 embeddings provide a more granular level of context, allowing for the possibility\\nof the model attending to particular elements of the conditioning text. To accommodate these sequential context\\nembeddings, the authors then propose to extend the diffusion transformer to attend not just to patches of the\\nimage, but to the text embeddings as well, thereby extending the conditioning capacity from the class-based\\nscheme originally proposed for DiT to sequential context embeddings. This proposed modified DiT is referred to\\nas a multi-modal DiT (MM-DiT), and is depicted in fig. 15. Their final, largest model has 8 billion parameters.\\nFor sampling, they use 50 steps (i.e. they have to evaluate the network 50 times) using a Euler simulation scheme\\nand a classifier-free guidance weight between 2.0-5.0.\\n5.3.2\\nMeta Movie Gen Video\\nNext, we discuss Meta’s video generator, Movie Gen Video (https://ai.meta.com/research/movie-gen/). As the\\ndata are not images but videos, the data x lie in the space RT ×C×H×W where T represents the new temporal\\ndimension (i.e. the number of frames). As we shall see, many of the design choices made in this video setting can\\nbe seen as adapting existing techniques (e.g., autoencoders, diffusion transformers, etc.) from the image setting to\\nhandle this extra temporal dimension.\\nMovie Gen Video utilizes the conditional flow matching objective with the same CondOT path (see algorithm 5).\\nLike Stable Diffusion 3, Movie Gen Video also operates in the latent space of frozen, pretrained autoencoder. Note\\nthat the autoencoder to reduce memory consumption is even more important for videos than for images - which\\nis why most video generators right now are pretty limited in the length of the video they generate. Specifically,\\nthe authors propose to handle the added time dimension by introducing a temporal autoencoder (TAE) which\\nmaps a raw video x′\\nt ∈RT ′×3×H×W to a latent xt ∈RT ×C×H×W , with T ′\\nT = H′\\nH = W ′\\nW = 8 [21]. To accomodate\\nlong videos, a temporal tiling procedure is proposed by which the video is chopped up into pieces, each piece is\\nencoder separately, and the latents are sticthed together [21]. The model itself - that is, uθ\\nt (xt) - is given by a\\n3In their work, they use a different convention to condition on the noise. But this is only notation and the algorithm is the same.\\n47'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 47}, page_content='5.3\\nA Survey of Large-Scale Image and Video Models\\nFigure 15: The architecture of the multi-modal diffusion transformer (MM-DiT) proposed in [7]. Figure also taken\\nfrom [7].\\nDiT-like backbone in which xt is patchified along the time and space dimensions. The image patches are then\\npassed through a transformer employing both self-attention among the image patches, and cross-attention with\\nlanguage model embeddings, similar to the MM-DiT employed by Stable Diffusion 3. For text conditioning, Movie\\nGen Video employs three types of text embeddings: UL2 embeddings, for granular, text-based reasoning [33],\\nByT5 embeddings, for attending to character-level details (for e.g., prompts explicitly requesting specific text to\\nbe present) [36], and MetaCLIP embeddings, trained in a shared text-image embedding space [13, 21]. Their final,\\nlargest model has 30 billion parameters. For a significantly more detailed and expansive treatment, we encourage\\nthe reader to check out the Movie Gen technical report itself [21].\\n48'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 48}, page_content='6\\nAcknowledgements\\nThis course would not have been possible without the generous support of many others. We would like to thank\\nTommi Jaakkola for serving as the advisor and faculty sponsor for this course, and for thoughtful feedback through-\\nout the process. We would like to thank Christian Fiedler, Tim Griesbach, Benedikt Geiger, and Albrecht Holder-\\nrieth for value feedback on the lecture notes. Further, we thank Elaine Mello from MIT Open Learning for support\\nwith lecture recordings and Ashay Athalye from Students for Open and Universal Learning for helping to cut and\\nprocess the videos. We would additionally like to thank Cameron Diao, Tally Portnoi, Andi Qu, Roger Trullo,\\nÁdám Burián, Zewen Yang, and many others for their invaluable contributions to the labs. We would also like to\\nthank Lisa Bella, Ellen Reid, and everyone else at MIT EECS for their generous support. Finally, we would like to\\nthank all participants in the original course offering (MIT 6.S184/6.S975, taught over IAP 2025), as well as readers\\nlike you for your interest in this class. Thanks!\\n7\\nReferences\\n[1]\\nMichael S Albergo, Nicholas M Boffi, and Eric Vanden-Eijnden. “Stochastic interpolants: A unifying frame-\\nwork for flows and diffusions”. In: arXiv preprint arXiv:2303.08797 (2023).\\n[2]\\nBrian DO Anderson. “Reverse-time diffusion equation models”. In: Stochastic Processes and their Applications\\n12.3 (1982), pp. 313–326.\\n[3]\\nYogesh Balaji et al. eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers. 2023.\\narXiv: 2211.01324 [cs.CV]. url: https://arxiv.org/abs/2211.01324.\\n[4]\\nEarl A Coddington, Norman Levinson, and T Teichmann. Theory of ordinary differential equations. 1956.\\n[5]\\nPrafulla Dhariwal and Alex Nichol. Diffusion Models Beat GANs on Image Synthesis. 2021. arXiv: 2105.05233\\n[cs.LG]. url: https://arxiv.org/abs/2105.05233.\\n[6]\\nAlexey Dosovitskiy et al. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.\\n2021. arXiv: 2010.11929 [cs.CV]. url: https://arxiv.org/abs/2010.11929.\\n[7]\\nPatrick Esser et al. Scaling Rectified Flow Transformers for High-Resolution Image Synthesis. 2024. arXiv:\\n2403.03206 [cs.CV]. url: https://arxiv.org/abs/2403.03206.\\n[8]\\nLawrence C Evans. Partial differential equations. Vol. 19. American Mathematical Society, 2022.\\n[9]\\nJonathan Ho, Ajay Jain, and Pieter Abbeel. “Denoising diffusion probabilistic models”. In: Advances in neural\\ninformation processing systems 33 (2020), pp. 6840–6851.\\n[10]\\nJonathan Ho and Tim Salimans. Classifier-Free Diffusion Guidance. 2022. arXiv: 2207.12598 [cs.LG]. url:\\nhttps://arxiv.org/abs/2207.12598.\\n[11]\\nArieh Iserles. A first course in the numerical analysis of differential equations. Cambridge university press,\\n2009.\\n[12]\\nTero Karras et al. “Elucidating the design space of diffusion-based generative models”. In: Advances in Neural\\nInformation Processing Systems 35 (2022), pp. 26565–26577.\\n49'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 49}, page_content='[13]\\nSamuel Lavoie et al. Modeling Caption Diversity in Contrastive Vision-Language Pretraining. 2024. arXiv:\\n2405.00740 [cs.CV]. url: https://arxiv.org/abs/2405.00740.\\n[14]\\nYaron Lipman et al. “Flow matching for generative modeling”. In: arXiv preprint arXiv:2210.02747 (2022).\\n[15]\\nYaron Lipman et al. “Flow Matching Guide and Code”. In: arXiv preprint arXiv:2412.06264 (2024).\\n[16]\\nXingchao Liu, Chengyue Gong, and Qiang Liu. “Flow straight and fast: Learning to generate and transfer\\ndata with rectified flow”. In: arXiv preprint arXiv:2209.03003 (2022).\\n[17]\\nNanye Ma et al. “Sit: Exploring flow and diffusion-based generative models with scalable interpolant trans-\\nformers”. In: arXiv preprint arXiv:2401.08740 (2024).\\n[18]\\nXuerong Mao. Stochastic differential equations and applications. Elsevier, 2007.\\n[19]\\nWilliam Peebles and Saining Xie. Scalable Diffusion Models with Transformers. 2023. arXiv: 2212.09748\\n[cs.CV]. url: https://arxiv.org/abs/2212.09748.\\n[20]\\nLawrence Perko. Differential equations and dynamical systems. Vol. 7. Springer Science & Business Media,\\n2013.\\n[21]\\nAdam Polyak et al. Movie Gen: A Cast of Media Foundation Models. 2024. arXiv: 2410.13720 [cs.CV]. url:\\nhttps://arxiv.org/abs/2410.13720.\\n[22]\\nAlec Radford et al. Learning Transferable Visual Models From Natural Language Supervision. 2021. arXiv:\\n2103.00020 [cs.CV]. url: https://arxiv.org/abs/2103.00020.\\n[23]\\nColin Raffel et al. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. 2023.\\narXiv: 1910.10683 [cs.LG]. url: https://arxiv.org/abs/1910.10683.\\n[24]\\nRobin Rombach et al. High-Resolution Image Synthesis with Latent Diffusion Models. 2022. arXiv: 2112.10752\\n[cs.CV]. url: https://arxiv.org/abs/2112.10752.\\n[25]\\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. “U-net: Convolutional networks for biomedical image\\nsegmentation”. In: Medical image computing and computer-assisted intervention–MICCAI 2015: 18th inter-\\nnational conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18. Springer. 2015, pp. 234–\\n241.\\n[26]\\nChitwan Saharia et al. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.\\n2022. arXiv: 2205.11487 [cs.CV]. url: https://arxiv.org/abs/2205.11487.\\n[27]\\nSimo Särkkä and Arno Solin. Applied stochastic differential equations. Vol. 10. Cambridge University Press,\\n2019.\\n[28]\\nJascha Sohl-Dickstein et al. “Deep unsupervised learning using nonequilibrium thermodynamics”. In: Inter-\\nnational conference on machine learning. PMLR. 2015, pp. 2256–2265.\\n[29]\\nYang Song and Stefano Ermon. “Generative modeling by estimating gradients of the data distribution”. In:\\nAdvances in neural information processing systems 32 (2019).\\n[30]\\nYang Song et al. Score-Based Generative Modeling through Stochastic Differential Equations. 2021. arXiv:\\n2011.13456 [cs.LG]. url: https://arxiv.org/abs/2011.13456.\\n50'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 50}, page_content='[31]\\nYang Song et al. “Score-Based Generative Modeling through Stochastic Differential Equations”. In: Interna-\\ntional Conference on Learning Representations (ICLR). 2021.\\n[32]\\nYang Song et al. “Score-based generative modeling through stochastic differential equations”. In: arXiv\\npreprint arXiv:2011.13456 (2020).\\n[33]\\nYi Tay et al. UL2: Unifying Language Learning Paradigms. 2023. arXiv: 2205.05131 [cs.CL]. url: https:\\n//arxiv.org/abs/2205.05131.\\n[34]\\nArash Vahdat, Karsten Kreis, and Jan Kautz. “Score-based generative modeling in latent space”. In: Advances\\nin neural information processing systems 34 (2021), pp. 11287–11302.\\n[35]\\nAshish Vaswani et al. Attention Is All You Need. 2023. arXiv: 1706.03762 [cs.CL]. url: https://arxiv.org/\\nabs/1706.03762.\\n[36]\\nLinting Xue et al. ByT5: Towards a token-free future with pre-trained byte-to-byte models. 2022. arXiv: 2105.\\n13626 [cs.CL]. url: https://arxiv.org/abs/2105.13626.\\n51'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 51}, page_content='A\\nA Reminder on Probability Theory\\nWe present a brief overview of basic concepts from probability theory. This section was partially taken from [15].\\nA.1\\nRandom vectors\\nConsider data in the d-dimensional Euclidean space x = (x1, . . . , xd) ∈Rd with the standard Euclidean inner\\nproduct ⟨x, y⟩= Pd\\ni=1 xiyi and norm ∥x∥=\\np\\n⟨x, x⟩. We will consider random variables (RVs) X ∈Rd with\\ncontinuous probability density function (PDF), defined as a continuous function pX : Rd →R≥0 providing event\\nA with probability\\nP(X ∈A) =\\nZ\\nA\\npX(x)dx,\\n(80)\\nwhere\\nR\\npX(x)dx = 1. By convention, we omit the integration interval when integrating over the whole space\\n(\\nR\\n≡\\nR\\nRd). To keep notation concise, we will refer to the PDF pXt of RV Xt as simply pt. We will use the notation\\nX ∼p or X ∼p(X) to indicate that X is distributed according to p. One common PDF in generative modeling is\\nthe d-dimensional isotropic Gaussian:\\nN(x; µ, σ2I) = (2πσ2)−d\\n2 exp\\n \\n−∥x −µ∥2\\n2\\n2σ2\\n!\\n,\\n(81)\\nwhere µ ∈Rd and σ ∈R>0 stand for the mean and the standard deviation of the distribution, respectively.\\nThe expectation of a RV is the constant vector closest to X in the least-squares sense:\\nE [X] = arg min\\nz∈Rd\\nZ\\n∥x −z∥2 pX(x)dx =\\nZ\\nxpX(x)dx.\\n(82)\\nOne useful tool to compute the expectation of functions of RVs is the law of the unconscious statistician:\\nE [f(X)] =\\nZ\\nf(x)pX(x)dx.\\n(83)\\nWhen necessary, we will indicate the random variables under expectation as EXf(X).\\nA.2\\nConditional densities and expectations\\nFigure 16: Joint PDF pX,Y\\n(in shades) and its marginals\\npX and pY (in black lines).\\nFigure from [15]\\n.\\nGiven two random variables X, Y ∈Rd, their joint PDF pX,Y (x, y) has marginals\\nZ\\npX,Y (x, y)dy = pX(x) and\\nZ\\npX,Y (x, y)dx = pY (y).\\n(84)\\nSee fig. 16 for an illustration of the joint PDF of two RVs in R (d = 1).\\nThe\\nconditional PDF pX|Y describes the PDF of the random variable X when conditioned\\non an event Y = y with density pY (y) > 0:\\npX|Y (x|y) := pX,Y (x, y)\\npY (y)\\n,\\n(85)\\n52'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 52}, page_content='A.2\\nConditional densities and expectations\\nand similarly for the conditional PDF pY |X. Bayes’ rule expresses the conditional\\nPDF pY |X with pX|Y by\\npY |X(y|x) = pX|Y (x|y)pY (y)\\npX(x)\\n,\\n(86)\\nfor pX(x) > 0.\\nThe conditional expectation E [X|Y ] is the best approximating function g⋆(Y ) to X in the least-squares sense:\\ng⋆:= arg min\\ng:Rd→Rd E\\nh\\n∥X −g(Y )∥2i\\n= arg min\\ng:Rd→Rd\\nZ\\n∥x −g(y)∥2 pX,Y (x, y)dxdy\\n= arg min\\ng:Rd→Rd\\nZ \\x14Z\\n∥x −g(y)∥2 pX|Y (x|y)dx\\n\\x15\\npY (y)dy.\\n(87)\\nFor y ∈Rd such that pY (y) > 0 the conditional expectation function is therefore\\nE [X|Y = y] := g⋆(y) =\\nZ\\nxpX|Y (x|y)dx,\\n(88)\\nwhere the second equality follows from taking the minimizer of the inner brackets in eq. (87) for Y = y, similarly\\nto eq. (82). Composing g⋆with the random variable Y , we get\\nE [X|Y ] := g⋆(Y ),\\n(89)\\nwhich is a random variable in Rd. Rather confusingly, both E [X|Y = y] and E [X|Y ] are often called conditional\\nexpectation, but these are different objects. In particular, E [X|Y = y] is a function Rd →Rd, while E [X|Y ] is a\\nrandom variable assuming values in Rd. To disambiguate these two terms, our discussions will employ the notations\\nintroduced here.\\nThe tower property is an useful property that helps simplify derivations involving conditional expectations of\\ntwo RVs X and Y :\\nE [E [X|Y ]] = E [X]\\n(90)\\nBecause E [X|Y ] is a RV, itself a function of the RV Y , the outer expectation computes the expectation of E [X|Y ].\\nThe tower property can be verified by using some of the definitions above:\\nE [E [X|Y ]] =\\nZ \\x12Z\\nxpX|Y (x|y)dx\\n\\x13\\npY (y)dy\\n(85)\\n=\\nZ Z\\nxpX,Y (x, y)dxdy\\n(84)\\n=\\nZ\\nxpX(x)dx\\n= E [X] .\\nFinally, consider a helpful property involving two RVs f(X, Y ) and Y , where X and Y are two arbitrary RVs.\\nThen, by using the Law of the Unconscious Statistician with (88), we obtain the identity\\nE [f(X, Y )|Y = y] =\\nZ\\nf(x, y)pX|Y (x|y)dx.\\n(91)\\n53'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 53}, page_content='B\\nA Proof of the Fokker-Planck equation\\nIn this section, we give here a self-contained proof of the Fokker-Planck equation (theorem 15) which includes the\\ncontinuity equation as a special case (theorem 12). We stress that this section is not necessary to understand\\nthe remainder of this document and is mathematically more advanced. If you desire to understand where the\\nFokker-Planck equation comes from, then this section is for you.\\nWe start by showing that the Fokker-Planck is a necessary condition, i.e. if Xt ∼pt, then the Fokker-Planck\\nequation is fulfilled. The trick for the proof is to use test functions f, i.e. functions f : Rd →R that are infinitely\\ndifferentiable (\"smooth\") and are only non-zero within a bounded domain (compact support). We use the fact that\\nfor arbitrary integrable functions g1, g2 : Rd →R it holds that\\ng1(x) = g2(x) for all x ∈Rd\\n⇔\\nZ\\nf(x)g1(x)dx =\\nZ\\nf(x)g2(x)dx for all test functions f\\n(92)\\nIn other words, we can express the pointwise equality as equality of taking integrals. The useful thing about test\\nfunctions is that they are smooth, i.e. we can take gradients and higher-order derivatives. In particular, we can\\nuse integration by parts for arbitrary test functions f1, f2:\\nZ\\nf1(x) ∂\\n∂xi\\nf2(x)dx = −\\nZ\\nf2(x) ∂\\n∂xi\\nf1(x)dx\\n(93)\\nBy using this together with the definition of the divergence and Laplacian (see eq. (23)), we get the identities:\\nZ\\n∇f T\\n1 (x)f2(x)dx = −\\nZ\\nf1(x)div(f2)(x)dx\\n(f1 : Rd →R, f2 : Rd →Rd)\\n(94)\\nZ\\nf1(x)∆f2(x)dx =\\nZ\\nf2(x)∆f1(x)dx\\n(f1 : Rd →R, f2 : Rd →R)\\n(95)\\nNow let’s proceed to the proof. We use the stochastic update of SDE trajectories as in eq. (6):\\nXt+h =Xt + hut(Xt) + σt(Wt+h −Wt) + hRt(h)\\n(96)\\n≈Xt + hut(Xt) + σt(Wt+h −Wt)\\n(97)\\nwhere for now we simply ignore the error term Rt(h) for readability as we will take h →0 anyway. We can then\\nmake the following calculation:\\nf(Xt+h) −f(Xt)\\n(97)\\n= f(Xt + hut(Xt) + σt(Wt+h −Wt)) −f(Xt)\\n(i)\\n=∇f(Xt)T (hut(Xt) + σt(Wt+h −Wt)))\\n+ 1\\n2 (hut(Xt) + σt(Wt+h −Wt)))T ∇2f(Xt) (hut(Xt) + σt(Wt+h −Wt)))\\n(ii)\\n= h∇f(Xt)T ut(Xt) + σt∇f(Xt)T (Wt+h −Wt)\\n+ 1\\n2h2ut(Xt)T ∇2f(Xt)ut(Xt) + hσtut(Xt)T ∇2f(Xt)(Wt+h −Wt) + 1\\n2σ2\\nt (Wt+h −Wt)T ∇2f(Xt)(Wt+h −Wt)\\n54'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 54}, page_content='where in (i) we used a 2nd Taylor approximation of f around Xt and in (ii) we used the fact that the Hessian ∇2f\\nis a symmetric matrix. Note that E[Wt+h −Wt|Xt] = 0 and Wt+h −Wt|Xt ∼N(0, hId). Therefore\\nE[f(Xt+h) −f(Xt)|Xt]\\n=h∇f(Xt)T ut(Xt) + 1\\n2h2ut(Xt)T ∇2f(Xt)ut(Xt) + h\\n2 σ2\\nt Eϵt∼N(0,Id)[ϵT\\nt ∇2f(Xt)ϵt]\\n(i)\\n=h∇f(Xt)T ut(Xt) + 1\\n2h2ut(Xt)T ∇2f(Xt)ut(Xt) + h\\n2 σ2\\nt trace(∇2f(Xt))\\n(ii)\\n= h∇f(Xt)T ut(Xt) + 1\\n2h2ut(Xt)T ∇2f(Xt)ut(Xt) + h\\n2 σ2\\nt ∆f(Xt)\\nwhere in (i) we used the fact that Eϵt∼N(0,Id)[ϵT\\nt Aϵt] = trace(A) and in (ii) we used the definition of the Laplacian\\nand the Hessian matrix. With this, we get that\\n∂tE[f(Xt)]\\n= lim\\nh→0\\n1\\nhE[f(Xt+h) −f(Xt)]\\n= lim\\nh→0\\n1\\nhE[E[f(Xt+h) −f(Xt)|Xt]]\\n=E[lim\\nh→0\\n1\\nh\\n\\x12\\nh∇f(Xt)T ut(Xt) + 1\\n2h2ut(Xt)T ∇2f(Xt)ut(Xt) + h\\n2 σ2\\nt ∆f(Xt)\\n\\x13\\n]\\n=E[∇f(Xt)T ut(Xt) + 1\\n2σ2\\nt ∆f(Xt)]\\n(i)\\n=\\nZ\\n∇f(x)T ut(x)pt(x)dx +\\nZ 1\\n2σ2\\nt ∆f(x)pt(x)dx\\n(ii)\\n= −\\nZ\\nf(x)div(utpt)(x)dx +\\nZ 1\\n2σ2\\nt f(x)∆pt(x)dx\\n=\\nZ\\nf(x)\\n\\x12\\n−div(utpt)(x) + 1\\n2σ2\\nt ∆pt(x)\\n\\x13\\ndx\\nwhere in (i) we used the assumption that pt as the distribution of Xt and in (ii) we used eq. (94) and eq. (95).\\nTherefore, it holds that\\n∂tE[f(Xt)] =\\nZ\\nf(x)\\n\\x12\\n−div(ptut)(x) + σ2\\nt\\n2 ∆pt(x)\\n\\x13\\ndx\\n(for all f and 0 ≤t ≤1)\\n(98)\\n(i)\\n⇔\\n∂t\\nZ\\nf(x)pt(x)dx =\\nZ\\nf(x)\\n\\x12\\n−div(ptut)(x) + σ2\\nt\\n2 ∆pt(x)\\n\\x13\\ndx\\n(for all f and 0 ≤t ≤1)\\n(99)\\n(ii)\\n⇔\\nZ\\nf(x)∂tpt(x)dx =\\nZ\\nf(x)\\n\\x12\\n−div(ptut)(x) + σ2\\nt\\n2 ∆pt(x)\\n\\x13\\ndx\\n(for all f and 0 ≤t ≤1)\\n(100)\\n(iii)\\n⇔\\n∂tpt(x) = −div(ptut)(x) + σ2\\nt\\n2 ∆pt(x)\\n(for all x ∈Rd, 0 ≤t ≤1)\\n(101)\\nwhere in (i) we used the assumption that Xt ∼pt, in (ii) we swapped the derivative with the integral and (iii) we\\nused eq. (92) . This completes the proof that the Fokker-Planck equation is a necessary condition.\\nFinally, we explain why it is also a sufficient condition. The Fokker-Planck equation is a partial differential\\nequation (PDE). More specifically, it is a so-called parabolic partial differential equation. Similar to theorem 3,\\n55'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'trapped': '', 'modDate': 'D:20250603021440Z', 'creationDate': 'D:20250603021440Z', 'page': 55}, page_content='such differential equations have a unique solution given fixed initial conditions (see e.g. [8, Chapter 7]). Now, if\\neq. (30) holds for pt, we just shown above that it must also hold for true distribution qt of Xt (i.e. Xt ∼qt) - in\\nother words, both pt and qt are solutions to the parabolic PDE. Further, we know that the initial conditions are\\nthe same, i.e. p0 = q0 = pinit by construction of an interpolating probability path (see ??). Hence, by uniqueness\\nof the solution of the differential equation, we know that pt = qt for all 0 ≤t ≤1 - which means Xt ∼qt = pt and\\nwhich is what we wanted to show.\\n56'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'format': 'PDF 1.4', 'title': '20250122_Lecture_2', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='Lecture 2\\nConstructing a Training Target for Flow and Diffusion Models\\nMIT IAP 2025 | Jan 22, 2025\\nPeter Holderrieth and Ezra Erives\\nSponsor: Tommi Jaakkola'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'format': 'PDF 1.4', 'title': '20250122_Lecture_2', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='Reminder: Flow and Diffusion Models \\nNeural network\\nvector field\\nInitialize:\\nODE:\\nInitialize:\\nSDE:\\nDiffusion coeff.\\nFlow \\nModel\\nDiffusion \\nModel\\nE.g. Gaussian\\nTo get samples, simulate ODE/SDE from t=0 to t=1 and return'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'format': 'PDF 1.4', 'title': '20250122_Lecture_2', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='Next step: Training the model\\nWithout training, the model produces “non-sense”→ We need to train \\nGoal of lecture 2 (today) and lecture 3 (tomorrow): \\nDerive training algorithm\\nImplies\\nTraining = Finding parameters         such that \\nStart with initial \\ndistribution\\nFollow along \\nthe vector field\\nThe distribution of \\nthe final point = data \\ndistribution'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'format': 'PDF 1.4', 'title': '20250122_Lecture_2', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='Today’s goal: Derive a Training Target\\n-\\nTypically, we train the model by minimizing a mean squared \\nerror:\\nToday: Derive a formula for the training target:\\nTomorrow: Training algorithm using \\n-\\nIn regression or classification, the training target is the label.\\n-\\nHere: No label :( → We have to derive a training target\\nTraining target'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'format': 'PDF 1.4', 'title': '20250122_Lecture_2', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='Marginal Vector Field\\nMarginal Score Function\\nFlow Matching\\nScore Matching\\nToday: Training target\\nTomorrow: Training algorithm'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'format': 'PDF 1.4', 'title': '20250122_Lecture_2', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='Section 2:\\nConstructing a Training Target\\nGoal: Derive a formula for a training target for training our \\nmodels'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'format': 'PDF 1.4', 'title': '20250122_Lecture_2', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='Today will be the technically most challenging lecture! \\nThe next lectures will be much much easier!\\nYou do not have to understand the derivations. \\nMake sure you understand the formulas for:\\nConditional \\nProbability Path\\nMarginal \\nProbability Path\\nConditional \\nVector Field\\nMarginal \\nVector Field\\nConditional \\nScore Function\\nMarginal \\nScore Function'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'format': 'PDF 1.4', 'title': '20250122_Lecture_2', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='Key terminology:  \\n“Conditional” = “Per single data point”\\n“Marginal” = “Across distribution of data points”'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'format': 'PDF 1.4', 'title': '20250122_Lecture_2', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='Probability Paths: The Path from Noise to Data'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'format': 'PDF 1.4', 'title': '20250122_Lecture_2', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='Conditional Probability Path\\nMarginal Probability Path\\nt=0\\nt=1'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'format': 'PDF 1.4', 'title': '20250122_Lecture_2', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='Conditional Probability Path\\nMarginal Probability Path'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'format': 'PDF 1.4', 'title': '20250122_Lecture_2', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='Conditional Probability Path\\nMarginal Probability Path'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'format': 'PDF 1.4', 'title': '20250122_Lecture_2', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content='Conditional Prob. Path\\nConditional \\nProbability Path\\nGaussian example\\nConditional \\nVector Field\\nConditional \\nScore \\nFunction\\nNotation\\nKey property\\nODE follows \\nconditional path \\nGradient of \\nlog-likelihood\\nInterpolates     \\nand a data point z'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'format': 'PDF 1.4', 'title': '20250122_Lecture_2', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='Marginal Prob. Path\\nMarginal \\nProbability   \\nPath\\nFormula\\nMarginal \\nVector \\nField\\nMarginal \\nScore \\nFunction\\nNotation\\nKey property\\nODE follows \\nmarginal path \\nGradient of \\nlog-likelihood \\nof marginal \\npath\\nInterpolates     \\nand'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'format': 'PDF 1.4', 'title': '20250122_Lecture_2', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 14}, page_content='Simulating ODE \\nwith Conditional \\nVector Field for \\nConditional \\nProbability Path\\nFigure credit: \\nYaron Lipman\\nNOTE: This is an \\nanimated gif and is \\nstatic in a PDF'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'format': 'PDF 1.4', 'title': '20250122_Lecture_2', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 15}, page_content='Ground truth\\nODE samples\\nODE Trajectories'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'format': 'PDF 1.4', 'title': '20250122_Lecture_2', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16}, page_content='Continuity Equation\\nGiven:\\nRandomly initialized ODE\\nContinuity equation holds\\nFollow probability path:\\nequivalent\\nMarginals are \\np_t\\nPDE holds'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'format': 'PDF 1.4', 'title': '20250122_Lecture_2', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 17}, page_content='Continuity Equation\\nOutflow\\nInflow\\nChange of \\nprobability \\nmass at x\\nOutflow - inflow\\nof probability \\nmass from u'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'format': 'PDF 1.4', 'title': '20250122_Lecture_2', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 18}, page_content='Gaussian\\nConditional\\nProbability Path\\nAnd\\nConditional\\nVector Field\\nFigure credit: \\nYaron Lipman'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'format': 'PDF 1.4', 'title': '20250122_Lecture_2', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 19}, page_content='Toy example\\nFigure credit: \\nYaron Lipman\\nNOTE: This is an \\nanimated gif and \\nis static in a PDF'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'format': 'PDF 1.4', 'title': '20250122_Lecture_2', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 20}, page_content='Simulating ODE \\nwith Marginal \\nVector Field for \\nGaussian \\nProbability Path\\nFigure credit: \\nYaron Lipman'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'format': 'PDF 1.4', 'title': '20250122_Lecture_2', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 21}, page_content='Conditional Prob. Path, Vector Field, and Score\\nConditional \\nProbability Path\\nGaussian example\\nConditional \\nVector Field\\nConditional \\nScore \\nFunction\\nNotation\\nKey property\\nODE follows \\nconditional path \\nGradient of \\nlog-likelihood\\nInterpolates     \\nand a data point z'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'format': 'PDF 1.4', 'title': '20250122_Lecture_2', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 22}, page_content='Marginal Prob. Path, Vector Field, and Score\\nMarginal \\nProbability   \\nPath\\nFormula\\nMarginal \\nVector \\nField\\nMarginal \\nScore \\nFunction\\nNotation\\nKey property\\nODE follows \\nmarginal path \\nGradient of \\nlog-likelihood \\nof marginal \\npath\\nInterpolates     \\nand'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'format': 'PDF 1.4', 'title': '20250122_Lecture_2', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 23}, page_content='Outlook (Next class) - Flow Matching Loss\\nThe Flow Matching loss is a mean squared error between the neural \\nnetwork and the marginal vector field: \\nTraining a Flow Model Consists of Learning the Marginal \\nVector Field (How? Next lecture!)'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'format': 'PDF 1.4', 'title': '20250122_Lecture_2', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 24}, page_content='Example marginal vector field - Meta MovieGen\\nThese videos are generated by simulating the ODE with \\nthe (learnt) marginal vector field'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'format': 'PDF 1.4', 'title': '20250122_Lecture_2', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 25}, page_content='Ground truth\\nSDE samples\\nSDE Trajectories'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'format': 'PDF 1.4', 'title': '20250122_Lecture_2', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 26}, page_content='Fokker-Planck equation\\nGiven:\\nRandomly initialized SDE\\nFokker-Planck equation holds\\nFollow probability path:\\nequivalent\\nMarginals are \\np_t\\nHeat  equ.\\nContinuity equ.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'format': 'PDF 1.4', 'title': '20250122_Lecture_2', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 27}, page_content='Continuity Equation\\nOutflow\\nInflow\\nChange of \\nprobability \\nmass at x\\nOutflow - inflow\\nof probability \\nmass from u'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'format': 'PDF 1.4', 'title': '20250122_Lecture_2', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 28}, page_content='Fokker-Planck Equation\\nDispersion\\nChange of \\nprobability \\nmass at x\\nHeat dispersion'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'format': 'PDF 1.4', 'title': '20250122_Lecture_2', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 29}, page_content='Outlook (Next class) - Score Matching Loss\\nThe Score Matching loss is a mean squared error between the neural \\nnetwork and the marginal score function:\\nTo train a diffusion model, we need to train the score network \\nby minimizing the score matching loss (How? Next class!)'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'format': 'PDF 1.4', 'title': '20250122_Lecture_2', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 30}, page_content='Marginal VF\\nMarginal VF + Score'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'format': 'PDF 1.4', 'title': '20250122_Lecture_2', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 31}, page_content='Training a Diffusion Model = Learning the Score Function\\nConversion of \\nof noise into \\nprotein \\nstructure by \\nmarginal \\nvector field \\nSlide credit: \\nJason Yim\\nNOTE: This is \\nan animated \\ngif and is \\nstatic in a \\nPDF'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'format': 'PDF 1.4', 'title': '20250122_Lecture_2', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 32}, page_content='Conditional Prob. Path, Vector Field, and Score\\nConditional \\nProbability Path\\nGaussian example\\nConditional \\nVector Field\\nConditional \\nScore \\nFunction\\nNotation\\nKey property\\nODE follows \\nconditional path \\nGradient of \\nlog-likelihood\\nInterpolates     \\nand a data point z'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'format': 'PDF 1.4', 'title': '20250122_Lecture_2', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 33}, page_content='Marginal Prob. Path, Vector Field, and Score\\nMarginal \\nProbability   \\nPath\\nFormula\\nMarginal \\nVector \\nField\\nMarginal \\nScore \\nFunction\\nNotation\\nKey property\\nODE follows \\nmarginal path \\nCan be used \\nto convert \\nODE target \\nto SDE\\nInterpolates     \\nand'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'format': 'PDF 1.4', 'title': '20250122_Lecture_2', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 34}, page_content='Today was the technically most challenging lecture! \\nThe next lectures will be much much easier!\\nMake sure you understand the formulas for:\\nThese 6 formulas is all we need for training!\\nConditional \\nProbability Path\\nMarginal \\nProbability Path\\nConditional \\nVector Field\\nMarginal \\nVector Field\\nConditional \\nScore Function\\nMarginal \\nScore Function'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'format': 'PDF 1.4', 'title': '20250122_Lecture_2', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 35}, page_content='Next class:\\nThursday (Tomorrow), 11am-12:30pm\\nTraining algorithm!\\nE25-111 (same room)\\nOffice hours: Today, 3pm-4:30pm in 37-212'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-09T02:33:09+00:00', 'source': '..\\\\data\\\\PDFs\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Transformers.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-02-09T02:33:09+00:00', 'trapped': '', 'modDate': 'D:20240209023309Z', 'creationDate': 'D:20240209023309Z', 'page': 0}, page_content='An Introduction to Transformers\\nRichard E. Turner\\nDepartment of Engineering, University of Cambridge, UK\\nMicrosoft Research, Cambridge, UK\\nret26@cam.ac.uk\\nAbstract. The transformer is a neural network component that can be used to learn useful represen-\\ntations of sequences or sets of data-points [Vaswani et al., 2017]. The transformer has driven recent\\nadvances in natural language processing [Devlin et al., 2019], computer vision [Dosovitskiy et al., 2021],\\nand spatio-temporal modelling [Bi et al., 2022]. There are many introductions to transformers, but most\\ndo not contain precise mathematical descriptions of the architecture and the intuitions behind the design\\nchoices are often also missing.1 Moreover, as research takes a winding path, the explanations for the\\ncomponents of the transformer can be idiosyncratic. In this note we aim for a mathematically precise,\\nintuitive, and clean description of the transformer architecture. We will not discuss training as this is\\nrather standard. We assume that the reader is familiar with fundamental topics in machine learning\\nincluding multi-layer perceptrons, linear transformations, softmax functions and basic probability.\\n1See Phuong and Hutter [2022] for an exception to this.\\narXiv:2304.10557v5  [cs.LG]  8 Feb 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-09T02:33:09+00:00', 'source': '..\\\\data\\\\PDFs\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Transformers.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-02-09T02:33:09+00:00', 'trapped': '', 'modDate': 'D:20240209023309Z', 'creationDate': 'D:20240209023309Z', 'page': 1}, page_content='Figure 1: The input to a transformer is N vectors\\nx(0)\\nn\\nwhich are each D dimensional. These can\\nbe collected together into an array X(0).\\n1 Strictly speaking, the collection of tokens does\\nnot need to have an order and the transformer\\ncan handle them as a set (where order does not\\nmatter), rather than a sequence. See section 3.\\n2 Note that much of the literature uses the trans-\\nposed notation whereby the data matrix is N×D,\\nbut I want sequences to run across the page and\\nfeatures down it in the schematics (a convention\\nI use in other lecture notes).\\nFigure 2: Encoding an image: an example [Doso-\\nvitskiy et al., 2021].\\nAn image is split into N\\npatches. Each patch is reshaped into a vector by\\nthe vec operator. This vector is acted upon by a\\nmatrix W which maps the patch to a D dimen-\\nsional vector x(0)\\nn . These vectors are collected\\ntogether into the input X(0).\\nThe matrix W\\ncan be learned with the rest of the transformer’s\\nparameters.\\n3 The idea of interleaving processing across the\\nsequence and across features is a common motif\\nof many machine learning architectures includ-\\ning graph neural networks (interleaves processing\\nacross nodes and across features), Fourier neu-\\nral operators (interleaves processing across space\\nand across features), and bottleneck blocks in\\nResNets (interleaves processing across pixels and\\nacross features).\\n1\\nPreliminaries\\nLet’s start by talking about the form of the data that is input into a transformer,\\nthe goal of the transformer, and the form of its output.\\n1.1\\nInput data format: sets or sequences of tokens\\nIn order to apply a transformer, data must be converted into a set or sequence1\\nof N tokens x(0)\\nn\\nof dimension D (see figure 1). The tokens can be collected\\ninto a matrix X(0) which is D × N.2 To give two concrete examples\\n1. a passage of text can be broken up into a sequence of words or sub-words,\\nwith each word being represented by a single unique vector,\\n2. an image can be broken up into a set of patches and each patch can be\\nmapped into a vector.\\nThe embeddings can be fixed or they can be learned with the rest of the pa-\\nrameters of the model e.g. the vectors representing words can be optimised or\\na learned linear transform can be used to embed image patches (see figure 2).\\nA sequence of tokens is a generic representation to use as an input – many\\ndifferent types of data can be “tokenised” and transformers are then immediately\\napplicable rather than requiring a bespoke architectures for each modality as\\nwas previously the case (CNNs for images, RNNs for sequences, deepsets for\\nsets etc.).\\nMoreover, this means that you don’t need bespoke handcrafted\\narchitectures for mixing data of different modalities — you can just throw them\\nall into a big set of tokens.\\n1.2\\nGoal: representations of sequences\\nThe transformer will ingest the input data X(0) and return a representation of\\nthe sequence in terms of another matrix X(M) which is also of size D × N.\\nThe slice xn = X(M)\\n:,n\\nwill be a vector of features representing the sequence at\\nthe location of token n. These representations can be used for auto-regressive\\nprediction of the next (n+1)th token, global classification of the entire sequence\\n(by pooling across the whole representation), sequence-to-sequence or image-\\nto-image prediction problems, etc. Here M denotes the number of layers in the\\ntransformer.\\n2\\nThe transformer block\\nThe representation of the input sequence will be produced by iteratively applying\\na transformer block\\nX(m) = transformer-block(X(m−1)).\\nThe block itself comprises two stages: one operating across the sequence and one\\noperating across the features. The first stage refines each feature independently\\naccording to relationships between tokens across the sequence e.g. how much\\na word in a sequence at position n depends on previous words at position n′,\\nor how much two different patches from an image are related to one another.\\nThis stage acts horizontally across rows of X(m−1). The second stage refines\\nthe features representing each token. This stage acts vertically across a column\\nof X(m−1). By repeatedly applying the transformer block the representation at\\ntoken n and feature d can be shaped by information at token n′ and feature d′.3\\n1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-09T02:33:09+00:00', 'source': '..\\\\data\\\\PDFs\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Transformers.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-02-09T02:33:09+00:00', 'trapped': '', 'modDate': 'D:20240209023309Z', 'creationDate': 'D:20240209023309Z', 'page': 2}, page_content='4 Relationship to Convolutional Neural Net-\\nworks (CNNs).\\nThe attention mechanism can\\nrecover convolutional filtering as a special case\\ne.g. if x(0)\\nn\\nis a 1D regularly sampled time-series\\nand A(m)\\nn′,n = A(m)\\nn′−n then the attention mecha-\\nnism in eq. 1 becomes a convolution. Unlike nor-\\nmal CNNs, these filters have full temporal sup-\\nport. Later we will see that the filters themselves\\ndynamically depend on the input, another differ-\\nence from standard CNNs.\\nWe will also see a\\nsimilarity: transformers will use multiple atten-\\ntion maps in each layer in the same way that\\nCNNs use multiple filters (though typically trans-\\nformers have fewer attention maps than CNNs\\nhave channels).\\n5 The need for transformers to store and com-\\npute N ×N attention arrays can be a major com-\\nputational bottleneck, which makes processing of\\nlong sequences challenging.\\n6 When training transformers to perform auto-\\nregressive prediction, e.g. predicting the next\\nword in a sequence based on the previous ones, a\\nclever modification to the model can be used to\\naccelerate training and inference. This involves\\napplying the transformer to the whole sequence,\\nand using masking in the attention mechanism\\n(A(m) becomes an upper triangular matrix) to\\nprevent future tokens affecting the representation\\nat earlier tokens. Causal predictions can then be\\nmade for the entire sequence in one forward pass\\nthrough the transformer. See section 4 for more\\ninformation.\\n7 We temporarily suppress the superscripts here\\nto ease the notation so A(m)\\nn,n′ becomes An,n′\\nand similarly x(m)\\nn\\nbecomes xn.\\n2.1\\nStage 1: self-attention across the sequence\\nThe output of the first stage of the transformer block is another D × N array,\\nY (m). The output is produced by aggregating information across the sequence\\nindependently for each feature using an operation called attention.\\nAttention. Specifically, the output vector at location n, denoted y(m)\\nn\\n, is pro-\\nduced by a simple weighted average of the input features at location n′ =\\n1 . . . N, denoted x(m−1)\\nn′\\n, that is4\\ny(m)\\nn\\n=\\nN\\nX\\nn′=1\\nx(m−1)\\nn′\\nA(m)\\nn′,n.\\n(1)\\nHere the weighting is given by a so-called attention matrix A(m)\\nn′,n which is of\\nsize5 N × N and normalises over its columns PN\\nn′=1 A(m)\\nn′,n = 1. Intuitively\\nspeaking A(m)\\nn′,n will take a high value for locations in the sequence n′ which are\\nof high relevance for location n. For irrelevant locations, it will take the value\\n0. For example, all patches of a visual scene coming from a single object might\\nhave high corresponding attention values.\\nWe can compactly write the relationship as a matrix multiplication,\\nY (m) = X(m−1)A(m),\\n(2)\\nand we illustrate it below in figure 3.6\\nFigure 3: The output of an element of the attention mechanism, Y (m)\\nd,n , is\\nproduced by the dot product of the input horizontally sliced through time X(m)\\nd,:\\nwith a vertical slice from the attention matrix A(m)\\n:,n . Here the shading in the\\nattention matrix represent the elements with a high value in white and those\\nwith a low value, near to 0, in black.\\nSelf-attention. So far, so simple. But where does the attention matrix come\\nfrom? The neat idea in the first stage of the transformer is that the attention\\nmatrix is generated from the input sequence itself – so-called self-attention.\\nA simple way of generating the attention matrix from the input would be to\\nmeasure the similarity between two locations by the dot product between the\\nfeatures at those two locations and then use a softmax function to handle the\\nnormalisation i.e.7\\nAn,n′ =\\nexp(x⊤\\nn xn′)\\nPN\\nn′′=1 exp(x⊤\\nn′′xn′)\\n.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-09T02:33:09+00:00', 'source': '..\\\\data\\\\PDFs\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Transformers.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-02-09T02:33:09+00:00', 'trapped': '', 'modDate': 'D:20240209023309Z', 'creationDate': 'D:20240209023309Z', 'page': 3}, page_content='8 Often you will see attention parameterised as\\nAn,n′ =\\nexp(x⊤\\nn U⊤Uxn′/\\n√\\nD)\\nPN\\nn′′=1 exp(x⊤\\nn′′U⊤Uxn′/\\n√\\nD)\\n.\\nDividing the exponents by the square-root of the\\ndimensionality of the projected vector helps nu-\\nmerical stability, but in this presentation we ab-\\nsorb this term into U to improve clarity.\\n9 Some of this effect could be handled by the\\nnormalisation in the denominator, but asymmet-\\nric similarity allows more flexibility. However, I\\ndo not know of experimental evidence to support\\nusing Uq ̸= Uk.\\n10\\nRelationship\\nto\\nRecurrent\\nNeural\\nNet-\\nworks (RNNs).\\nIt is illuminating to compare\\nthe temporal processing in the transformer to\\nthat of RNNs which recursively update a hid-\\nden state feature representation (x(1)\\nn ) based\\non the current observation (x(0)\\nn ) and the pre-\\nvious hidden state x(1)\\nn\\n= f(x(1)\\nn−1; x(0)\\nn ) =\\nf(f(x(1)\\nn−2; x(0)\\nn−1); x(0)\\nn ).\\nHere we’ve unrolled\\nthe RNN one step to show that observations\\nwhich are nearby to the hidden state (e.g. x(0)\\nn )\\nare treated differently from observations that\\nare further away (e.g. x(0)\\nn−1), as information is\\npropagated by recurrent application of the func-\\ntion f(·). In contrast, in the transformer, self-\\nattention treats all observations at all time-points\\nin an identical manner, no matter how far away\\nthey are.\\nThis is one reason why they find it\\nsimpler to learn long-range relationships.\\n11 If attention matrices are viewed as a data-\\ndriven version of filters in a CNN, then the need\\nfor more filters / channels is clear.\\nTypical\\nchoices for the number of heads H is 8 or 16,\\nlower than typical numbers of channels in a CNN.\\n12 The computational cost of multi-head self-\\nattention is usually dominated by the matrix mul-\\ntiplication involving the attention matrix and is\\ntherefore O(HDN2).\\n13 The product of the matrices V (m)\\nh\\nX(m−1)\\nis related to the so-called values which are nor-\\nmally introduced in descriptions of self-attention\\nalong side queries and keys. In the usual presen-\\ntation, there is a redundancy between the linear\\ntransform used to compute the values and the lin-\\near projection at the end of the multi-head self-\\nattention, so we have not explicitly introduced\\nthem here. The standard presentation can be re-\\ncovered by setting Vh to be a low-rank matrix\\nVh = UhUv,h where Uh is DxK and Uv,h is\\nKxD. Typically K is set to K = D/H so that\\nchanging the number of heads leads to models\\nwith similar numbers of parameters and compu-\\ntational demands.\\nHowever, this naïve approach entangles information about the similarity between\\nlocations in the sequence with the content of the sequence itself.\\nAn alternative is to perform the same operation on a linear transformation of\\nthe sequence, Uxn, so that8\\nAn,n′ =\\nexp(x⊤\\nn U ⊤Uxn′)\\nPN\\nn′′=1 exp(x⊤\\nn′′U ⊤Uxn′)\\nTypically, U will project to a lower dimensional space i.e. U is K×D dimensional\\nwith K < D. In this way only some of the features in the input sequence need\\nbe used to compute the similarity, the others being projected out, thereby de-\\ncoupling the attention computation from the content. However, the numerator\\nin this construction is symmetric. This could be a disadvantage. For example,\\nwe might want the word ‘caulking iron’ to be strongly associated with the word\\n‘tool’ (as it is a type of tool), but have the word ‘tool’ more weakly associated\\nwith the word ‘caulking iron’ (because most of us rarely encounter it).9\\nFortunately, it is simple to generalise the attention mechanism above to be asym-\\nmetric by applying two different linear transformations to the original sequence,\\nAn,n′ =\\nexp\\n\\x00x⊤\\nn U ⊤\\nk Uqxn′\\x01\\nPN\\nn′′=1 exp\\n\\x00x⊤\\nn′′U ⊤\\nk Uqxn′\\x01.\\n(3)\\nThe two quantities that are dot-producted together here qn = Uqxn and kn =\\nUkxn are typically known as the queries and the keys, respectively.\\nTogether equations 2 and 3 define the self-attention mechanism. Notice that\\nthe K × D matrices Uq and Uk are the only parameters of this mechanism.10\\nMulti-head self-attention (MHSA). In the self-attention mechanisms de-\\nscribed above, there is one attention matrix which describes the similarity of\\ntwo locations within the sequence. This can act as a bottleneck in the architec-\\nture – it would be useful for pairs of points to be similar in some ‘dimensions’\\nand different in others.11\\nIn order to increase capacity of the first self-attention stage, the transformer\\nblock applies H sets of self-attention in parallel12 (termed H heads) and then\\nlinearly projects the results down to the D × N array required for further pro-\\ncessing. This slight generalisation is called multi-head self-attention.\\nY (m) = MHSAθ(X(m−1)) =\\nH\\nX\\nh=1\\nV (m)\\nh\\nX(m−1)A(m)\\nh\\n, where\\n(4)\\n[A(m)\\nh\\n]n,n′ =\\nexp\\n\\x12\\x10\\nk(m)\\nh,n\\n\\x11⊤\\nq(m)\\nh,n′\\n\\x13\\nPN\\nn′′=1 exp\\n\\x12\\x10\\nk(m)\\nh,n′′\\n\\x11⊤\\nq(m)\\nh,n′\\n\\x13\\n(5)\\nq(m)\\nh,n = U (m)\\nq,h x(m−1)\\nn\\nand k(m)\\nh,n = U (m)\\nk,h x(m−1)\\nn\\n.\\n(6)\\nHere the H matrices V (m)\\nh\\nwhich are D × D project the H self-attention stages\\ndown to the required output dimensionality D.13\\nThe addition of the matrices V (m)\\nh\\n, and the fact that retaining just the diagonal\\nelements of the attention matrix A(m) will interact the signal instantaneously\\nwith itself, does mean there is some cross-feature processing in multi-head self-\\nattention, as opposed to it containing purely cross-sequence processing. How-\\never, the stage has limited capacity for this type of processing and it is the job\\nof the second stage to address this.\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-09T02:33:09+00:00', 'source': '..\\\\data\\\\PDFs\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Transformers.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-02-09T02:33:09+00:00', 'trapped': '', 'modDate': 'D:20240209023309Z', 'creationDate': 'D:20240209023309Z', 'page': 4}, page_content='14 The MLPs used typically have one or two\\nhidden-layers with dimension equal to the num-\\nber of features D (or larger). The computational\\ncost of this step is therefore roughly N×D×D. If\\nthe feature embedding size approaches the length\\nof the sequence D ≈N, the MLPs can start to\\ndominate the computational complexity (e.g. this\\ncan be the case for vision transformers which em-\\nbed large patches).\\n15\\nRelationship to Graph Neural Networks\\n(GNNs). At a high level, graph neural networks\\ninterleave two steps.\\nFirst, a message passing\\nstep where each node receives messages from its\\nneighbours which are then aggregated together.\\nSecond, a feature processing step where the in-\\ncoming aggregated messages are used to update\\neach node’s features.\\nThrough this lens, the\\ntransformer can be viewed as an unrolled GNN\\nwith each token corresponding to an edge of a\\nfully connected graph.\\nMHSA forms the mes-\\nsage passing step, and the MLPs forming the\\nfeature update step. Each transformer block cor-\\nresponds to one update of the GNN. Moreover,\\nmany methods for scaling transformers introduce\\nsparse forms of attention where each token at-\\ntends to only a restricted set of other tokens,\\nthat is they specify a sparse graph connectivity\\nstructure. Arguably, in this way transformers are\\nmore general as they can use different graphs at\\ndifferent layers in the transformer.\\n16 This is also known as z-scoring in some fields\\nand is related to whitening.\\nFigure 4 shows multi-head self-attention schematically. Multi-head attention\\ncomprises the following parameters θ = {Uq,h, Uk,h, Vh}H\\nh=1 i.e. 3H matrices\\nof size K × D, K × D, and D × D respectively.\\nFigure 4: Multi-head self-attention applies H self-\\nattention operations in parallel and then linearly\\nprojects the HD × N dimensional output down to\\nD ×N by applying a linear transform, implemented\\nhere by the H matrices Vh.\\n2.2\\nStage 2: multi-layer perceptron across features\\nThe second stage of processing in the transformer block operates across features,\\nrefining the representation using a non-linear transform. To do this, we simply\\napply a multi-layer perceptron (MLP) to the vector of features at each location\\nn in the sequence,\\nx(m)\\nn\\n= MLPθ(y(m)\\nn\\n).\\nNotice that the parameters of the MLP, θ, are the same for each location n.14\\n15\\n2.3\\nThe transformer block: Putting it all together with residual con-\\nnections and layer normalisation\\nWe can now stack MHSA and MLP layers to produce the transformer block.\\nRather than doing this directly, we make use of two ubiquitous transformations\\nto produce a more stable model that trains more easily: residual connections\\nand normalisation.\\nResidual connections. The use of residual connections is widespread across\\nmachine learning as they make initialisation simple, have a sensible inductive bias\\ntowards simple functions, and stabilise learning [Szegedy et al., 2017]. Instead\\nof directly specifying a function x(m) = fθ(x(m−1)), the idea is to parameterise\\nit in terms of an identity mapping and a residual term\\nx(m) = x(m−1) + resθ(x(m−1)).\\nEquivalently, this can be viewed as modelling the differences between the repre-\\nsentation x(m) −x(m−1) = resθ(x(m−1)) and will work well when the function\\nthat is being modelled is close to identity. This type of parameterisation is used\\nfor both the MHSA and MLP stages in the transformer, with the idea that each\\napplies a mild non-linear transformation to the representation. Over many layers,\\nthese mild non-linear transformations compose to form large transformations.\\nToken normalisation. The use of normalisation, such as LayerNorm and Batch-\\nNorm, is also widespread across the deep learning community as a means to\\nstabilise learning. There are many potential choices for how to compute nor-\\nmalisation statistics (see figure 5 for a discussion), but the standard approach\\nis use LayerNorm [Ba et al., 2016] which normalises each token separately, re-\\nmoving the mean and dividing by the standard deviation,16\\n¯xd,n =\\n1\\np\\nvar(xn)\\n(xd,n −mean(xn)) γd + βd = LayerNorm(X)d,n\\nwhere mean(xn) = 1\\nD\\nPD\\nd=1 xd,n and var(xn) = 1\\nD\\nPD\\nd=1(xd,n −mean(xn))2.\\nThe two parameters γd and βd are a learned scale and shift.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-09T02:33:09+00:00', 'source': '..\\\\data\\\\PDFs\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Transformers.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-02-09T02:33:09+00:00', 'trapped': '', 'modDate': 'D:20240209023309Z', 'creationDate': 'D:20240209023309Z', 'page': 5}, page_content='batch\\nfeature\\nsequence\\nd=D\\nd=1\\nn=N\\nn=1\\nbatch\\nfeature\\nsequence\\nd=D\\nd=1\\nn=N\\nn=1\\nLayerNorm for transformers\\n(TokenNorm)\\nBatchNorm for transformers\\nFigure 5:\\nTransformers perform layer normali-\\nsation (left hand schematic) which normalises the\\nmean and standard deviation of each individual to-\\nken in each sequence in the batch. Batch normal-\\nisation (right hand schematic), which normalises\\nover the feature and batch dimension together, is\\nfound to be far less stable [Shen et al., 2020].\\nOther flavours of normalisation are possible and po-\\ntentially under-explored e.g. instance normalisation\\nwould normalise across the sequence dimension in-\\nstead.\\n17\\nWhilst it is possible to control the non-\\nlinearities and weights in neural networks to pre-\\nvent explosion of the representation, the con-\\nstraints this places on the activation functions\\ncan adversely affect learning.\\nThe LayerNorm\\napproach is arguably simpler and simpler to train.\\nbatch\\nfeature\\nimage height\\nand width\\nd=D\\nd=1\\nbatch\\nfeature\\nsequence\\nd=D\\nd=1\\nn=N\\nn=1\\nLayerNorm for CNNs\\nBatchNorm for CNNs\\nFigure 6: In CNNs LayerNorm is conventionally\\napplied to both the features and across the fea-\\nture maps (i.e. across the height and width of the\\nimages) (left hand schematic). As the height and\\nwidth dimension in CNNs corresponds to the se-\\nquence dimension, 1 . . . N of transformers, the\\nterm ’LayerNorm’ is arguably used inconsistently\\n(compare to figure 5). I would prefer to call the\\nnormalisation used in transformers ’token nor-\\nmalisation’ instead to avoid confusion.\\nBatch\\nnormalisation (right hand schematic) is consis-\\ntently defined.\\n18 The exact configuration of the normalisation\\nand residual layers can differ, but here we show\\na standard setup [Xiong et al., 2020].\\nAs this transform normalises each token individually and as LayerNorm is ap-\\nplied differently in CNNs, see figure 6, I would prefer to call this normalisation\\nTokenNorm.\\nThis transform stops feature representations blowing up in magnitude as non-\\nlinearities are repeatedly applied through neural networks.17\\nIn transformers,\\nLayerNorm is usually applied in the residual terms of both the MHSA and MLP\\nstages.\\nPutting this all together, we have the standard transformer block shown schemat-\\nically in figure 7.18\\nFigure 7: The transformer block. Residual connections are added to the multi-\\nhead self-attention (MHSA) stage and the multi-layer perceptron (MLP) stage.\\nLayer normalisation is also applied to the inputs of both the MHSA and the\\nMLP. They are then stacked. This block can then be repeated M times.\\n3\\nPosition encoding\\nThe transformer treats the data as a set — if you permute the columns of X(0)\\n(i.e. re-order the tokens in the input sequence) you permute all the represen-\\ntations throughout the network X(m) in the same way. This is key for many\\napplications since there may not be a natural way to order the original data into\\na sequence of tokens. For example, there is no single ‘correct’ order to map\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-09T02:33:09+00:00', 'source': '..\\\\data\\\\PDFs\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Transformers.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-02-09T02:33:09+00:00', 'trapped': '', 'modDate': 'D:20240209023309Z', 'creationDate': 'D:20240209023309Z', 'page': 6}, page_content='19 Vision transformers [Dosovitskiy et al., 2021]\\nuse x(0)\\nn\\n= Wpn + en where pn is the nth\\nvectorised patch, en is the learned position em-\\nbedding, and W is the patch embedding matrix.\\nArguably it would be more intuitive to append\\nthe position embedding to the patch embedding.\\nHowever, if we use the concatenation approach\\nand consider what happens after applying a linear\\ntransform,\\nV\\nh Wpn\\nen\\ni\\n=\\nh V11\\nV12\\nV21\\nV22\\nih Wpn\\nen\\ni\\n=\\nh V11Wpn + V12en\\nV21Wpn + V22en\\ni\\n= W ′pn + e′\\nn\\nwe recover the additive construction, which is\\none hint as to why the additive construction\\nworks.\\n20 Note that I’m overloading the notation here:\\npreviously superscripts denoted layers in the\\ntransformer, but here I’m using them to denote\\nthe number of items in the input sequence.\\nimage patches into a one dimensional sequence.\\nHowever, this presents a problem since positional information is key in many\\nproblems and the transformer has thrown it out.\\nThe sequence ‘herbivores\\neat plants’ should not have the same representation (up to permutation) as\\n‘plants eat herbivores’. Nor should an image have the same representation as\\none comprising the same patches randomly permuted. Thankfully, there is a\\nsimple fix for this: the location of each token within the original dataset should\\nbe included in the token itself, or through the way it is processed. There are\\nseveral options how to do this, one is to include this information directly into the\\nembedding X(0). E.g. by simply adding the position embedding (surprisingly this\\nworks19) or concatenating. The position information can be fixed e.g. adding a\\nvector of sinusoids of different frequencies and phases to encode position of a\\nword in a sentence [Vaswani et al., 2017], or it can be a free parameter which is\\nlearned [Devlin et al., 2019], as it often done in image transformers. There are\\nalso approaches to include relative distance information between pairs of tokens\\nby modifying the self-attention mechanism [Wu et al., 2021] which connects to\\nequivariant transformers.\\n4\\nApplication specific transformer variants\\nFor completeness we will give some simple examples for how the standard trans-\\nformer architecture above is used and modified for specific applications. This\\nincludes adding a head to the transformer blocks to carry out the desired pre-\\ndiction task, but also modifications to the standard construction of the body.\\n4.1\\nAuto-regressive language modelling\\nIn auto-regressive language modelling the goal is to predict the next word wn\\nin the sequence given the previous words w1:n−1, that is to return p(wn =\\nw|w1:n−1). Two modifications are required to use the transformer for this task\\n— a change to the body to make the architecture efficient and the addition of\\na head to make the predictions for the next word.\\nModification to the body: auto-regressive masking. Applying the version\\nof the transformer we have covered so far to auto-regressive prediction is compu-\\ntationally expensive, both during training and testing. To see this, note that AR\\nprediction requires making a sequence of predictions: you start by predicting the\\nfirst word p(w1 = w), then you predict the second given the first p(w2 = w|w1),\\nthen the third word given the first two p(w2 = w|w1, w2), and so on until you\\npredict the last item in the sequence p(wN = w|w1:N−1). This requires apply-\\ning the transformer N −1 times with input sequences that grow by one word\\neach time: w1, w1:2, . . . , w1:N−1. This is very costly at both training-time and\\ntest-time.\\nFortunately, there is a neat way around this by enabling the transformer to\\nsupport incremental updates whereby if you add a new token to an existing\\nsequence, you do not change the representation for the old tokens. To make this\\nproperty clear, I will define it mathematically: let the output of the incremental\\ntransformer applied to the first n words be denoted20\\nX(n) = transformer-incremental(w1:n).\\nThen the output of the incremental transformer when applied to n + 1 words is\\nX(n+1) = transformer-incremental(w1:n+1).\\nIn the incremental transformer X(n) = X(n+1)\\n1:D,1:n i.e. the representation of the\\nold tokens has not changed by adding the new one. If we have this property\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-09T02:33:09+00:00', 'source': '..\\\\data\\\\PDFs\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Transformers.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-02-09T02:33:09+00:00', 'trapped': '', 'modDate': 'D:20240209023309Z', 'creationDate': 'D:20240209023309Z', 'page': 7}, page_content='21 Notice that this masking operation also en-\\ncodes position information since you can infer\\nthe order of the tokens from the mask.\\n22 This restriction to the attention will cause a\\nloss of representational power. It’s an open ques-\\ntion as to how significant this is and whether in-\\ncreasing the capacity of the model can mitigate\\nit e.g. by using higher dimensional tokens, i.e. in-\\ncreasing D.\\nthen 1. at test-time auto-regressive generation can use incremental updates to\\ncompute the new representation efficiently, 2. at training time we can make\\nthe N auto-regressive predictions for the whole sequence p(w1 = w)p(w2 =\\nw|w1)p(w2 = w|w1, w2) . . . p(wN = w|w1:N−1) in a single forwards pass.\\nUnfortunately, the standard transformer introduced above does not have this\\nproperty due to the form of the attention used. Every token attends to every\\nother token, so if we add a new token to the sequence then the representation\\nfor every token changes throughout the transformer. However, if we mask the\\nattention matrix so that it is upper-triangular An,n′ = 0 when n > n′ then\\nthe representation of each word only depends on the previous words.21 This\\nthen gives us the incremental property as none of the other operations in the\\ntransformer operate across the sequence.22\\nAdding a head. We’re now almost set to perform auto-regressive language\\nmodelling. We apply the masked transformer block M times to the input se-\\nquence of words. We then take the representation at token n −1, that is x(M)\\nn−1\\nwhich captures causal information in the sequence at this point, and generate\\nthe probability of the next word through a softmax operation\\np(wn = w|w1:n−1) = p(wn = w|x(M)\\nn−1) =\\nexp(g⊤\\nwx(M)\\nn−1)\\nPW\\nw=1 exp(g⊤\\nwx(M)\\nn−1)\\n.\\nHere W is the vocabulary size, the wth word is w and {gw}W\\nw=1 are softmax\\nweights that will be learned.\\n4.2\\nImage classification\\nFor image classification the goal is to predict the label y given the input image\\nwhich has been tokenised into the sequence X(0), that is p(y|X(0)). One way\\nof computing this distribution would be to apply the standard transformer body\\nM times to the tokenised image patches before aggregating the final layer of the\\ntransformer, X(M), across the sequence e.g. by spatial pooling h = PN\\nn=1 x(M)\\nn\\nin order to form a feature representation for the entire image. The representation\\nh could then be used to perform softmax classification. An alternative approach\\nis found to perform better [Dosovitskiy et al., 2021]. Instead we introduce a\\nnew fixed (learned) token at the start n = 0 of the input sequence x(0)\\n0 . At\\nthe head we use the n = 0 vector, x(M)\\n0\\n, to perform the softmax classification.\\nThis approach has the advantage that the transformer maintains and refines a\\nglobal representation of the sequence at each layer m of the transformer that is\\nappropriate for classification.\\n4.3\\nMore complex uses\\nThe transformer block can also be used as part of more complicated systems\\ne.g. in encoder-decoder architectures for sequence-to-sequence modelling for\\ntranslation [Devlin et al., 2019, Vaswani et al., 2017] or in masked auto-encoders\\nfor self-supervised vision systems [He et al., 2021].\\n5\\nConclusion\\nThis concludes this basic introduction to transformers which aspired to be math-\\nematically precise and to provide intuitions behind the design decisions.\\nWe have not talked about loss functions or training in any detail, but this is\\nbecause rather standard deep learning approaches are used for these. Briefly,\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-09T02:33:09+00:00', 'source': '..\\\\data\\\\PDFs\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Transformers.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-02-09T02:33:09+00:00', 'trapped': '', 'modDate': 'D:20240209023309Z', 'creationDate': 'D:20240209023309Z', 'page': 8}, page_content='transformers are typically trained using the Adam optimiser. They are often\\nslow to train compared to other architectures and typically get more unstable\\nas training progresses. Gradient clipping, decaying learning rate schedules, and\\nincreasing batch sizes through training help to mitigate these instabilities, but\\noften they still persist.\\nAcknowledgements. We thank Dr. Max Patacchiola, Sasha Shysheya, John\\nBronskill, Runa Eschenhagen and Jess Riedel for feedback on previous versions\\nof this note. Richard E. Turner is supported by Microsoft, Google, Amazon,\\nARM, Improbable and EPSRC grant EP/T005386/1.\\nReferences\\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization.\\narXiv preprint arXiv:1607.06450, 2016.\\nKaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian.\\nPangu-weather:\\nA 3d high-resolution model for fast and accurate global\\nweather forecast, 2022.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT:\\nPre-training of deep bidirectional transformers for language understanding.\\nIn Proceedings of the 2019 Conference of the North American Chapter\\nof the Association for Computational Linguistics: Human Language Tech-\\nnologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapo-\\nlis, Minnesota, June 2019. Association for Computational Linguistics. doi:\\n10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423.\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\\naohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\\nGeorg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.\\nAn im-\\nage is worth 16x16 words: Transformers for image recognition at scale. In\\n9th International Conference on Learning Representations, ICLR 2021, Vir-\\ntual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https:\\n//openreview.net/forum?id=YicbFdNTTy.\\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Gir-\\nshick. Masked autoencoders are scalable vision learners, 2021.\\nMary Phuong and Marcus Hutter. Formal algorithms for transformers. arXiv\\npreprint arXiv:2207.09238, 2022.\\nSheng Shen, Zhewei Yao, Amir Gholami, Michael Mahoney, and Kurt Keutzer.\\nPowerNorm: Rethinking batch normalization in transformers. In Hal Daumé\\nIII and Aarti Singh, editors, Proceedings of the 37th International Confer-\\nence on Machine Learning, volume 119 of Proceedings of Machine Learn-\\ning Research, pages 8741–8751. PMLR, 13–18 Jul 2020.\\nURL https:\\n//proceedings.mlr.press/v119/shen20e.html.\\nChristian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander Alemi.\\nInception-v4, inception-resnet and the impact of residual connections on learn-\\ning.\\nIn Proceedings of the AAAI conference on artificial intelligence, vol-\\nume 31, 2017.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\\nAidan N Gomez, Lukasz Kaiser, and Illia Polosukhin.\\nAttention is all\\nyou need.\\nIn I. Guyon,\\nU. Von Luxburg,\\nS. Bengio,\\nH. Wallach,\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-09T02:33:09+00:00', 'source': '..\\\\data\\\\PDFs\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\PDFs\\\\Transformers.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-02-09T02:33:09+00:00', 'trapped': '', 'modDate': 'D:20240209023309Z', 'creationDate': 'D:20240209023309Z', 'page': 9}, page_content='R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neu-\\nral Information Processing Systems, volume 30. Curran Associates, Inc.,\\n2017.\\nURL https://proceedings.neurips.cc/paper_files/paper/\\n2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\\nK. Wu, H. Peng, M. Chen, J. Fu, and H. Chao.\\nRethinking and improv-\\ning relative position encoding for vision transformer.\\nIn 2021 IEEE/CVF\\nInternational Conference on Computer Vision (ICCV), pages 10013–10021,\\nLos Alamitos, CA, USA, oct 2021. IEEE Computer Society. doi: 10.1109/\\nICCV48922.2021.00988. URL https://doi.ieeecomputersociety.org/\\n10.1109/ICCV48922.2021.00988.\\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing,\\nHuishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normal-\\nization in the transformer architecture. In Hal Daumé III and Aarti Singh,\\neditors, Proceedings of the 37th International Conference on Machine Learn-\\ning, volume 119 of Proceedings of Machine Learning Research, pages 10524–\\n10533. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/\\nv119/xiong20b.html.\\n9')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_docs = pdf_loader.load()\n",
    "pdf_docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NetworkSecurity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
