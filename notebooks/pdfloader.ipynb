{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea4c7e7f",
   "metadata": {},
   "source": [
    "### Embedding Manager & VectorstoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd35d601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\renju\\Documents\\Renju\\AI ML\\Projects\\UV Projects\\RAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from  chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import os\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed98b1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 3 PDF Files\n",
      "Loaded 56 pages\n",
      "Loaded 36 pages\n",
      "Loaded 10 pages\n",
      "\n",
      " Total Documents loaded: 102\n"
     ]
    }
   ],
   "source": [
    "##  Read all the pdfs inside a directory\n",
    "def process_allpdfs(pdfdirectory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "\n",
    "    pdf_dir = Path(pdfdirectory)\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "\n",
    "    print(f\"found {len(pdf_files)} PDF Files\")\n",
    "\n",
    "    try:\n",
    "    \n",
    "        for pdf_file in pdf_files:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_document'] = pdf_file\n",
    "                doc.metadata['file_type'] = \"pdf\"\n",
    "\n",
    "            all_documents.extend(documents)\n",
    "            print(f\"Loaded {len(documents)} pages\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error {e}\")\n",
    "\n",
    "    print(f\"\\n Total Documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "all_pdf_documents = process_allpdfs(\"../data/PDFs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c996b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 0, 'page_label': '1', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='MIT Class 6.S184:Generative AI With Stochastic Differential Equations, 2025\\nAn Introduction to Flow Matching and Diffusion Models\\nPeter Holderrieth and Ezra Erives\\nWebsite: https://diffusion.csail.mit.edu/\\n1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\\n1.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\\n1.2 Course Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\n1.3 Generative Modeling As Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\n2 Flow and Diffusion Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n2.1 Flow Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n2.2 Diffusion Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n3 Constructing the Training Target . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n3.1 Conditional and Marginal Probability Path . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n3.2 Conditional and Marginal Vector Fields. . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n3.3 Conditional and Marginal Score Functions . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n4 Training the Generative Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\\n4.1 Flow Matching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\\n4.2 Score Matching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\\n4.3 A Guide to the Diffusion Model Literature. . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n5 Building an Image Generator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\n5.1 Guidance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\n5.2 Neural network architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\\n5.3 A Survey of Large-Scale Image and Video Models. . . . . . . . . . . . . . . . . . . . . . 46\\n6 Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\\n7 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\\nA A Reminder on Probability Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\\nA.1 Random vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\\nA.2 Conditional densities and expectations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\\nB A Proof of the Fokker-Planck equation . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\\n1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 1, 'page_label': '2', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='1 Introduction\\nCreating noise from data is easy; creating data\\nfrom noise is generative modeling.\\nSong et al. [30]\\n1.1 Overview\\nIn recent years, we all have witnessed a tremendous revolution in artificial intelligence (AI). Image generators like\\nStable Diffusion 3 can generate photorealistic and artistic images across a diverse range of styles, video models\\nlike Meta’sMovie Gen Videocan generate highly realistic movie clips, and large language models likeChatGPT\\ncan generate seemingly human-level responses to text prompts. At the heart of this revolution lies a new ability\\nof AI systems: the ability togenerate objects. While previous generations of AI systems were mainly used for\\nprediction, these new AI system are creative: they dream or come up with new objects based on user-specified\\ninput. Such generative AIsystems are at the core of this recent AI revolution.\\nThe goal of this class is to teach you two of the most widely used generative AI algorithms:denoising diffusion\\nmodels [32] andflow matching[14, 16, 1, 15]. These models are the backbone of the best image, audio, and video\\ngeneration models (e.g.,Stable Diffusion 3and Movie Gen Video), and have most recently became the state-of-the-\\nart in scientific applications such as protein structures (e.g.,AlphaFold3 is a diffusion model). Without a doubt,\\nunderstanding these models is truly an extremely useful skill to have.\\nAll of these generative models generate objects by iteratively convertingnoise into data. This evolution from\\nnoise to data is facilitated by the simulation ofordinary or stochastic differential equations (ODEs/SDEs). Flow\\nmatching and denoising diffusion models are a family of techniques that allow us to construct, train, and simulate,\\nsuch ODEs/SDEs at large scale with deep neural networks. While these models are rather simple to implement,\\nthe technical nature of SDEs can make these models difficult to understand. In this course, our goal is to provide a\\nself-contained introduction to the necessary mathematical toolbox regarding differential equations to enable you to\\nsystematically understand these models. Beyond being widely applicable, we believe that the theory behind flow\\nand diffusion models is elegant in its own right. Therefore, most importantly, we hope that this course will be a\\nlot of fun to you.\\nRemark 1(Additional Resources)\\nWhile these lecture notes are self-contained, there are two additional resources that we encourage you to use:\\n1. Lecture recordings:These guide you through each section in a lecture format.\\n2. Labs: These guide you in implementing your own diffusion model from scratch. We highly recommend\\nthat you “get your hands dirty” and code.\\nYou can find these on our course website: https://diffusion.csail.mit.edu/.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 2, 'page_label': '3', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='1.2 Course Structure\\n1.2 Course Structure\\nWe give a brief overview over of this document.\\n• Section 1, Generative Modeling as Sampling:We formalize what it means to “generate” an image, video,\\nprotein, etc. We will translate the problem of e.g., “how to generate an image of a dog?” into the more precise\\nproblem of sampling from a probability distribution.\\n• Section 2, Flow and Diffusion Models:Next, we explain the machinery of generation. As you can guess by\\nthe name of this class, this machinery consists of simulating ordinary and stochastic differential equations.\\nWe provide an introduction to differential equations and explain how to construct them with neural networks.\\n• Section 3, Constructing a Training Target:To train our generative model, we must first pin down precisely\\nwhat it is that our model is supposed to approximate. In other words, what’s the ground truth? We will\\nintroduce the celebratedFokker-Planck equation, which will allow us to formalize the notion of ground truth.\\n• Section 4, Training:This section formulates atraining objective, allowing us to approximate the training\\ntarget, or ground truth, of the previous section. With this, we are ready to provide a minimal implementation\\nof flow matching and denoising diffusion models.\\n• Section 5, Conditional Image Generation:We learn how to build a conditional image generator. To do so,\\nwe formulate how to condition our samples on a prompt (e.g. “an image of a cat”). We then discuss common\\nneural network architectures and survey state-of-the-art models for both image and video generation.\\nRequired background. Due to the technical nature of this subject, we recommend some base level of mathematical\\nmaturity, and in particular some familiarity with probability theory. For this reason, we included a brief reminder\\nsection on probability theory in appendix A. Don’t worry if some of the concepts there are unfamiliar to you.\\n1.3 Generative Modeling As Sampling\\nLet’s begin by thinking about various data types, ormodalities, that we might encounter, and how we will go\\nabout representing them numerically:\\n1. Image: Consider images withH × W pixels where H describes the height andW the width of the image,\\neach with three color channels (RGB). For every pixel and every color channel, we are given an intensity value\\nin R. Therefore, an image can be represented by an elementz ∈ RH×W×3.\\n2. Video: A video is simply a series of images in time. If we haveT time points or frames, a video would\\ntherefore be represented by an elementz ∈ RT×H×W×3.\\n3. Molecular structure:A naive way would be to represent the structure of a molecule by a matrix\\nz = (z1, . . . , zN ) ∈ R3×N where N is the number of atoms in the molecule and eachzi ∈ R3 describes the\\nlocation of that atom. Of course, there are other, more sophisticated ways of representing such a molecule.\\nIn all of the above examples, the object that we want to generate can be mathematically represented as a vector\\n(potentially after flattening). Therefore, throughout this document, we will have:\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 3, 'page_label': '4', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='1.3 Generative Modeling As Sampling\\nKey Idea 1(Objects as Vectors)\\nWe identify the objects being generated as vectorsz ∈ Rd.\\nA notable exception to the above is text data, which is typically modeled as a discrete object via autoregressive\\nlanguage models (such asChatGPT). While flow and diffusion models for discrete data have been developed, this\\ncourse focuses exclusively on applications to continuous data.\\nGeneration as Sampling. Let us define what it means to “generate” something. For example, let’s say we want\\nto generate an image of a dog. Naturally, there aremany possible images of dogs that we would be happy with. In\\nparticular, there is no one single “best” image of a dog. Rather, there is a spectrum of images that fit better or worse.\\nIn machine learning, it is common to think of this diversity of possible images as aprobability distribution. We call\\nit the data distribution and denote it aspdata. In the example of dog images, this distribution would therefore\\ngive higher likelihood to images that look more like a dog. Therefore, how \"good\" an image/video/molecule fits -\\na rather subjective statement - is replaced by how \"likely\" it is under the data distributionpdata. With this, we\\ncan mathematically express the task of generation as sampling from the (unknown) distributionpdata:\\nKey Idea 2(Generation as Sampling)\\nGenerating an objectz is modeled as sampling from the data distributionz ∼ pdata.\\nAgenerative modelis a machine learning model that allows us to generate samples frompdata. In machine learning,\\nwe require data to train models. In generative modeling, we usually assume access to a finite number of examples\\nsampled independently frompdata, which together serve as a proxy for the true distribution.\\nKey Idea 3(Dataset)\\nA dataset consists of a finite number of samplesz1, . . . , zN ∼ pdata.\\nFor images, we might construct a dataset by compiling publicly available images from the internet. For videos,\\nwe might similarly use YouTube as a database. For protein structures, we can use experimental data bases from\\nsources such as the Protein Data Bank (PDB) that collected scientific measurements over decades. As the size of\\nour dataset grows very large, it becomes an increasingly better representation of the underlying distributionpdata.\\nConditional Generation. In many cases, we want to generate an objectconditionedon some datay. For example,\\nwemightwanttogenerateanimageconditionedon y =“adogrunningdownahillcoveredwithsnowwithmountains\\nin the background”. We can rephrase this as sampling from aconditional distribution:\\nKey Idea 4(Conditional Generation)\\nConditional generation involves sampling fromz ∼ pdata(·|y), wherey is a conditioning variable.\\nWe callpdata(·|y) the conditional data distribution. The conditional generative modeling task typically involves\\nlearning to condition on an arbitrary, rather than fixed, choice of y. Using our previous example, we might\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 4, 'page_label': '5', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='1.3 Generative Modeling As Sampling\\nalternatively want to condition on a different text prompt, such asy =“a photorealistic image of a cat blowing out\\nbirthday candles”. We therefore seek a single model which may be conditioned on any such choice ofy. It turns out\\nthat techniques for unconditional generation are readily generalized to the conditional case. Therefore, for the first\\n3 sections, we will focus almost exclusively on the unconditional case (keeping in mind that conditional generation\\nis what we’re building towards).\\nFrom Noise to Data. So far, we have discussed thewhat of generative modeling: generating samples frompdata.\\nHere, we will briefly discuss thehow. For this, we assume that we have access to someinitial distribution pinit\\nthat we can easily sample from, such as the Gaussianpinit = N(0, Id). The goal of generative modeling is then to\\ntransform samples fromx ∼ pinit into samples frompdata. We note thatpinit does not have to be so simple as a\\nGaussian. As we shall see, there are interesting use cases for leveraging this flexibility. Despite this, in the majority\\nof applications we take it to be a simple Gaussian and it is important to keep that in mind.\\nSummary We summarize our discussion so far as follows.\\nSummary 2(Generation as Sampling)\\nWe summarize the findings of this section:\\n1. In this class, we consider the task of generating objects that are represented as vectorsz ∈ Rd such as\\nimages, videos, or molecular structures.\\n2. Generation is the task of generating samples from a probability distributionpdata having access to a\\ndataset of samplesz1, . . . , zN ∼ pdata during training.\\n3. Conditional generation assumes that we condition the distribution on a labely and we want to sample\\nfrom pdata(·|y) having access to data set of pairs(z1, y) . . . ,(zN , y) during training.\\n4. Our goal is to train a generative model to transform samples from a simple distributionpinit (e.g. a\\nGaussian) into samples frompdata.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 5, 'page_label': '6', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='2 Flow and Diffusion Models\\nIn the previous section, we formalized generative modeling as sampling from a data distributionpdata. Further,\\nwe saw that sampling could be achieved via the transformation of samples from a simple distributionpinit, such as\\nthe GaussianN(0, Id), to samples from the target distributionpdata. In this section, we describe how the desired\\ntransformation can be obtained as the simulation of a suitably constructed differential equation. For example,\\nflow matching and diffusion models involve simulatingordinary differential equations (ODEs) and stochastic\\ndifferential equations (SDEs), respectively. The goal of this section is therefore to define and construct these\\ngenerative models as they will be used throughout the remainder of the notes. Specifically, we first define ODEs\\nand SDEs, and discuss their simulation. Second, we describe how to parameterize an ODE/SDE using a deep neural\\nnetwork. This leads to the definition of a flow and diffusion model and the fundamental algorithms to sample from\\nsuch models. In later sections, we then explore how to train these models.\\n2.1 Flow Models\\nWe start by definingordinary differential equations (ODEs). A solution to an ODE is defined by atrajectory, i.e.\\na function of the form\\nX : [0, 1] → Rd, t 7→ Xt,\\nthat maps from timet to some location in spaceRd. Every ODE is defined by avector fieldu, i.e. a function of\\nthe form\\nu : Rd × [0, 1] → Rd, (x, t) 7→ ut(x),\\ni.e. for every timet and locationx we get a vectorut(x) ∈ Rd specifying a velocity in space (see fig. 1). An ODE\\nimposes a condition on a trajectory: we want a trajectoryX that “follows along the lines” of the vector fieldut,\\nstarting at the pointx0. We may formalize such a trajectory as being the solution to the equation:\\nd\\ndtXt = ut(Xt) ▶ ODE (1a)\\nX0 = x0 ▶ initial conditions (1b)\\nEquation (1a) requires that the derivative ofXt is specified by the direction given byut. Equation (1b) requires\\nthat we start atx0 at timet = 0. We may now ask: if we start atX0 = x0 at t = 0, where are we at timet (what\\nis Xt)? This question is answered by a function called theflow, which is a solution to the ODE\\nψ : Rd × [0, 1] 7→Rd, (x0, t) 7→ ψt(x0) (2a)\\nd\\ndtψt(x0) = ut(ψt(x0)) ▶ flow ODE (2b)\\nψ0(x0) = x0 ▶ flow initial conditions (2c)\\nFor a given initial conditionX0 = x0, a trajectory of the ODE is recovered viaXt = ψt(X0). Therefore, vector\\nfields, ODEs, and flows are, intuitively, three descriptions of the same object:vector fields define ODEs whose\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 6, 'page_label': '7', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='2.1 Flow Models\\nFigure 1: A flowψt : Rd → Rd (red square grid) is defined by a velocity fieldut : Rd → Rd (visualized with blue\\narrows) that prescribes its instantaneous movements at all locations (here,d = 2). We show three different times\\nt. As one can see, a flow is a diffeomorphism that \"warps\" space. Figure from [15].\\nsolutions are flows. As with every equation, we should ask ourselves about an ODE: Does a solution exist and if\\nso, is it unique? A fundamental result in mathematics is \"yes!\" to both, as long we impose weak assumptions on\\nut:\\nTheorem 3(Flow existence and uniqueness)\\nIf u : Rd ×[0, 1] → Rd is continuously differentiable with a bounded derivative, then the ODE in (2) has a unique\\nsolution given by a flowψt. In this case,ψt is adiffeomorphism for allt, i.e. ψt is continuously differentiable\\nwith a continuously differentiable inverseψ−1\\nt .\\nNote that the assumptions required for the existence and uniqueness of a flow are almost always fulfilled in machine\\nlearning, as we use neural networks to parameterizeut(x) and they always have bounded derivatives. Therefore,\\ntheorem 3 should not be a concern for you but rather good news:flows exist and are unique solutions to ODEs\\nin our cases of interest.A proof can be found in [20, 4].\\nExample 4(Linear Vector Fields)\\nLet us consider a simple example of a vector fieldut(x) that is a simple linear function inx, i.e. ut(x) = −θx\\nfor θ >0. Then the function\\nψt(x0) = exp (−θt) x0 (3)\\ndefines a flowψ solving the ODE in eq. (2). You can check this yourself by checking thatψ0(x0) = x0 and\\ncomputing\\nd\\ndtψt(x0)\\n(3)\\n= d\\ndt (exp (−θt) x0)\\n(i)\\n= −θ exp (−θt) x0\\n(3)\\n= −θψt(x0) = ut(ψt(x0)),\\nwhere in (i) we used the chain rule. In fig. 3, we visualize a flow of this form converging to0 exponentially.\\nSimulating an ODE. In general, it is not possible to compute the flowψt explicitly if ut is not as simple as a\\nlinear function. In these cases, one usesnumerical methodsto simulate ODEs. Fortunately, this is a classical and\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 7, 'page_label': '8', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='2.1 Flow Models\\nwell researched topic in numerical analysis, and a myriad of powerful methods exist [11]. One of the simplest and\\nmost intuitive methods is theEuler method. In the Euler method, we initialize withX0 = x0 and update via\\nXt+h = Xt + hut(Xt) ( t = 0, h,2h, 3h, . . . ,1 − h) (4)\\nwhere h = n−1 > 0 is a step size hyperparameter withn ∈ N. For this class, the Euler method will be good enough.\\nTo give you a taste of a more complex method, let us considerHeun’s methoddefined via the update rule\\nX′\\nt+h = Xt + hut(Xt) ▶ initial guess of new state\\nXt+h = Xt + h\\n2 (ut(Xt) + ut+h(X′\\nt+h)) ▶ update with averageu at current and guessed state\\nIntuitively, the Heun’s method is as follows: it takes a first guessX′\\nt+h of what the next step could be but corrects\\nthe direction initially taken via an updated guess.\\nFlow models. We can now construct a generative model via an ODE. Remember that our goal was to convert a\\nsimple distributionpinit into a complex distributionpdata. The simulation of an ODE is thus a natural choice for\\nthis transformation. Aflow modelis described by the ODE\\nX0 ∼ pinit ▶ random initialization\\nd\\ndtXt = uθ\\nt (Xt) ▶ ODE\\nwhere the vector fielduθ\\nt is a neural networkuθ\\nt with parametersθ. For now, we will speak ofuθ\\nt as being a generic\\nneural network; i.e. a continuous functionuθ\\nt : Rd ×[0, 1] → Rd with parametersθ. Later, we will discuss particular\\nchoices of neural network architectures. Our goal is to make the endpointX1 of the trajectory have distribution\\npdata, i.e.\\nX1 ∼ pdata ⇔ ψθ\\n1(X0) ∼ pdata\\nwhere ψθ\\nt describes the flow induced byuθ\\nt . Note however: although it is calledflow model, the neural network\\nparameterizes the vector field, not the flow. In order to compute the flow, we need to simulate the ODE. In\\nalgorithm 1, we summarize the procedure how to sample from a flow model.\\nAlgorithm 1Sampling from a Flow Model with Euler method\\nRequire: Neural network vector fielduθ\\nt , number of stepsn\\n1: Set t = 0\\n2: Set step sizeh = 1\\nn\\n3: Draw a sampleX0 ∼ pinit\\n4: for i = 1, . . . , ndo\\n5: Xt+h = Xt + huθ\\nt (Xt)\\n6: Update t ← t + h\\n7: end for\\n8: return X1\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 8, 'page_label': '9', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='2.2 Diffusion Models\\n2.2 Diffusion Models\\nStochastic differential equations (SDEs) extend the deterministic trajectories from ODEs withstochastic trajecto-\\nries. A stochastic trajectory is commonly called astochastic process(Xt)0≤t≤1 and is given by\\nXt is a random variable for every0 ≤ t ≤ 1\\nX : [0, 1] → Rd, t 7→ Xt is a random trajectory for every draw ofX\\nIn particular, when we simulate the same stochastic process twice, we might get different outcomes because the\\ndynamics are designed to be random.\\nBrownian Motion. SDEs are constructed via aBrownian motion- a fundamental stochastic process that came\\nout of the study physical diffusion processes. You can think of a Brownian motion as a continuous random walk.\\nFigure 2: Sample trajectories of a Brownian\\nmotion Wt in dimensiond = 1 simulated using\\neq. (5).\\nLetusdefineit: A Brownianmotion W = (Wt)0≤t≤1 isastochastic\\nprocess such thatW0 = 0, the trajectoriest 7→ Wt are continuous,\\nand the following two conditions hold:\\n1. Normalincrements: Wt−Ws ∼ N(0, (t−s)Id) forall 0 ≤ s <\\nt, i.e. increments have a Gaussian distribution with variance\\nincreasing linearly in time (Id is the identity matrix).\\n2. Independent increments: For any0 ≤ t0 < t1 < ··· < tn =\\n1, theincrementsWt1 −Wt0 , . . . , Wtn −Wtn−1 areindependent\\nrandom variables.\\nBrownian motion is also called aWiener process, which is why\\nwe denote it with a \"W\".1 We can easily simulate a Brownian\\nmotion approximately with step sizeh >0 by settingW0 = 0 and\\nupdating\\nWt+h =Wt +\\n√\\nhϵt, ϵ t ∼ N(0, Id) ( t = 0, h,2h, . . . ,1 − h) (5)\\nIn fig. 2, we plot a few example trajectories of a Brownian motion.\\nBrownian motion is as central to the study of stochastic processes as the Gaussian distribution is to the study of\\nprobability distributions. From finance to statistical physics to epidemiology, the study of Brownian motion has\\nfar reaching applications beyond machine learning. In finance, for example, Brownian motion is used to model the\\nprice of complex financial instruments. Also just as a mathematical construction, Brownian motion is fascinating:\\nFor example, while the paths of a Brownian motion are continuous (so that you could draw it without ever lifting\\na pen), they are infinitely long (so that you would never stop drawing).\\nFrom ODEs to SDEs.The idea of an SDE is to extend the deterministic dynamics of an ODE by adding stochastic\\ndynamics driven by a Brownian motion. Because everything is stochastic, we may no longer take the derivative as\\n1NobertWienerwasafamousmathematicianwhotaughtatMIT.YoucanstillseehisportraitshangingattheMITmathdepartment.\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 9, 'page_label': '10', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='2.2 Diffusion Models\\nFigure 3: Illustration of Ornstein-Uhlenbeck processes (eq. (8)) in dimensiond = 1 for θ = 0.25 and various choices\\nof σ (increasing from left to right). Forσ = 0, we recover a flow (smooth, deterministic trajectories) that converges\\nto the origin ast → ∞. Forσ >0 we have random paths which converge towards the GaussianN(0, σ2\\n2θ ) as t → ∞.\\nin eq. (1a). Hence, we need to find anequivalent formulation of ODEs that does not use derivatives. For this,\\nlet us therefore rewrite trajectories(Xt)0≤t≤1 of an ODE as follows:\\nd\\ndtXt = ut(Xt) ▶ expression via derivatives\\n(i)\\n⇔ 1\\nh (Xt+h − Xt) = ut(Xt) + Rt(h)\\n⇔ Xt+h = Xt + hut(Xt) + hRt(h) ▶ expression via infinitesimal updates\\nwhere Rt(h) describes a negligible function for smallh, i.e. such thatlim\\nh→0\\nRt(h) = 0, and in(i) we simply use the\\ndefinition of derivatives. The derivation above simply restates what we already know: A trajectory(Xt)0≤t≤1 of\\nan ODE takes, at every timestep, a small step in the directionut(Xt). We may now amend the last equation to\\nmake it stochastic: A trajectory(Xt)0≤t≤1 of an SDE takes, at every timestep, a small step in the directionut(Xt)\\nplus some contribution from a Brownian motion:\\nXt+h = Xt + hut(Xt)| {z }\\ndeterministic\\n+σt (Wt+h − Wt)| {z }\\nstochastic\\n+ hRt(h)| {z }\\nerror term\\n(6)\\nwhere σt ≥ 0 describes thediffusion coefficientand Rt(h) describes a stochastic error term such that the standard\\ndeviation E[∥Rt(h)∥2]1/2 → 0 goes to zero for h → 0. The above describes a stochastic differential equation\\n(SDE). It is common to denote it in the following symbolic notation:\\ndXt = ut(Xt)dt + σtdWt ▶ SDE (7a)\\nX0 = x0 ▶ initial condition (7b)\\nHowever, always keep in mind that the \"dXt\"-notation above is a purely informal notation of eq. (6). Unfortunately,\\nSDEs do not have a flow mapϕt anymore. This is because the valueXt is not fully determined byX0 ∼ pinit\\nanymore as the evolution itself is stochastic. Still, in the same way as for ODEs, we have:\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 10, 'page_label': '11', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='2.2 Diffusion Models\\nTheorem 5(SDE Solution Existence and Uniqueness)\\nIf u : Rd × [0, 1] → Rd is continuously differentiable with a bounded derivative andσt is continuous, then the\\nSDE in (7) has a solution given by the unique stochastic process(Xt)0≤t≤1 satisfying eq. (6).\\nIf this was a stochastic calculus class, we would spend several lectures proving this theorem and constructing SDEs\\nwith full mathematical rigor, i.e. constructing a Brownian motion from first principles and constructing the process\\nXt via stochastic integration. As we focus on machine learning in this class, we refer to [18] for a more technical\\ntreatment. Finally, note that every ODE is also an SDE - simply with a vanishing diffusion coefficientσt = 0.\\nTherefore, for the remainder of this class,when we speak about SDEs, we consider ODEs as a special case.\\nExample 6(Ornstein-Uhlenbeck Process)\\nLet us consider a constant diffusion coefficientσt = σ ≥ 0 and a constant linear driftut(x) = −θx for θ >0,\\nyielding the SDE\\ndXt = −θXtdt + σdWt. (8)\\nA solution(Xt)0≤t≤1 to the above SDE is known as anOrnstein-Uhlenbeck (OU) process. We visualize it in\\nfig. 3. The vector field−θx pushes the process back to its center0 (as I always go the inverse direction of where\\nI am), while the diffusion coefficientσ always adds more noise. This process converges towards a Gaussian\\ndistribution N(0, σ2) if we simulate it fort → ∞. Note that forσ = 0, we have a flow with linear vector field\\nthat we have studied in eq. (3).\\nSimulating an SDE. If you struggle with the abstract definition of an SDE so far, then don’t worry about it. A\\nmore intuitive way of thinking about SDEs is given by answering the question: How might we simulate an SDE?\\nThe simplest such scheme is known as theEuler-Maruyama method, and is essentially to SDEs what the Euler\\nmethod is to ODEs. Using the Euler-Maruyama method, we initializeX0 = x0 and update iteratively via\\nXt+h = Xt + hut(Xt) +\\n√\\nhσtϵt, ϵ t ∼ N(0, Id) (9)\\nwhere h = n−1 > 0 is a step size hyperparameter forn ∈ N. In other words, to simulate using the Euler-Maruyama\\nmethod, we take a small step in the direction ofut(Xt) as well as add a little bit of Gaussian noise scaled by\\n√\\nhσt.\\nWhen simulating SDEs in this class (such as in the accompanying labs), we will usually stick to the Euler-Maruyama\\nmethod.\\nDiffusion Models. We can now construct a generative model via an SDE in the same way as we did for ODEs.\\nRemember that our goal was to convert a simple distributionpinit into a complex distribution pdata. Like for\\nODEs, the simulation of an SDE randomly initialized withX0 ∼ pinit is a natural choice for this transformation.\\nTo parameterize this SDE, we can simply parameterize its central ingredient - the vector fieldut - a neural network\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 11, 'page_label': '12', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='2.2 Diffusion Models\\nAlgorithm 2Sampling from a Diffusion Model (Euler-Maruyama method)\\nRequire: Neural networkuθ\\nt , number of stepsn, diffusion coefficientσt\\n1: Set t = 0\\n2: Set step sizeh = 1\\nn\\n3: Draw a sampleX0 ∼ pinit\\n4: for i = 1, . . . , ndo\\n5: Draw a sampleϵ ∼ N(0, Id)\\n6: Xt+h = Xt + huθ\\nt (Xt) + σt\\n√\\nhϵ\\n7: Update t ← t + h\\n8: end for\\n9: return X1\\nuθ\\nt . A diffusion modelis thus given by\\ndXt = uθ\\nt (Xt)dt + σtdWt ▶ SDE\\nX0 ∼ pinit ▶ random initialization\\nIn algorithm 2, we describe the procedure by which to sample from a diffusion model with the Euler-Maruyama\\nmethod. We summarize the results of this section as follows.\\nSummary 7(SDE generative model)\\nThroughout this document, adiffusion modelconsists of a neural networkuθ\\nt with parametersθ that parame-\\nterize a vector field and a fixed diffusion coefficientσt:\\nNeural network:uθ : Rd × [0, 1] → Rd, (x, t) 7→ uθ\\nt (x) with parametersθ\\nFixed: σt : [0, 1] → [0, ∞), t7→ σt\\nTo obtain samples from our SDE model (i.e. generate objects), the procedure is as follows:\\nInitialization: X0 ∼ pinit ▶ Initialize with simple distribution, e.g. a Gaussian\\nSimulation: dXt = uθ\\nt (Xt)dt + σtdWt ▶ Simulate SDE from 0 to 1\\nGoal: X1 ∼ pdata ▶ Goal is to makeX1 have distributionpdata\\nA diffusion model withσt = 0 is aflow model.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 12, 'page_label': '13', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='3 Constructing the Training Target\\nIn the previous section, we constructed flow and diffusion models where we obtain trajectories (Xt)0≤t≤1 by\\nsimulating the ODE/SDE\\nX0 ∼pinit, dXt = uθ\\nt (Xt)dt (Flow model) (10)\\nX0 ∼pinit, dXt = uθ\\nt (Xt)dt + σtdWt (Diffusion model) (11)\\nwhere uθ\\nt is a neural network andσt is a fixed diffusion coefficient. Naturally, if we just randomly initialize the\\nparameters θ of our neural networkuθ\\nt , simulating the ODE/SDE will just produce nonsense. As always in machine\\nlearning, we need to train the neural network. We accomplish this by minimizing a loss functionL(θ), such as the\\nmean-squared error\\nL(θ) = ∥uθ\\nt (x) − utarget\\nt (x)| {z }\\ntraining target\\n∥2,\\nwhere utarget\\nt (x) is the training target that we would like to approximate. To derive a training algorithm, we\\nproceed in two steps: In this chapter, our goal is tofind an equation for the training targetutarget\\nt . In the next\\nchapter, we will describe a training algorithm that approximates the training targetutarget\\nt . Naturally, like the\\nneural network uθ\\nt , the training target should itself be a vector fieldutarget\\nt : Rd × [0, 1] → Rd. Further, utarget\\nt\\nshould do what we wantuθ\\nt to do: convert noise into data.Therefore, the goal of this chapter is to derive a\\nformula for the training targeturef\\nt such that the corresponding ODE/SDE convertspinit into pdata. Along the\\nway we will encounter two fundamental results from physics and stochastic calculus: thecontinuity equationand\\nthe Fokker-Planck equation. As before, we will first describe the key ideas for ODEs before generalizing them to\\nSDEs.\\nRemark 8\\nThere are a number of different approaches to deriving a training target for flow and diffusion models. The\\napproach we present here is both the most general and arguably most simple and is in line with recent state-\\nof-the-art models. However, it might well differ from other, older presentations of diffusion models you have\\nseen. Later, we will discuss alternative formulations.\\nFigure 4: Gradual interpolation from noise to data via a Gaussian conditional probability path for a collection of\\nimages.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 13, 'page_label': '14', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='3.1 Conditional and Marginal Probability Path\\n3.1 Conditional and Marginal Probability Path\\nThe first step of constructing the training targetutarget\\nt is by specifying aprobability path. Intuitively, a probability\\npath specifies a gradual interpolation between noisepinit and datapdata (see fig. 4). We explain the construction\\nin this section. In the following, for a data pointz ∈ Rd, we denote withδz the Dirac delta“distribution”. This\\nis the simplest distribution that one can imagine: sampling fromδz always returnsz (i.e. it is deterministic). A\\nconditional (interpolating) probability pathis a set of distributionpt(x|z) over Rd such that:\\np0(·|z) = pinit, p 1(·|z) = δz for allz ∈ Rd. (12)\\nIn other words, a conditional probability path gradually converts asingle data point into the distributionpinit (see\\ne.g. fig. 4). You can think of a probability path as a trajectory in the space of distributions. Every conditional\\nprobability pathpt(x|z) induces a marginal probability pathpt(x) defined as the distribution that we obtain by\\nfirst sampling a data pointz ∼ pdata from the data distribution and then sampling frompt(·|z):\\nz ∼ pdata, x ∼ pt(·|z) ⇒ x ∼ pt ▶ sampling from marginal path (13)\\npt(x) =\\nZ\\npt(x|z)pdata(z)dz ▶ density of marginal path (14)\\nNote that we know how to sample frompt but we don’t know the density valuespt(x) as the integral is intractable.\\nCheck for yourself that because of the conditions onpt(·|z) in eq. (12), the marginal probability pathpt interpolates\\nbetween pinit and pdata:\\np0 = pinit and p1 = pdata. ▶ noise-data interpolation (15)\\nExample 9(Gaussian Conditional Probability Path)\\nOne particularly popular probability path is theGaussian probability path. This is theprobability path used\\nby denoising diffusion models. Let αt, βt be noise schedulers: two continuously differentiable, monotonic\\nfunctions withα0 = β1 = 0 and α1 = β0 = 1. We then define the conditional probability path\\npt(·|z) = N(αtz, β2\\nt Id) ▶ Gaussian conditional path (16)\\nwhich, by the conditions we imposed onαt and βt, fulfills\\np0(·|z) = N(α0z, β2\\n0Id) = N(0, Id), and p1(·|z) = N(α1z, β2\\n1Id) = δz,\\nwhere we have used the fact that a normal distribution with zero variance and meanz is justδz. Therefore,\\nthis choice ofpt(x|z) fulfills eq. (12) forpinit = N(0, Id) and is therefore a valid conditional interpolating path.\\nThe Gaussian conditional probability path has several useful properties which makes it especially amenable to\\nour goals, and because of this we will use it as our prototypical example of a conditional probability path for\\nthe rest of the section. In fig. 4, we illustrate its application to an image. We can express sampling from the\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 14, 'page_label': '15', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='3.2 Conditional and Marginal Vector Fields\\nFigure 5: Illustration of a conditional (top) and marginal (bottom) probability path. Here, we plot a Gaussian\\nprobability path withαt = t, βt = 1 − t. The conditional probability path interpolates a Gaussianpinit = N(0, Id)\\nandpdata = δz forsingledatapoint z. ThemarginalprobabilitypathinterpolatesaGaussianandadatadistribution\\npdata (Here, pdata is a toy distribution in dimensiond = 2 represented by a chess board pattern.)\\nmarginal pathpt as:\\nz ∼pdata, ϵ∼ pinit = N(0, Id) ⇒ x = αtz + βtϵ ∼ pt ▶ sampling from marginal Gaussian path (17)\\nIntuitively, the above procedure adds more noise for lowert until time t = 0 , at which point there is only\\nnoise. In fig. 5, we plot an example of such an interpolating path between Gaussian noise and a simple data\\ndistribution.\\n3.2 Conditional and Marginal Vector Fields\\nWe proceed now by constructing a training targetutarget\\nt for a flow model using the recently defined notion of a\\nprobability path pt. The idea is to constructutarget\\nt from simple components that we can derive analytically by\\nhand.\\nTheorem 10(Marginalization trick)\\nFor every data pointz ∈ Rd, letutarget\\nt (·|z) denote aconditional vector field, defined so that the corresponding\\nODE yields the conditional probability pathpt(·|z), viz.,\\nX0 ∼ pinit, d\\ndtXt = utarget\\nt (Xt|z) ⇒ Xt ∼ pt(·|z) (0 ≤ t ≤ 1). (18)\\nThen themarginal vector fieldutarget\\nt (x), defined by\\nutarget\\nt (x) =\\nZ\\nutarget\\nt (x|z)pt(x|z)pdata(z)\\npt(x) dz, (19)\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 15, 'page_label': '16', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='3.2 Conditional and Marginal Vector Fields\\nFigure 6: Illustration of theorem 10. Simulating a probability path with ODEs. Data distributionpdata in blue\\nbackground. Gaussian pinit in red background. Top row: Conditional probability path. Left: Ground truth\\nsamples from conditional pathpt(·|z). Middle: ODE samples over time. Right: Trajectories by simulating ODE\\nwith utarget\\nt (x|z) in eq. (21). Bottom row: Simulating a marginal probability path. Left: Ground truth samples\\nfrom pt. Middle: ODE samples over time. Right: Trajectories by simulating ODE with marginal vector field\\nuflow\\nt (x). As one can see, the conditional vector field follows the conditional probability path and the marginal\\nvector field follows the marginal probability path.\\nfollows the marginal probability path, i.e.\\nX0 ∼ pinit, d\\ndtXt = utarget\\nt (Xt) ⇒ Xt ∼ pt (0 ≤ t ≤ 1). (20)\\nIn particular,X1 ∼ pdata for this ODE, so that we might say \"utarget\\nt converts noisepinit into datapdata\".\\nSee fig. 6 for an illustration. Before we prove the marginalization trick, let us first explain why it is useful: The\\nmarginalization trick from theorem 10 allows us to construct the marginal vector field from a conditional vector field.\\nThis simplifies the problem of finding a formula for a training target significantly as we can often find a conditional\\nvector fieldutarget\\nt (·|z) satisfying eq. (18) analytically by hand (i.e. by just doing some algebra ourselves). Let us\\nillustrate this by deriving a conditional vector fieldut(x|z) for our running example of a Gaussian probability path.\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 16, 'page_label': '17', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='3.2 Conditional and Marginal Vector Fields\\nExample 11(Target ODE for Gaussian probability paths)\\nAs before, letpt(·|z) = N(αtz, β2\\nt Id) for noise schedulersαt, βt (see eq. (16)). Let ˙αt = ∂tαt and ˙βt = ∂tβt\\ndenote respective time derivatives ofαt and βt. Here, we want to show that theconditional Gaussian vector\\nfield given by\\nutarget\\nt (x|z) =\\n \\n˙αt −\\n˙βt\\nβt\\nαt\\n!\\nz +\\n˙βt\\nβt\\nx (21)\\nis a valid conditional vector field model in the sense of theorem 10: its ODE trajectoriesXt satisfy Xt ∼\\npt(·|z) = N(αtz, β2\\nt Id) if X0 ∼ N(0, Id). In fig. 6, we confirm this visually by comparing samples from the\\nconditional probability path (ground truth) to samples from simulated ODE trajectories of this flow. As you\\ncan see, the distribution match. We will now prove this.\\nProof. Let us construct a conditional flow modelψtarget\\nt (x|z) first by defining\\nψtarget\\nt (x|z) = αtz + βtx. (22)\\nIf Xt is the ODE trajectory ofψtarget\\nt (·|z) with X0 ∼ pinit = N(0, Id), then by definition\\nXt = ψtarget\\nt (X0|z) = αtz + βtX0 ∼ N(αtz, β2Id) = pt(·|z).\\nWe conclude that the trajectories are distributed like the conditional probability path (i.e, eq. (18) is fulfilled).\\nIt remains to extract the vector fieldutarget\\nt (x|z) from ψtarget\\nt (x|z). By the definition of a flow (eq. (2b)), it\\nholds\\nd\\ndtψtarget\\nt (x|z) = utarget\\nt (ψtarget\\nt (x|z)|z) for allx, z∈ Rd\\n(i)\\n⇔ ˙αtz + ˙βtx = utarget\\nt (αtz + βtx|z) for allx, z∈ Rd\\n(ii)\\n⇔ ˙αtz + ˙βt\\n\\x12x − αtz\\nβt\\n\\x13\\n= utarget\\nt (x|z) for allx, z∈ Rd\\n(iii)\\n⇔\\n \\n˙αt −\\n˙βt\\nβt\\nαt\\n!\\nz +\\n˙βt\\nβt\\nx = utarget\\nt (x|z) for allx, z∈ Rd\\nwhere in(i) we used the definition ofψtarget\\nt (x|z) (eq. (22)), in(ii) we reparameterizedx → (x − αtz)/βt, and\\nin (iii) we just did some algebra. Note that the last equation is the conditional Gaussian vector field as we\\ndefined in eq. (21). This proves the statement.a\\naOne can also double check this by plugging it into the continuity equation introduced later in this section.\\nThe remainder of this section will now prove theorem 10 via thecontinuity equation, a fundamental result in\\nmathematics and physics. To explain it, we will require the use of thedivergence operator div, which we define as\\ndiv(vt)(x) =\\ndX\\ni=1\\n∂\\n∂xi\\nvt(x) (23)\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 17, 'page_label': '18', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='3.3 Conditional and Marginal Score Functions\\nTheorem 12(Continuity Equation)\\nLet us consider an flow model with vector fieldutarget\\nt with X0 ∼ pinit. Then Xt ∼ pt for all0 ≤ t ≤ 1 if and\\nonly if\\n∂tpt(x) = −div(ptutarget\\nt )(x) for allx ∈ Rd, 0 ≤ t ≤ 1, (24)\\nwhere ∂tpt(x) = d\\ndt pt(x) denotes the time-derivative ofpt(x). Equation 24 is known as thecontinuity equation.\\nFor the mathematically-inclined reader, we present a self-contained proof of the Continuity Equation in appendix B.\\nBefore we move on, let us try and understand intuitively the continuity equation. The left-hand side∂tpt(x)\\ndescribes how much the probabilitypt(x) at x changes over time. Intuitively, the change should correspond to the\\nnet inflow of probability mass. For a flow model, a particleXt follows along the vector fieldutarget\\nt . As you might\\nrecall from physics, the divergence measures a sort of net outflow from the vector field. Therefore, the negative\\ndivergence measures the net inflow. Scaling this by the total probability mass currently residing atx, we get that\\nthe net−div(ptut) measures the total inflow of probability mass. Since probability mass is conserved, the left-hand\\nand right-hand side of the equation should be the same! We now proceed with a proof of the marginalization trick\\nfrom theorem 10.\\nProof. By theorem 12, we have to show that the marginal vector fieldutarget\\nt , as defined as in eq. (19), satisfies the\\ncontinuity equation. We can do this by direct calculation:\\n∂tpt(x)\\n(i)\\n= ∂t\\nZ\\npt(x|z)pdata(z)dz\\n=\\nZ\\n∂tpt(x|z)pdata(z)dz\\n(ii)\\n=\\nZ\\n−div(pt(·|z)utarget\\nt (·|z))(x)pdata(z)dz\\n(iii)\\n= −div\\n\\x12Z\\npt(x|z)utarget\\nt (x|z)pdata(z)dz\\n\\x13\\n(iv)\\n= −div\\n\\x12\\npt(x)\\nZ\\nutarget\\nt (x|z)pt(x|z)pdata(z)\\npt(x) dz\\n\\x13\\n(x)\\n(v)\\n= −div\\n\\x00\\nptutarget\\nt\\n\\x01\\n(x),\\nwhere in(i) we used the definition ofpt(x) in eq. (13), in(ii) we used the continuity equation for the conditional\\nprobability pathpt(·|z), in(iii) we swapped the integral and divergence operator using eq. (23), in(iv) we multiplied\\nand divided bypt(x), and in(v) we used eq. (19). The beginning and end of the above chain of equations show\\nthat the continuity equation is fulfilled forutarget\\nt . By theorem 12, this is enough to imply eq. (20), and we are\\ndone.\\n3.3 Conditional and Marginal Score Functions\\nWe just successfully constructed a training target for a flow model. We now extend this reasoning to SDEs. To\\ndo so, let us define themarginal score functionof pt as ∇log pt(x). We can use this to extend the ODE from the\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 18, 'page_label': '19', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='3.3 Conditional and Marginal Score Functions\\nFigure 7: Illustration of theorem 13. Simulating a probability path with SDEs. This repeats the plots from fig. 6\\nwith SDE sampling using eq. (25). Data distributionpdata in blue background. Gaussianpinit in red background.\\nTop row: Conditional path. Bottom row: Marginal probability path. As one can see, the SDE transports samples\\nfrom pinit into samples fromδz (for the conditional path) and topdata (for the marginal path).\\nprevious section to an SDE, as the following result demonstrates.\\nTheorem 13(SDE extension trick)\\nDefine the conditional and marginal vector fields utarget\\nt (x|z) and utarget\\nt (x) as before. Then, for diffusion\\ncoefficient σt ≥ 0, we may construct an SDE which follows the same probability path:\\nX0 ∼pinit, dXt =\\n\\x14\\nutarget\\nt (Xt) + σ2\\nt\\n2 ∇log pt(Xt)\\n\\x15\\ndt + σtdWt (25)\\n⇒ Xt ∼pt (0 ≤ t ≤ 1) (26)\\nIn particular, X1 ∼ pdata for this SDE. The same identity holds if we replace the marginal probabilitypt(x)\\nand vector fieldutarget\\nt (x) with the conditional probability pathpt(x|z) and vector fieldutarget\\nt (x|z).\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 19, 'page_label': '20', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='3.3 Conditional and Marginal Score Functions\\nWe illustrate the theorem in fig. 7. The formula in theorem 13 is useful because, similar to before, we can express\\nthe marginal score function via theconditional score function∇log pt(x|z)\\n∇log pt(x) = ∇pt(x)\\npt(x) = ∇\\nR\\npt(x|z)pdata(z)dz\\npt(x) =\\nR\\n∇pt(x|z)pdata(z)dz\\npt(x) =\\nZ\\n∇log pt(x|z)pt(x|z)pdata(z)\\npt(x) dz (27)\\nand the conditional score function∇log pt(x|z) is something we usually know analytically, as illustrated by the\\nfollowing example.\\nExample 14(Score Function for Gaussian Probability Paths.)\\nFor the Gaussian pathpt(x|z) = N(x; αtz, β2\\nt Id), we can use the form of the Gaussian probability density (see\\neq. (81)) to get\\n∇log pt(x|z) = ∇log N(x; αtz, β2\\nt Id) = −x − αtz\\nβ2\\nt\\n. (28)\\nNote that the score is a linear function ofx. This is a unique feature of Gaussian distributions.\\nIn the remainder of this section, we will prove theorem 13 via theFokker-Planck equation, which extends the\\ncontinuity equation from ODEs to SDEs. To do so, let us first define theLaplacian operator ∆ via\\n∆wt(x) =\\ndX\\ni=1\\n∂2\\n∂2xi\\nwt(x) = div(∇wt)(x). (29)\\nTheorem 15(Fokker-Planck Equation)\\nLet pt be a probability path and let us consider the SDE\\nX0 ∼ pinit, dXt = ut(Xt)dt + σtdWt.\\nThen Xt has distributionpt for all0 ≤ t ≤ 1 if and only if theFokker-Planck equationholds:\\n∂tpt(x) = −div(ptut)(x) + σ2\\nt\\n2 ∆pt(x) for allx ∈ Rd, 0 ≤ t ≤ 1. (30)\\nA self-contained proof of the Fokker-Planck equation can be found in appendix B. Note that the continuity equation\\nis recovered from the Fokker-Planck equation whenσt = 0. The additional Laplacian term∆pt might be hard to\\nrationalize at first. Those familiar with physics will note that the same term also appears in the heat equation\\n(which is in fact a special case of the Fokker-Planck equation). Heat diffuses through a medium. We also add a\\ndiffusion process (not a physical but a mathematical one) and hence we add this additional Laplacian term. Let\\nus now use the Fokker-Planck equation to help us prove theorem 13.\\nProof of Theorem 13.By theorem 15, we need to show that that the SDE defined in eq. (25) satisfies the Fokker-\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 20, 'page_label': '21', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='3.3 Conditional and Marginal Score Functions\\nFigure 8: Top row: Particles evolving under the Langevin dynamics given by eq. (31), withp(x) taken to be a\\nGaussian mixture with 5 modes. Bottom row: A kernel density estimate of the same samples shown in the top row.\\nAs one can see, the distribution of samples converges to the equilibrium distributionp (blue background colour).\\nPlanck equation forpt. We can do this by direction calculation:\\n∂tpt(x)\\n(i)\\n= − div(ptutarget\\nt )(x)\\n(ii)\\n= − div(ptutarget\\nt )(x) − σ2\\nt\\n2 ∆pt(x) + σ2\\nt\\n2 ∆pt(x)\\n(iii)\\n= − div(ptutarget\\nt )(x) − div(σ2\\nt\\n2 ∇pt)(x) + σ2\\nt\\n2 ∆pt(x)\\n(iv)\\n= − div(ptutarget\\nt )(x) − div(pt\\n\\x14σ2\\nt\\n2 ∇log pt\\n\\x15\\n)(x) + σ2\\nt\\n2 ∆pt(x)\\n(v)\\n= − div\\n\\x12\\npt\\n\\x14\\nutarget\\nt + σ2\\nt\\n2 ∇log pt\\n\\x15\\x13\\n(x) + σ2\\nt\\n2 ∆pt(x),\\nwhere in(i) we used the Contuity Equation, in(ii) we added and subtracted the same term, in(iii) we used the\\ndefinition of the Laplacian (eq. (29)), in(iv) we used that ∇log pt = ∇pt\\npt\\n, and in (v) we used the linearity of\\nthe divergence operator. The above derivation shows that the SDE defined in eq. (25) satisfies the Fokker-Planck\\nequation forpt. By theorem 15, this impliesXt ∼ pt for 0 ≤ t ≤ 1, as desired.\\nRemark 16(Langevin dynamics.)\\nThe above construction has a famous special case when the probability path is static, i.e.pt = p for a fixed\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 21, 'page_label': '22', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='3.3 Conditional and Marginal Score Functions\\ndistribution p. In this case, we setutarget\\nt = 0 and obtain the SDE\\ndXt = σ2\\nt\\n2 ∇log p(Xt)dt + σtdWt, (31)\\nwhich is commonly known asLangevin dynamics. The fact thatpt is static implies that∂tpt(x) = 0. It follows\\nimmediately from theorem 13 that these dynamics satisfy the Fokker-Planck equation for the static pathpt = p\\nin theorem 13. Therefore, we may conclude thatp is a stationary distribution of Langevin dynamics, so that\\nX0 ∼ p ⇒ Xt ∼ p (t ≥ 0)\\nAs with many Markov chains, these dynamics converge to the stationary distributionp under rather general\\nconditions (see section 3.3). That is, if we instead we takeX0 ∼ p′ ̸= p, so thatXt ∼ p′\\nt, then under mild\\nconditions pt → p. This fact makes Langevin dynamics extremely useful, and it accordingly serves as the basis\\nfor e.g.,molecular dynamicssimulations, and many other Markov chain Monte Carlo (MCMC) methods across\\nBayesian statistics and the natural sciences.\\nLet us summarize the results of this section.\\nSummary 17(Derivation of the Training Target)\\nTheflowtrainingtargetisthemarginalvectorfield utarget\\nt . Toconstructit, wechoosea conditionalprobability\\npath pt(x|z) that fulfils p0(·|z) = pinit, p1(·|z) = δz. Next, we find aconditional vector fielduflow\\nt (x|z) such\\nthat its corresponding flowψtarget\\nt (x|z) fulfills\\nX0 ∼ pinit ⇒ Xt = ψtarget\\nt (X0|z) ∼ pt(·|z),\\nor, equivalently, thatutarget\\nt satisfies the continuity equation. Then themarginal vector fielddefined by\\nutarget\\nt (x) =\\nZ\\nutarget\\nt (x|z)pt(x|z)pdata(z)\\npt(x) dz, (32)\\nfollows the marginal probability path, i.e.,\\nX0 ∼ pinit, dXt = utarget\\nt (Xt)dt ⇒ Xt ∼ pt (0 ≤ t ≤ 1). (33)\\nIn particular,X1 ∼ pdata for this ODE, so thatutarget\\nt \"converts noise into data\", as desired.\\nExtending to SDEs. For a time-dependent diffusion coefficientσt ≥ 0, we can extend the above ODE to\\nan SDE with the same marginal probability path:\\nX0 ∼pinit, dXt =\\n\\x14\\nutarget\\nt (Xt) + σ2\\nt\\n2 ∇log pt(Xt)\\n\\x15\\ndt + σtdWt (34)\\n⇒ Xt ∼pt (0 ≤ t ≤ 1), (35)\\nwhere ∇log pt(x) is themarginal score function\\n∇log pt(x) =\\nZ\\n∇log pt(x|z)pt(x|z)pdata(z)\\npt(x) dz. (36)\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 22, 'page_label': '23', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='3.3 Conditional and Marginal Score Functions\\nIn particular, for the trajectoriesXt of the above SDE, it holds thatX1 ∼ pdata, so that the SDE \"converts\\nnoise into data\", as desired. An important example is theGaussian probability path, yielding the formulae:\\npt(x|z) =N(x; αtz, β2\\nt Id) (37)\\nuflow\\nt (x|z) =\\n \\n˙αt −\\n˙βt\\nβt\\nαt\\n!\\nz +\\n˙βt\\nβt\\nx (38)\\n∇log pt(x|z) = − x − αtz\\nβ2\\nt\\n, (39)\\nfor noise schedulers αt, βt ∈ R: continuously differentiable, monotonic functions such that α0 = β1 = 0\\nα1 = β0 = 1.\\n23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 23, 'page_label': '24', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='4 Training the Generative Model\\nIn the last two sections, we showed how to construct a generative model with a vector fielduθ\\nt given by a neural\\nnetwork, and we derived a formula for the training targetutarget\\nt . In this section, we will describe how to train the\\nneural networkuθ\\nt to approximate the training targetutarget\\nt . First, we restrict ourselves to ODEs again, in doing\\nso recoveringflow matching. Second, we explain how to extend the approach to SDEs viascore matching. Finally,\\nwe consider the special case of Gaussian probability paths, in doing so recoveringdenoising diffusion models. With\\nthese tools, we will at last have an end-to-end procedure to train and sample from a generative model with ODEs\\nand SDEs.\\n4.1 Flow Matching\\nAs before, let us consider a flow model given by\\nX0 ∼ pinit, dXt = uθ\\nt (Xt) dt. ▶ flow model (40)\\nAs we learned, we want the neural networkuθ\\nt to equal the marginal vector fieldutarget\\nt . In other words, we\\nwould like to find parametersθ so thatuθ\\nt ≈ utarget\\nt . In the following, we denote by Unif= Unif[0,1] the uniform\\ndistribution on the interval[0, 1], and byE the expected value of a random variable. An intuitive way of obtaining\\nuθ\\nt ≈ utarget\\nt is to use a mean-squared error, i.e. to use theflow matching lossdefined as\\nLFM(θ) = Et∼Unif,x∼pt[∥uθ\\nt (x) − utarget\\nt (x)∥2] (41)\\n(i)\\n= Et∼Unif,z∼pdata,x∼pt(·|z)[∥uθ\\nt (x) − utarget\\nt (x)∥2], (42)\\nwhere pt(x) =\\nR\\npt(x|z)pdata(z)dz is the marginal probability path and in(i) we used the sampling procedure given\\nby eq. (13). Intuitively, this loss says: First, draw a random timet ∈ [0, 1]. Second, draw a random pointz from our\\ndata set, sample frompt(·|z) (e.g., by adding some noise), and computeuθ\\nt (x). Finally, compute the mean-squared\\nerror between the output of our neural network and the marginal vector fieldutarget\\nt (x). Unfortunately, we arenot\\ndone here. While we do know the formula forutarget\\nt by theorem 10\\nutarget\\nt (x) =\\nZ\\nutarget\\nt (x|z)pt(x|z)pdata(z)\\npt(x) dz, (43)\\nwe cannot compute it efficiently because the above integral is intractable. Instead, we will exploit the fact that the\\nconditional velocity fieldutarget\\nt (x|z) is tractable. To do so, let us define theconditional flow matching loss\\nLCFM(θ) = Et∼Unif,z∼pdata,x∼pt(·|z)[∥uθ\\nt (x) − utarget\\nt (x|z)∥2]. (44)\\nNote the difference to eq. (41): we use the conditional vector field utarget\\nt (x|z) instead of the marginal vector\\nutarget\\nt (x). As we have an analytical formula forutarget\\nt (x|z), we can minimize the above loss easily. But wait, what\\nsense does it make to regress against the conditional vector field if it’s the marginal vector field we care about?\\nAs it turns out, byexplicitly regressing against the tractable, conditional vector field, we areimplicitly regressing\\nagainst the intractable, marginal vector field. The next result makes this intuition precise.\\n24'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 24, 'page_label': '25', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='4.1 Flow Matching\\nTheorem 18\\nThe marginal flow matching loss equals the conditional flow matching loss up to a constant. That is,\\nLFM(θ) = LCFM(θ) + C,\\nwhere C is independent ofθ. Therefore, their gradients coincide:\\n∇θLFM(θ) = ∇θLCFM(θ).\\nHence, minimizing LCFM(θ) with e.g., stochastic gradient descent (SGD) is equivalent to minimizingLFM(θ)\\nwith in the same fashion. In particular, for the minimizer θ∗ of LCFM(θ), it will hold that uθ∗\\nt = utarget\\nt\\n(assuming an infintely expressive parameterization).\\nProof. The proof works by expanding the mean-squared error into three components and removing constants:\\nLFM(θ)\\n(i)\\n= Et∼Unif,x∼pt[∥uθ\\nt (x) − utarget\\nt (x)∥2]\\n(ii)\\n= Et∼Unif,x∼pt[∥uθ\\nt (x)∥2 − 2uθ\\nt (x)T utarget\\nt (x) + ∥utarget\\nt (x)∥2]\\n(iii)\\n= Et∼Unif,x∼pt\\n\\x02\\n∥uθ\\nt (x)∥2\\x03\\n− 2Et∼Unif,x∼pt[uθ\\nt (x)T utarget\\nt (x)] + Et∼Unif[0,1],x∼pt[∥utarget\\nt (x)∥2]\\n| {z }\\n=:C1\\n(iv)\\n= Et∼Unif,z∼pdata,x∼pt(·|z)[∥uθ\\nt (x)∥2] − 2Et∼Unif,x∼pt[uθ\\nt (x)T utarget\\nt (x)] + C1\\nwhere (i) holds by definition, in(ii) we used the formula∥a−b∥2 = ∥a∥2 −2aT b+∥b∥2, in(iii) we define a constant\\nC1 and in(iv) we used the sampling procedure ofpt given by eq. (13). Let us reexpress the second summand:\\nEt∼Unif,x∼pt[uθ\\nt (x)T utarget\\nt (x)]\\n(i)\\n=\\n1Z\\n0\\nZ\\npt(x)uθ\\nt (x)T utarget\\nt (x) dx dt\\n(ii)\\n=\\n1Z\\n0\\nZ\\npt(x)uθ\\nt (x)T\\n\\x14Z\\nutarget\\nt (x|z)pt(x|z)pdata(z)\\npt(x) dz\\n\\x15\\ndx dt\\n(iii)\\n=\\n1Z\\n0\\nZ Z\\nuθ\\nt (x)T utarget\\nt (x|z)pt(x|z)pdata(z) dz dx dt\\n(iv)\\n= Et∼Unif,z∼pdata,x∼pt(·|z)[uθ\\nt (x)T utarget\\nt (x|z)]\\nwhere in (i) we expressed the expected value as an integral, in(ii) we use eq. (43), in(iii) we use the fact that\\nintegrals are linear, in(iv) we express the integral as an expected value. Note that this was really the crucial\\nstep of the proof: The beginning of the equality used the marginal vector fieldutarget\\nt (x), while the end uses the\\n25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 25, 'page_label': '26', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='4.1 Flow Matching\\nconditional vector fieldutarget\\nt (x|z). We plug is into the equation forLFM to get:\\nLFM(θ)\\n(i)\\n= Et∼Unif,z∼pdata,x∼pt(·|z)[∥uθ\\nt (x)∥2] − 2Et∼Unif,z∼pdata,x∼pt(·|z)[uθ\\nt (x)T utarget\\nt (x|z)] + C1\\n(ii)\\n= Et∼Unif,z∼pdata,x∼pt(·|z)[∥uθ\\nt (x)∥2 − 2uθ\\nt (x)T utarget\\nt (x|z) + ∥utarget\\nt (x|z)∥2 − ∥utarget\\nt (x|z)∥2] + C1\\n(iii)\\n= Et∼Unif,z∼pdata,x∼pt(·|z)[∥uθ\\nt (x) − utarget\\nt (x|z)∥2] + Et∼Unif,z∼pdata,x∼pt(·|z)[−∥utarget\\nt (x|z)∥2]| {z }\\nC2\\n+C1\\n(iv)\\n= LCFM(θ) + C2 + C1| {z }\\n=:C\\nwhere in(i) we plugged in the derived equation, in(ii) we added and subtracted the same value, in(iii) we used\\nthe formula∥a−b∥2 = ∥a∥2 −2aT b+∥b∥2 again, and in(iv) we defined a constant inθ. This finishes the proof.\\nOnce uθ\\nt has been trained, we may simulate the flow model\\ndXt = uθ\\nt (Xt) dt, X 0 ∼ pinit (45)\\nvia e.g., algorithm 1 to obtain samplesX1 ∼ pdata. This whole pipeline is calledflow matchingin the literature\\n[14, 16, 1, 15]. The training procedure is summarized in algorithm 5 and visualized in fig. 9. Let us now instantiate\\nthe conditional flow matching loss for the choice of Gaussian probability paths:\\nExample 19(Flow Matching for Gaussian Conditional Probability Paths)\\nLet us return to the example of Gaussian probability pathspt(·|z) = N(αtz; β2\\nt Id), where we may sample from\\nthe conditional path via\\nϵ ∼ N(0, Id) ⇒ xt = αtz + βtϵ ∼ N(αtz, β2\\nt Id) = pt(·|z). (46)\\nAs we derived in eq. (21), the conditional vector fieldutarget\\nt (x|z) is given by\\nutarget\\nt (x|z) =\\n \\n˙αt −\\n˙βt\\nβt\\nαt\\n!\\nz +\\n˙βt\\nβt\\nx, (47)\\nwhere ˙αt = ∂tαt and ˙βt = ∂tβt are the respective time derivatives. Plugging in this formula, the conditional\\nflow matching loss reads\\nLCFM(θ) = Et∼Unif,z∼pdata,x∼N(αtz,β2\\nt Id)[∥uθ\\nt (x) −\\n \\n˙αt −\\n˙βt\\nβt\\nαt\\n!\\nz −\\n˙βt\\nβt\\nx∥2]\\n(i)\\n= Et∼Unif,z∼pdata,ϵ∼N(0,Id)[∥uθ\\nt (αtz + βtϵ) − ( ˙αtz + ˙βtϵ)∥2]\\nwhere in(i) we plugged in eq. (46) and replacedx byαtz+βtϵ. Note the simplicity ofLCFM : We sample a data\\npoint z, sample some noiseϵ and then we take a mean squared error. Let us make this even more concrete for\\nthe special case ofαt = t, andβt = 1 −t. The corresponding probabilitypt(x|z) = N(tz, (1 −t)2) is sometimes\\n26'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 26, 'page_label': '27', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='4.1 Flow Matching\\nFigure 9: Illustration of theorem 18 with a Gaussian CondOT probability path: simulating an ODE from a trained\\nflow matching model. The data distribution is the chess board pattern (top right). Top row: Histogram from\\nground truth marginal probability pathpt(x). Bottom row: Histogram of samples from flow matching model. As\\none can see, the top row and bottom row match after training (up to training error). The model was trained using\\nalgorithm 5.\\nreferred to as the (Gaussian)CondOT probability path. Then we have˙αt = 1, ˙βt = −1, so that\\nLcfm(θ) =Et∼Unif,z∼pdata,ϵ∼N(0,Id)[∥uθ\\nt (tz + (1 − t)ϵ) − (z − ϵ)∥2]\\nMany famous state-of-the-art models have been trained using this simple yet effective procedure, e.g.Stable\\nDiffusion 3, Meta’sMovie Gen Video, and probably many more proprietary models. In fig. 9, we visualize it\\nin a simple example and in algorithm 5 we summarize the training procedure.\\nAlgorithm 3Flow Matching Training Procedure (here for Gaussian CondOT pathpt(x|z) = N(tz, (1 − t)2))\\nRequire: A dataset of samplesz ∼ pdata, neural networkuθ\\nt\\n1: for each mini-batch of datado\\n2: Sample a data examplez from the dataset.\\n3: Sample a random timet ∼ Unif[0,1].\\n4: Sample noiseϵ ∼ N(0, Id)\\n5: Set x = tz + (1 − t)ϵ (General case: x ∼ pt(·|z))\\n6: Compute loss\\nL(θ) =∥uθ\\nt (x) − (z − ϵ)∥2 (General case: = ∥uθ\\nt (x) − utarget\\nt (x|z)∥2)\\n7: Update the model parametersθ via gradient descent onL(θ).\\n8: end for\\n27'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 27, 'page_label': '28', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='4.2 Score Matching\\n4.2 Score Matching\\nLet us extend the algorithm we just found from ODEs to SDEs. Remember we can extend the target ODE to an\\nSDE with the same marginal distribution given by\\ndXt =\\n\\x14\\nutarget\\nt (Xt) + σ2\\nt\\n2 ∇log pt(Xt)\\n\\x15\\ndt + σtdWt (48)\\nX0 ∼ pinit, (49)\\n⇒ Xt ∼ pt (0 ≤ t ≤ 1) (50)\\nwhere utarget\\nt is the marginal vector field and∇log pt(x) is themarginal scorefunction represented via the formula\\n∇log pt(x) =\\nZ\\n∇log pt(x|z)pt(x|z)pdata(z)\\npt(x) dz. (51)\\nTo approximate the marginal score∇log pt, we can use a neural network that we callscore networksθ\\nt : Rd×[0, 1] →\\nRd. In the same way as before, we can design ascore matchingloss and aconditional score matchingloss:\\nLSM(θ) = Et∼Unif,z∼pdata,x∼pt(·|z)[∥sθ\\nt (x) − ∇log pt(x)∥2] ▶ score matching loss\\nLCSM(θ) = Et∼Unif,z∼pdata,x∼pt(·|z)[∥sθ\\nt (x) − ∇log pt(x|z)∥2] ▶ conditional score matching loss\\nwhere again the difference is using the marginal score∇log pt(x) vs. using the conditional score∇log pt(x|z). As\\nbefore, we ideally would want to minimize the score matching loss but can’t because we don’t know∇log pt(x).\\nBut similarly as before, the conditional score matching loss is a tractable alternative:\\nTheorem 20\\nThe score matching loss equals the conditional score matching loss up to a constant:\\nLSM(θ) = LCSM(θ) + C,\\nwhere C is independent of parametersθ. Therefore, their gradients coincide:\\n∇θLSM(θ) = ∇θLCSM(θ).\\nIn particular, for the minimizerθ∗, it will hold thatsθ∗\\nt = ∇log pt.\\nProof. Note that the formula for∇log pt (eq. (51)) looks the same as the formula forutarget\\nt (eq. (43)). Therefore,\\nthe proof is identical to the proof of theorem 18 replacingutarget\\nt with ∇log pt.\\nThe above procedure describes the vanilla procedure of training a diffusion model. After training, we can choose\\nan arbitrary diffusion coefficientσt ≥ 0 and then simulate the SDE\\nX0 ∼pinit, dXt =\\n\\x14\\nuθ\\nt (Xt) + σ2\\nt\\n2 sθ\\nt (Xt)\\n\\x15\\ndt + σtdWt, (52)\\nto generate samplesX1 ∼ pdata. In theory, everyσt should give samplesX1 ∼ pdata at perfect training. In practice,\\nwe encounter two types of errors: (1) numerical errors by simulating the SDE imperfectly and (2) training errors\\n28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 28, 'page_label': '29', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='4.2 Score Matching\\n(i.e., the modeluθ\\nt is not exactly equal toutarget\\nt ). Therefore, there is an optimal unknown noise levelσt - this can\\nbe determined empirically by just testing our different values of empirically (see e.g. [1, 12, 17]). At first sight, it\\nmight seem to be a disadvantage that we have to learn bothsθ\\nt and uθ\\nt if we wanted to use diffusion model now\\nas opposed to a flow model. However, note we can often directlysθ\\nt and uθ\\nt in a single network with two outputs,\\nso that the additional computational effort is usually minimal. Further, as we will see now for the special case of\\nthe Gaussian probability path,sθ\\nt and uθ\\nt may be converted into one another so that we don’t have to train them\\nseparately.\\nRemark 21(Denoising Diffusion Models)\\nIf you are familiar with diffusion models, you have probably encountered the termdenoising diffusion model.\\nThis model has become so popular that most people nowadays drop the word \"denoising\" and simply use the\\nterm \"diffusion model\" to describe it. In the language of this document, these are simply diffusion models with\\nGaussian probability pathspt(·|z) = N(αtz; β2\\nt Id). However, it is important to note that this might not be\\nimmediately obvious if you read some of the first diffusion model papers: they use a different time convention\\n(time is inverted) - so you need apply an appropriate time re-scaling - and they construct their probability path\\nvia so-calledforward processes(we will discuss this in section 4.3).\\nExample 22(Denoising Diffusion Models: Score Matching for Gaussian Probability Paths)\\nFirst, let us instantiate the denoising score matching loss for the case ofpt(x|z) = N(αtz, β2\\nt Id). As we derived\\nin eq. (28), the conditional score∇log pt(x|z) has the formula\\n∇log pt(x|z) = −x − αtz\\nβ2\\nt\\n. (53)\\nPlugging in this formula, the conditional score matching loss becomes:\\nLCSM(θ) = Et∼Unif,z∼pdata,x∼pt(·|z)[∥sθ\\nt (x) + x − αtz\\nβ2\\nt\\n∥2]\\n(i)\\n= Et∼Unif,z∼pdata,ϵ∼N(0,Id)[∥sθ\\nt (αtz + βtϵ) + ϵ\\nβt\\n∥2]\\n= Et∼Unif,z∼pdata,ϵ∼N(0,Id)[ 1\\nβ2\\nt\\n∥βtsθ\\nt (αtz + βtϵ) + ϵ∥2]\\nwhere in(i) we plugged in eq. (46) and replacedx by αtz + βtϵ. Note that the networksθ\\nt essentially learns\\nto predict the noise that was used to corrupt a data samplez. Therefore, the above training loss is also called\\ndenoising score matchingand it was the one of the first procedures used to learn diffusion models. It was soon\\nrealized that the above loss is numerically unstable forβt ≈ 0 close to zero (i.e. denoising score matching only\\nworks if you add a sufficient amount of noise). In some of the first works on denoising diffusion models (see\\nDenoising Diffusion Probabilitic Models, [9]) it was therefore proprosed to drop the constant1\\nβ2\\nt\\nin the loss\\nand reparameterizesθ\\nt into anoise predictornetwork ϵθ\\nt : Rd × [0, 1] → Rd via:\\n−βtsθ\\nt (x) = ϵθ\\nt (x) ⇒ L DDPM(θ) =Et∼Unif,z∼pdata,ϵ∼N(0,Id)\\n\\x02\\n∥ϵθ\\nt (αtz + βtϵ) − ϵ∥2\\x03\\n29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 29, 'page_label': '30', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='4.2 Score Matching\\nAs before, the networkϵθ\\nt essentially learns to predict the noise that was used to corrupt a data samplez. In\\nalgorithm 4, we summarize the training procedure.\\nAlgorithm 4Score Matching Training Procedure for Gaussian probability path\\nRequire: A dataset of samplesz ∼ pdata, score networksθ\\nt or noise predictorϵθ\\nt\\n1: for each mini-batch of datado\\n2: Sample a data examplez from the dataset.\\n3: Sample a random timet ∼ Unif[0,1].\\n4: Sample noiseϵ ∼ N(0, Id)\\n5: Set xt = αtz + βtϵ (General case: xt ∼ pt(·|z))\\n6: Compute loss\\nL(θ) =∥sθ\\nt (xt) + ϵ\\nβt\\n∥2 (General case: = ∥sθ\\nt (xt) − ∇log pt(xt|z)∥2)\\nAlternatively: L(θ) =∥ϵθ\\nt (xt) − ϵ∥2\\n7: Update the model parametersθ via gradient descent onL(θ).\\n8: end for\\nBeyond its simplicity, there is another useful property of the Gaussian probability path: By learningsθ\\nt or ϵθ\\nt ,\\nwe also learnuθ\\nt automatically and the other way around:\\nProposition 1(Conversion formula for Gaussian probability path)\\nFor the Gaussian probability pathpt(x|z) = N(αtz, β2\\nt Id), it holds that that the conditional (resp. marginal)\\nvector field can be converted into the conditional (resp. marginal) score:\\nutarget\\nt (x|z) =\\n\\x12\\nβ2\\nt\\n˙αt\\nαt\\n− ˙βtβt\\n\\x13\\n∇log pt(x|z) + ˙αt\\nαt\\nx\\nutarget\\nt (x) =\\n\\x12\\nβ2\\nt\\n˙αt\\nαt\\n− ˙βtβt\\n\\x13\\n∇log pt(x) + ˙αt\\nαt\\nx\\nwhere the formula for the above marginal vector fieldutarget\\nt is called probability flow ODEin the literature\\n(more correctly, the corresponding ODE).\\nProof. For the conditional vector field and conditional score, we can derive:\\nutarget\\nt (x|z) =\\n \\n˙αt −\\n˙βt\\nβt\\nαt\\n!\\nz +\\n˙βt\\nβt\\nx\\n(i)\\n=\\n\\x12\\nβ2\\nt\\n˙αt\\nαt\\n− ˙βtβt\\n\\x13\\x12 αtz − x\\nβ2\\nt\\n\\x13\\n+ ˙αt\\nαt\\nx =\\n\\x12\\nβ2\\nt\\n˙αt\\nαt\\n− ˙βtβt\\n\\x13\\n∇log pt(x|z) + ˙αt\\nαt\\nx\\nwhere in (i) we just did some algebra. By taking integrals, the same identity holds for the marginal flow vector\\nfield and the marginal score function:\\nutarget(x) =\\nZ\\nutarget\\nt (x|z)pt(x|z)pdata(z)\\npt(x) dz =\\nZ \\x14\\x12\\nβ2\\nt\\n˙αt\\nαt\\n− ˙βtβt\\n\\x13\\n∇log pt(x|z) + ˙αt\\nαt\\nx\\n\\x15 pt(x|z)pdata(z)\\npt(x) dz\\n(i)\\n=\\n\\x12\\nβ2\\nt\\n˙αt\\nαt\\n− ˙βtβt\\n\\x13\\n∇log pt(x) + ˙αt\\nαt\\nx\\n30'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 30, 'page_label': '31', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='4.2 Score Matching\\nwhere in(i) we used eq. (51).\\nFigure 10: A comparison of the score, as obtained in two different ways. Top: A visualization of the score field\\nsθ\\nt (x) learned independently with score matching (see algorithm 4). Bottom: A visualization of the score field˜sθ\\nt (x)\\nparameterized usinguθ\\nt (x) as in eq. (55).\\nWe can use the conversion formula to parameterize the score networksθ\\nt and the vector field networkuθ\\nt into one\\nanother via\\nuθ\\nt =\\n\\x12\\nβ2\\nt\\n˙αt\\nαt\\n− ˙βtβt\\n\\x13\\nsθ\\nt (x) + ˙αt\\nαt\\nx. (54)\\nSimilarly, so long asβ2\\nt ˙αt − αt ˙βtβt ̸= 0 (always true fort ∈ [0, 1)), it follows that\\nsθ\\nt (x) = αtuθ\\nt (x) − ˙αtx\\nβ2\\nt ˙αt − αt ˙βtβt\\n. (55)\\nUsing this parameterization, it can be shown that the denoising score matching and the conditional flow matching\\nlosses are the same up to a constant. We conclude that for Gaussian probability paths there is no need to\\nseparately train both the marginal score and the marginal vector field, as knowledge of one is sufficient to\\ncompute the other. In particular, we can choose whether we want to use flow matching or score matching\\nto train it. In fig. 10, we compare visually the score as approximated using score matching and the parameterized\\nscore using eq. (55). If we have trained a score networksθ\\nt , we know by eq. (52) that we can use arbitraryσt ≥ 0\\nto sample from the SDE\\nX0 ∼pinit, dXt =\\n\\x14\\x12\\nβ2\\nt\\n˙αt\\nαt\\n− ˙βtβt + σ2\\nt\\n2\\n\\x13\\nsθ\\nt (x) + ˙αt\\nαt\\nx\\n\\x15\\ndt + σtdWt (56)\\n31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 31, 'page_label': '32', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='4.3 A Guide to the Diffusion Model Literature\\nto obtain samplesX1 ∼ pdata (up to training and simulation error). This corresponds tostochastic sampling from\\na denoising diffusion model.\\n4.3 A Guide to the Diffusion Model Literature\\nThere is a whole family of models around diffusion models and flow matching in the literature. When you read\\nthese papers, you will likely find a different (but equivalent) way of presenting the material from this class. This\\nmakes it sometimes a little confusing to read these papers. For this reason, we want to give a brief overview over\\nvarious frameworks and their differences and put them also in their historical context.This is not necessary to\\nunderstand the remainder of this documentbut rather intended to be a support for you in case you read the\\nliterature.\\nDiscrete time vs. continuous time. The first denoising diffusion model papers [28, 29, 9] did not use SDEs but\\nconstructed Markov chains indiscrete time, i.e. with time stepst = 0, 1, 2, 3, . . .. To this date, you will find a lot\\nof works in the literature working with this discrete-time formulation. While this construction is appealing due to\\nits simplicity, the disadvantage of the time-discrete approach is that it forces you to choose a time discretization\\nbefore training. Further, the loss function needs to be approximated via anevidence lower bound (ELBO)- which\\nis, as the name suggests, only a lower bound to the loss we actually want to minimize. Later, Song et al. [32]\\nshowed that these constructions were essentially an approximation of a time-continuous SDEs. Further, the ELBO\\nloss becomes tight (i.e. it is not a lower bound anymore) in the continuous time case (e.g. note that theorem 18\\nand theorem 20 are equalities and not lower bounds - this would be different in the discrete time case). This made\\nthe SDE construction popular because it was considered mathematically \"cleaner\" and that one could control the\\nsimulation error via ODE/SDE samplers post training. It is important to note however that both models employ\\nthe same loss and arenot fundamentally different.\\n\"Forward process\" vs probability paths.The first wave of denoising diffusion models [28, 29, 9, 32] did not use\\nthe term probability path but constructed a noising procedure of a data pointz ∈ Rd via a so-called forward\\nprocess. This is an SDE of the form\\n¯X0 = z, d ¯Xt = uforw\\nt ( ¯Xt)dt + σforw\\nt d ¯Wt (57)\\nThe idea is that after drawing a data pointz ∼ pdata one simulates the forward process and thereby corrupts or\\n\"noises\" the data. The forward process is designed such that fort → ∞its distribution converges to a Gaussian\\nN(0, Id). In other words, for T ≫ 0 it holds that ¯XT ∼ N(0, Id) approximately. Note that this essentially\\ncorresponds to a probability path: the conditional distribution of ¯Xt given ¯X0 = z is a conditional probability\\npath ¯pt(·|z) and the distribution of¯Xt marginalized overz ∼ pdata corresponds to a marginal probability path¯pt.2\\nHowever, note that with this construction, we need to know the distribution ofXt|X0 = z in closed form in order\\nto train our models to avoid simulating the SDE. This essentially restrict the vector fielduforw\\nt to ones such that\\nwe know the distribution¯Xt| ¯X0 = z in closed form. Therefore, throughout the diffusion model literature, vector\\nfields in forward processes are always of the affine form, i.e.uforw\\nt (x) = atx for some continuous functionat. For\\n2Note however that they use aninverted time convention: ¯p0(·|z) = pdata here.\\n32'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 32, 'page_label': '33', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='4.3 A Guide to the Diffusion Model Literature\\nthis choice, we can use known formulas of the conditional distribution [27, 31, 12]:\\n¯Xt| ¯X0 = z ∼ N\\n\\x00\\nαtz, β2\\nt I\\n\\x01\\n, α t = exp\\n\\uf8eb\\n\\uf8ed\\ntZ\\n0\\nardr\\n\\uf8f6\\n\\uf8f8, β 2\\nt = α2\\nt\\ntZ\\n0\\n(σforw\\nr )2\\nα2r\\ndr\\nNote that these are simply Gaussian probability paths. Therefore, one can say thata forward process is a specific\\nway of constructing a (Gaussian) probability path.The term probability path was introduced by flow matching\\n[14] to both simplify the construction and make it more general at the same time: First, the \"forward process\"\\nof diffusion models is never actually simulated (only samples from¯pt(·|z) are drawn during training). Second, a\\nforward process only converges fort → ∞(i.e. we will never arrive atpinit in finite time). Therefore, we choose to\\nuse probability paths in this document.\\nTime-Reversals vs Solving the Fokker-Planck equation. The original description of diffusion models did not\\nconstruct the training targetutarget\\nt or ∇log pt via the Fokker-Planck equation (or Continuity equation) but via\\na time-reversal of the forward process [2]. A time-reversal(Xt)0≤t≤T is an SDE with the same distribution over\\ntrajectories inverted in time, i.e.\\nP[ ¯Xt1 ∈ A1, . . . ,¯Xtn ∈ An] = P[XT−t1 ∈ A1, . . . , XT−tn ∈ An] (58)\\nfor all0 ≤ t1, . . . , tn ≤ T, and A1, . . . , An ⊂ S (59)\\nAs shown in Anderson [2], one can obtain a time-reversal satisfying the above condition by the SDE:\\ndXt =\\n\\x02\\n−ut(Xt) + σ2\\nt ∇log pt(Xt)\\n\\x03\\ndt + σtdWt, u t(x) = uforw\\nT−t(x), σt = ¯σT−t\\nAs ut(Xt) = atXt, the above corresponds to a specific instance of training target we derived in proposition 1\\n(this is not immediately trivial as different time conventions are used. See e.g. [15] for a derivation). However,\\nfor the purposes of generative modeling, we often only use the final pointX1 of the Markov process (e.g., as a\\ngenerated image) and discard earlier time points. Therefore, whether a Markov process is a “true” time-reversal\\nor follows along a probability path does not matter for many applications. Therefore, using a time-reversal is not\\nnecessary and often leads to suboptimal results, e.g. the probability flow ODE is often better [12, 17]. All ways of\\nsampling from a diffusion models that are different from the time-reversal rely again on using the Fokker-Planck\\nequation. We hope that this illustrates why nowadays many people construct the training targets directly via the\\nFokker-Planck equation - as pioneered by [14, 16, 1] and done in this class.\\nFlow Matching [14] and Stochastic Interpolants [1].The framework that we present is most closely related to\\nthe frameworks of flow matching andstochastic interpolants (SIs). As we learnt, flow matching restricts itself to\\nflows. In fact, one of the key innovations of flow matching was to show that one does not need a construction via a\\nforward process and SDEs but flow models alone can be trained in a scalable manner. Due to this restriction, you\\nshould keep in mind that sampling from a flow matching model will be deterministic (only the initialX0 ∼ pinit will\\nbe random). Stochastic interpolants included both the pure flow and the SDE extension via \"Langevin dynamics\"\\nthat we use here (see theorem 13). Stochastic interpolants get their name from ainterpolant function I(t, x, z)\\nintended to interpolate between two distributions. In the terminology we use here, this corresponds to a different\\n33'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 33, 'page_label': '34', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='4.3 A Guide to the Diffusion Model Literature\\nyet (mainly) equivalent way of constructing a conditional and marginal probability path. The advantage of flow\\nmatching and stochastic interpolants over diffusion models is both their simplicity and their generality: their\\ntraining framework is very simple but at the same time they allow you to go from an arbitrary distributionpinit to\\nan arbitrary distributionpdata - while denoising diffusion models only work for Gaussian initial distributions and\\nGaussian probability path. This opens up new possibilities for generative modeling that we will touch upon briefly\\nlater in this class.\\nLet us summarize the results of this section:\\nSummary 23(Training the Generative Model)\\nFlow matchingconsists of training a neural networkuθ\\nt via minimizing theconditional flow matching loss\\nLCFM(θ) = Ez∼pdata,t∼Unif, x∼pt(·|z)[∥uθ\\nt (x) − utarget\\nt (x|z)∥2] ( conditional flow matching loss) (60)\\nwhere utarget\\nt (x|z) is the conditional vector field (see algorithm 5). After training, one generates samples by\\nsimulating the corresponding ODE (see algorithm 1). To extend this to a diffusion model, we can use ascore\\nnetworksθ\\nt and train it viaconditional score matching\\nLCSM(θ) = Ez∼pdata, t∼Unif, x∼pt(·|z)[∥sθ\\nt (x) − ∇log pt(x|z)∥2] ( denoising score matching loss) (61)\\nFor every diffusion coefficientσt ≥ 0, simulating the SDE (e.g. via algorithm 2)\\nX0 ∼pinit, dXt =\\n\\x14\\nuθ\\nt (Xt) + σ2\\nt\\n2 sθ\\nt (Xt)\\n\\x15\\ndt + σtdWt (62)\\nwill result in generating approximate samples frompdata. One can empirically find the optimalσt ≥ 0.\\nGaussian probability paths. For the special case of a Gaussian probability pathpt(x|z) = N(x; αtz, β2\\nt Id), the\\nconditional score matching is also calleddenoising score matching. This loss and conditional flow matching\\nloss are then given by:\\nLCFM(θ) =Et∼Unif,z∼pdata,ϵ∼N(0,Id)[∥uθ\\nt (αtz + βtϵ) − ( ˙αtz + ˙βtϵ)∥2]\\nLCSM(θ) =Et∼Unif,z∼pdata,ϵ∼N(0,Id)[∥sθ\\nt (αtz + βtϵ) + ϵ\\nβt\\n∥2]\\nIn this case, there is no need to trainsθ\\nt and uθ\\nt separately as we can convert them post training via the formula:\\nuθ\\nt (x) =\\n\\x12\\nβ2\\nt\\n˙αt\\nαt\\n− ˙βtβt\\n\\x13\\nsθ\\nt (x) + ˙αt\\nαt\\nx\\nAlso here, after training we can simulate the SDE in eq. (62) via algorithm 2 to obtain samplesX1.\\nDenoising diffusion models. Denoising diffusion models are diffusion models with Gaussian probability paths.\\nFor this reason, it is sufficient for them to learn eitheruθ\\nt or sθ\\nt as they can be converted into one another.\\n34'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 34, 'page_label': '35', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='4.3 A Guide to the Diffusion Model Literature\\nWhile flow matching only allows for a simulation procedure that is deterministic via ODE, they allow for a\\nsimulation that is deterministic (probability flow ODE) or stochastic (SDE sampling). However, unlike flow\\nmatching or stochastic interpolants that allow to convert arbitrary distributionspinit into arbitrary distributions\\npdata via arbitrary probability pathspt, denoising diffusion models only works for Gaussian initial distributions\\npinit = N(0, Id) and a Gaussian probability path.\\nLiterature Alternative formulations for diffusion models that are popular in the literature are:\\nDiscrete-time: Approximations of SDEs via discrete-time Markov chains are often used.\\n1. 2.Inverted time convention:It is popular to use an inverted time convention wheret = 0 corresponds to\\npdata (as opposed to here wheret = 0 corresponds topinit).\\n3. Forwardprocess: Forwardprocesses(ornoisingprocesses)arewaysofconstructing(Gaussian)probability\\npaths.\\n4. Training target via time-reversal: A training target can also be constructed via the time-reversal of\\nSDEs. This is a specific instance of the construction presented here (with an inverted time convention).\\n35'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 35, 'page_label': '36', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='5 Building an Image Generator\\nIn the previous sections, we learned how to train a flow matching or diffusion model to sample from a distribution\\npdata(x). This recipe is general and can be applied to a variety of different data types and applications. In this\\nsection, we learn how to apply this framework to build an image or video generator, such as e.g.,Stable Diffusion\\n3 and Meta Movie Gen Video. To build such a model, there are two main ingredients that we are missing: First,\\nwe will need to formulateconditional generation(guidance), e.g. how do we generate an image that fits a specific\\ntext prompt, and how our existing objectives may be suitably adapted to this end. We will also learn about\\nclassifier-free guidance, a popular technique used to enhance the quality of conditional generation. Second, we will\\ndiscuss common neural network architectures, again focusing on those designed for images and videos. Finally,\\nwe will examine in depth the two state-of-the-art image and video models mentioned above -Stable Diffusionand\\nMeta MovieGen - to give you a taste of how things are done at scale.\\n5.1 Guidance\\nSo far, the generative models we considered wereunconditional, e.g. an image model would simply generatesome\\nimage. However, the task is not merely to generate an arbitrary object, but to generate an objectconditioned on\\nsome additional information. For example, one might imagine a generative model for images which takes in a text\\nprompt y, and then generates an imagex conditioned on y. For fixed prompty, we would thus like to sample from\\npdata(x|y), that is, the data distributionconditioned ony. Formally, we think ofy to live in a spaceY. When y\\ncorresponds to a text-prompt, for example,Y would likely be some continuous space likeRdy . Wheny corresponds\\nto some discrete class label,Y would be discrete. In the lab, we will work with the MNIST dataset, in which case\\nwe will takeY = {0, 1, . . . ,9} to correspond to the identities of handwritten digits.\\nTo avoid a notation and terminology clash with the use of the word \"conditional\" to refer to conditioning on\\nz ∼ pdata (conditional probability path/vector field), we will make use of the termguided to refer specifically to\\nconditioning ony.\\nRemark 24(Guided vs. Conditional Terminology)\\nIn these notes, we opt to use the termguided in place ofconditional to refer to the act of conditioning ony.\\nHere, we will refer to e.g., aguided vector field utarget\\nt (x|y) and a conditional vector field utarget\\nt (x|z). This\\nterminology is consistent with other works such as [15].\\nThe goal ofguided generative modelingis thus to be able to sample frompdata(x|y) for any suchy. In the\\nlanguage of flow and score matching, and in which our generative models correspond to the simulation of ordinary\\nand stochastic differential equations, this can be phrased as follows.\\nKey Idea 5(Guided Generative Model)\\nWe define aguided diffusion modelto consist of aguided vector fielduθ\\nt (·|y), parameterized by some neural\\n36'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 36, 'page_label': '37', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='5.1 Guidance\\nnetwork, and a time-dependent diffusion coefficientσt, together given by\\nNeural network:uθ : Rd × Y ×[0, 1] → Rd, (x, y, t) 7→ uθ\\nt (x|y)\\nFixed:σt : [0, 1] → [0, ∞), t7→ σt\\nNotice the difference from summary 7: we are additionally guidinguθ\\nt with the inputy ∈ Y. For any such\\ny ∈ Rdy , samples may then be generated from such a model as follows:\\nInitialization: X0 ∼ pinit ▶ Initialize with simple distribution (such as a Gaussian)\\nSimulation: dXt = uθ\\nt (Xt|y)dt + σtdWt ▶ Simulate SDE fromt = 0 to t = 1.\\nGoal: X1 ∼ pdata(·|y) ▶ Goal is forX1 to be distributed likepdata(·|y).\\nWhen σt = 0, we say that such a model is aguided flow model.\\n5.1.1 Guidance for Flow Models\\nIfweimaginefixingourchoiceof y, andtakeourdatadistributionas pdata(x|y), thenwehaverecoveredtheunguided\\ngenerative problem, and can accordingly construct a generative model using the conditional flow matching objective,\\nviz.,\\nEz∼pdata(·|y),x∼pt(·|z)∥uθ\\nt (x|y) − utarget\\nt (x|z)∥2. (63)\\nNotethatthelabel y doesnotaffecttheconditionalprobabilitypath pt(·|z) ortheconditionalvectorfield utarget\\nt (x|z)\\n(although in principle, we could make it dependent). Expanding the expectation over all such choices ofy, and\\nover all timest ∈ Unif[0, 1), we thus obtain aguided conditional flow matching objective\\nLguided\\nCFM (θ) = E(z,y)∼pdata(z,y), t∼Unif[0,1), x∼pt(·|z)∥uθ\\nt (x|y) − utarget\\nt (x|z)∥2. (64)\\nOne of the main differences between the guided objective in eq. (64) and the unguided objective from eq. (44) is\\nthat here we are sampling(z, y) ∼ pdata rather than justz ∼ pdata. The reason is that our data distribution is\\nnow, in principle, a joint distribution over e.g., both imagesz and text promptsy. In practice, this means that\\na PyTorch implementation of eq. (64) would involve a dataloader which returned batches ofboth z and y. The\\nabove procedure leads to a faithful generation procedure ofpdata(·|y).\\nClassifier-Free Guidance. While the above conditional training procedure is theoretically valid, it was soon em-\\npirically realized that images samples with this procedure did not fit well enough to the desired labely. It was\\ndiscovered that perceptual quality is increased when the effect of the guidance variabley is artificially reinforced.\\nThis insight was distilled into a technique known asclassifier-free guidance that is widely used in the context\\nof state-of-the-art diffusion models, and which we discuss next. For simplicity, we will focus here on the case of\\nGaussian probability paths. Recall from eq. (16) that aGaussian conditional probability pathis given by\\npt(·|z) = N(αtz, β2\\nt Id)\\n37'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 37, 'page_label': '38', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='5.1 Guidance\\nwhere the noise schedulersαt and βt are continuously differentiable, monotonic, and satisfyα0 = β1 = 0 and\\nα1 = β0 = 1. To gain intuition for classifier-free guidance, we can use proposition 1 to rewrite the guided vector\\nfield utarget\\nt (x|y) in the following form using the guided score function∇log pt(x|y)\\nutarget\\nt (x|y) = atx + bt∇log pt(x|y), (65)\\nwhere\\n(at, bt) =\\n \\n˙αt\\nαt\\n, ˙αtβ2\\nt − ˙βtβtαt\\nαt\\n!\\n. (66)\\nHowever, notice that by Bayes’ rule, we can rewrite the guided score as\\n∇log pt(x|y) = ∇log\\n\\x12pt(x)pt(y|x)\\npt(y)\\n\\x13\\n= ∇log pt(x) + ∇log pt(y|x), (67)\\nwhere we used that the gradient∇ is taken with respect to the variablex, so that∇log pt(y) = 0. We may thus\\nrewrite\\nutarget\\nt (x|y) = atx + bt(∇log pt(x) + ∇log pt(y|x)) = utarget\\nt (x) + bt∇log pt(x|y).\\nNotice the shape of the above equation: The guided vector fieldutarget\\nt (x|y) is a sum of the unguided vector field\\nplus a guided score∇log pt(x|y). As people observed that their imagex did not fit their prompty well enough, it\\nwas a natural idea to scale up the contribution of the∇log pt(y|x) term, yielding\\n˜ut(x|y) = utarget\\nt (x) + wbt∇log pt(y|x),\\nwhere w > 1 is known as theguidance scale. Note that this is a heuristic: forw ̸= 1, it holds that˜ut(x|y) ̸=\\nutarget\\nt (x|y), i.e. therefore not the true, guided vector field. However, empirical results have shown to yield preferable\\nresults (whenw >1).\\nRemark 25(Where is the classifier?)\\nThe term log pt(y|x) can be considered as a sort of classifier of noised data (i.e. it gives the likelihoods ofy\\ngiven x). In fact, early works in diffusion trained actual classifiers and used them to the guide via the above\\nprocedure. This leads toclassifier guidance[5, 30]. As it has been largely superseded by classifier-free guidance,\\nwe do not consider it here.\\nWe may again apply the equality\\n∇log pt(x|y) = ∇log pt(x) + ∇log pt(y|x)\\n38'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 38, 'page_label': '39', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='5.1 Guidance\\nto obtain\\n˜ut(x|y) = utarget\\nt (x) + wbt∇log pt(y|x)\\n= utarget\\nt (x) + wbt(∇log pt(x|y) − ∇log pt(x))\\n= utarget\\nt (x) − (watx + wbt∇log pt(x)) + (watx + wbt∇log pt(x|y))\\n= (1 − w)utarget\\nt (x) + wutarget\\nt (x|y).\\nWe may therefore express the scaled guided vector field˜ut(x|y) as the linear combination of the unguided vector\\nfield utarget\\nt (x) with the guided vector fieldutarget\\nt (x|y). The idea might then to to train both an unguidedutarget\\nt (x)\\n(using e.g., eq. (44)) as well as a guidedutarget\\nt (x|y) (using e.g., eq. (64)), and then combine them at inference time\\nto obtain ˜ut(x|y). \"But wait!\", you might ask, \"wouldn’t we need to train two models then !?\". It turns out\\nwe do can train both in model: we may thus augment our label set with a new, additional∅ label that denotes\\nthe absence of conditioning. We can then treatutarget\\nt (x) = utarget\\nt (x|∅). With that, we do not need to train\\na separate model to reinforce the effect of a hypothetical classifier. This approach of training a conditional and\\nunconditional model in one (and subsequently reinforcing the conditioning) is known asclassifier-free guidance\\n(CFG) [10].\\nRemark 26(Derivation for general probability paths)\\nNote that the construction\\n˜ut(x|y) = (1 − w)utarget\\nt (x) + wutarget\\nt (x|y),\\nis equally valid for any choice probability path, not just a Gaussian one. Whenw = 1, it is straightforward to\\nverify that ˜ut(x|y) = utarget\\nt (x|y). Our derivation using Gaussian paths was simply to illustrate the intuition\\nbehind the construction, and in particular of amplifying the contribution of a “classifier”∇log pt(y|x).\\nTraining and Context-Free Guidance.We must now amend the guided conditional flow matching objective from\\neq. (64) to account for the possibility ofy = ∅. The challenge is that when sampling(z, y) ∼ pdata, we will never\\nobtain y = ∅. It follows that we must introduce the possibility ofy = ∅ artificially. To do so, we will define some\\nhyperparameter η to be the probability that we discard the original labely, and replace it with∅. We thus arrive\\nat ourCFG conditional flow matching training objective\\nLCFG\\nCFM(θ) = E□∥uθ\\nt (x|y) − utarget\\nt (x|z)∥2 (68)\\n□ = (z, y) ∼ pdata(z, y), t∼ Unif[0, 1), x∼ pt(·|z), replace y = ∅ with prob. η (69)\\nWe summarize our findings below.\\nSummary 27(Classifier-Free Guidance for Flow Models)\\nGiven the unguided marginal vector fieldutarget\\nt (x|∅), the guided marginal vector field utarget\\nt (x|y), and a\\nguidance scalew >1, we define theclassifier-free guided vector field˜ut(x|y) by\\n˜ut(x|y) = (1 − w)utarget\\nt (x|∅) + wutarget\\nt (x|y). (70)\\n39'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 39, 'page_label': '40', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='5.1 Guidance\\nFigure 11: The effect of classifier guidance. The prompt here is the \"class\" chosen to be \"Corgi\" (a specific type\\nof dog). Left: samples generated with no guidance (i.e.,w = 1). Right: samples generated with classifier guidance\\nand w = 4. As shown, classifier-free guidance improves the similarity to the prompt. Figure taken from [10].\\nBy approximatingutarget\\nt (x|∅) and utarget\\nt (x|y) using the same neural network, we may leverage the following\\nclassifier-free guidance CFM(CFG-CFM) objective, given by\\nLCFG\\nCFM(θ) = E□∥uθ\\nt (x|y) − utarget\\nt (x|z)∥2 (71)\\n□ = (z, y) ∼ pdata(z, y), t∼ Unif[0, 1), x∼ pt(·|z), replace y = ∅ with prob. η (72)\\nIn plain English,LCFG\\nCFM might be approximated by\\n(z, y) ∼ pdata(z, y) ▶ Sample (z, y) from data distribution.\\nt ∼ Unif[0, 1) ▶ Sample t uniformly on[0, 1).\\nx ∼ pt(x|z) ▶ Sample x from the conditional probability pathpt(x|z).\\nwith prob.η, y← ∅ ▶ Replace y with ∅ with probabilityη.\\n\\\\LCFG\\nCFM(θ) = ∥uθ\\nt (x|y) − utarget\\nt (x|z)∥2 ▶ Regress model against conditional vector field.\\nAbove, we made use multiple times of the fact thatutarget\\nt (x|z) = utarget\\nt (x|z, y). At inference time, for a fixed\\nchoice ofy, we may sample via\\nInitialization: X0 ∼ pinit(x) ▶ Initialize with simple distribution (such as a Gaussian)\\nSimulation: dXt = ˜uθ\\nt (Xt|y)dt ▶ Simulate ODE fromt = 0 to t = 1.\\nSamples: X1 ▶ Goal is forX1 to adhere to the guiding variabley.\\nNote that the distribution ofX1 is not necessarily aligned withX1 ∼ pdata(·|y) anymore if we use a weightw >1.\\nHowever, empirically, this shows better alignment with conditioning. In fig. 11, we illustrate class-based classifier-\\nfree guidance on 128x128 ImageNet, as in [10]. Similarly, in fig. 12, we visualize the affect of various guidance scales\\nw when applying classifier-free guidance to sampling from the MNIST dataset of handwritten digits.\\n40'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 40, 'page_label': '41', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='5.1 Guidance\\nFigure 12: The effect of classifier-free guidance applied at various guidance scales for the MNIST dataset of hand-\\nwritten digits. Left: Guidance scale set tow = 1.0. Middle: Guidance scale set tow = 2.0. Right: Guidance scale\\nset tow = 4.0. You will generate a similar image yourself in the lab three!\\nAlgorithm 5Classifier-free guidance training for Gaussian probability pathpt(x|z) = N(x; αtz, β2\\nt Id)\\nRequire: Paired dataset(z, y) ∼ pdata, neural networkuθ\\nt\\n1: for each mini-batch of datado\\n2: Sample a data example(z, y) from the dataset.\\n3: Sample a random timet ∼ Unif[0,1].\\n4: Sample noiseϵ ∼ N(0, Id)\\n5: Set x = αtz + βtϵ\\n6: With probabilityp drop label: y ← ∅\\n7: Compute loss\\nL(θ) =∥uθ\\nt (x|y) − ( ˙αtϵ + ˙βtz)∥2\\n8: Update the model parametersθ via gradient descent onL(θ).\\n9: end for\\n5.1.2 Guidance for Diffusion Models\\nIn this section we extend the reasoning of the previous section to diffusion models. First, in the same way that we\\nobtained eq. (64), we may generalize the conditional score matching loss eq. (61) to obtain theguided conditional\\nscore matching objective\\nLguided\\nCSM (θ) = E□[∥sθ\\nt (x|y) − ∇log pt(x|z)∥2] (73)\\n□ = (z, y) ∼ pdata(z, y), t∼ Unif, x∼ pt(·|z). (74)\\nA guided score networksθ\\nt (x|y) trained with eq. (73) might then be combined with the guided vector fielduθ\\nt (x|y)\\nto simulate the SDE\\nX0 ∼ pinit, dXt =\\n\\x14\\nuθ\\nt (Xt|y) + σ2\\nt\\n2 sθ\\nt (Xt|y)\\n\\x15\\ndt + σtdWt.\\n41'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 41, 'page_label': '42', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='5.1 Guidance\\nClassifier-Free Guidance. We now extend the classifier-free guidance construction to the diffusion setting. By\\nBayes’ rule (see eq. (67)),\\n∇log pt(x|y) = ∇log pt(x) + ∇log pt(y|x),\\nso that forguidance scalew >1 we may define\\n˜st(x|y) = ∇log pt(x) + w∇log pt(y|x)\\n= ∇log pt(x) + w(∇log pt(x|y) − ∇log pt(x))\\n= (1 − w)∇log pt(x) + w∇log pt(x|y)\\n= (1 − w)∇log pt(x|∅) + w∇log pt(x|y)\\nWe thus arrive at the CFG-compatible (that is, accounting for the possibility of∅) objective\\nLCFG\\nDSM(θ) = E□∥sθ\\nt (x|y) − ∇log pt(x|z)∥2 (75)\\n□ = (z, y) ∼ pdata(z, y), t∼ Unif[0, 1), x∼ pt(·|z), replace y = ∅ with prob. η, (76)\\nwhereη isahyperparameter(theprobabilityofreplacing y with∅). Wewillrefer LCFG\\nCSM(θ) asthe guidedconditional\\nscore matching objective. We recap as follows\\nSummary 28(Classifier-Free Guidance for Diffusions)\\nGiven the unguided marginal score∇log pt(x|∅), the guided marginal score field∇log pt(x|y), and aguidance\\nscale w >1, we define theclassifier-free guided score˜st(x|y) by\\n˜st(x|y) = (1 − w)∇log pt(x|∅) + w∇log pt(x|y). (77)\\nBy approximating∇log pt(x|∅) and ∇log pt(x|y) using the same neural networksθ\\nt (x|y), we may leverage the\\nfollowing classifier-free guidance CSM(CFG-CSM) objective, given by\\nLCFG\\nCSM(θ) = E□∥sθ\\nt (x|(1 − ξ)y + ξ∅) − ∇log pt(x|z)∥2 (78)\\n□ = (z, y) ∼ pdata(z, y), t∼ Unif[0, 1), x∼ pt(·|z), replace y = ∅ with prob. η (79)\\nIn plain English,LCFG\\nDSM might be approximated by\\n(z, y) ∼ pdata(z, y) ▶ Sample (z, y) from data distribution.\\nt ∼ Unif[0, 1) ▶ Sample t uniformly on[0, 1).\\nx ∼ pt(x|z, y) ▶ Sample x from cond. pathpt(x|z).\\nwith prob.η, y← ∅ ▶ Replace y with ∅ with probabilityη.\\n\\\\LCFG\\nDSM(θ) = ∥sθ\\nt (x|y) − ∇log pt(x|z)∥2 ▶ Regress model against conditional score.\\nAt inference time, for a fixed choice ofw >1, we may combinesθ\\nt (x|y) with a guided vector fielduθ\\nt (x|y) and\\n42'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 42, 'page_label': '43', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='5.2 Neural network architectures\\ndefine\\n˜sθ\\nt (x|y) = (1 − w)sθ\\nt (x|∅) + wsθ\\nt (x|y),\\n˜uθ\\nt (x|y) = (1 − w)uθ\\nt (x|∅) + wuθ\\nt (x|y).\\nThen we may sample via\\nInitialization: X0 ∼ pinit(x) ▶ Initialize with simple distribution (such as a Gaussian)\\nSimulation: dXt =\\n\\x14\\n˜uθ\\nt (Xt|y) + σ2\\nt\\n2 ˜sθ\\nt (Xt|y)\\n\\x15\\ndt + σtdWt ▶ Simulate SDE fromt = 0 to t = 1.\\nSamples: X1 ▶ Goal is forX1 to adhere to the guiding variabley.\\n5.2 Neural network architectures\\nWe next discuss the design of neural networks for flow and diffusion models. Specifically, we answer the question of\\nhow to construct a neural network architecture that represents the (guided) vector fielduθ\\nt (x|y) with parametersθ.\\nNote that the neural network must have 3 inputs - a vectorx ∈ Rd, a conditioning variabley ∈ Y, and a time value\\nt ∈ [0, 1] - and one output - a vectoruθ\\nt (x|y) ∈ Rd. For low-dimensional distributions (e.g. the toy distributions\\nwe have seen in previous sections), it is sufficient to parameterizeuθ\\nt (x|y) as a multi-layer perceptron (MLP), oth-\\nerwise known as a fully connected neural network. That is, in this simple setting, a forward pass throughuθ\\nt (x|y)\\nwould involve concatenating our inputx, y, and t, and passing them through an MLP. However, for complex,\\nhigh-dimensional distributions, such as those over images, videos, and proteins, an MLP is rarely sufficient, and\\nit is common to use special, application-specific architectures. For the remainder of this section, we will consider\\nthe case of images (and by extension, videos), and discuss two common architectures: theU-Net [25], and the\\ndiffusion transformer(DiT).\\n5.2.1 U-Nets and Diffusion Transformers\\nBefore we dive into the specifics of these architectures, let us recall from the introduction that an image is simply\\na vector x ∈ RCimage×H×W . Here Cimage denotes the number ofchannels (an RGB image typically would have\\nCinput = 3 color channels),H denotes theheight of the image in pixels, andW denotes thewidth of the image in\\npictures.\\nU-Nets. The U-Net architecture [25] is a specific type of convolutional neural network. Originally designed for\\nimage segmentation, its crucial feature is that both its input and its output have the shape of images (possibly with\\na different number of channels). This makes it ideal to parameterize a vector fieldx 7→ uθ\\nt (x|y) as for fixedy, tits\\ninput has the shape of an image and its output does, too. Therefore, U-Net were widely used in the development of\\ndiffusion models. A U-Net consists of a series ofencoders Ei, and a corresponding sequence ofdecoders Di, along\\nwith a latent processing block in between, which we shall refer to as amidcoder (midcoder is a term is not used in\\nthe literature usually). For sake of example, let us walk through the path taken by an imagext ∈ R3×256×256 (we\\n43'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 43, 'page_label': '44', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='5.2 Neural network architectures\\nFigure 13: The simplified U-Net architecture used in lab three.\\nhave taken(Cinput, H, W) = (3, 256, 256)) as it is processed by the U-Net:\\nxinput\\nt ∈ R3×256×256 ▶ Input to the U-Net.\\nxlatent\\nt = E(xinput\\nt ) ∈ R512×32×32 ▶ Pass through encoders to obtain latent.\\nxlatent\\nt = M(xlatent\\nt ) ∈ R512×32×32 ▶ Pass latent through midcoder.\\nxoutput\\nt = D(xlatent\\nt ) ∈ R3×256×256 ▶ Pass through decoders to obtain output.\\nNotice that as the input passes through the encoders, the number of channels in its representation increases, while\\nthe height and width of the images are decreased. Both the encoder and the decoder usually consist of a series\\nof convolutional layers (with activation functions, pooling operations, etc. in between). Not shown above are two\\npoints: First, the inputxinput\\nt ∈ R3×256×256 is often fed into an initial pre-encoding block to increase the number\\nof channels before being fed into the first encoder block. Second, the encoders and decoders are often connected\\nby residual connections. The complete picture is shown in fig. 13. At a high level, most U-Nets involve some\\nvariant of what is described above. However, certain of the design choices described above may well differ from\\nvarious implementations in practice. In particular, we opt above for a purely-convolutional architecture whereas it\\nis common to include attention layers as well throughout the encoders and decoders. The U-Net derives its name\\nfrom the “U”-like shape formed by its encoders and decoders (see fig. 13).\\nDiffusion Transformers. One alternative to U-Nets arediffusion transformers(DiTs), which dispense with con-\\nvolutions and purely useattention [35, 19]. Diffusion transformers are based onvision transformers (ViTs), in\\nwhich the big idea is essentially to divide up an image into patches, embed each of these patches, and then attend\\nbetween the patches [6].Stable Diffusion 3, trained with conditional flow matching, parameterizes the velocity field\\n44'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 44, 'page_label': '45', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='5.2 Neural network architectures\\nuθ\\nt (x) as a modified DiT, as we discuss later in section 5.3 [7].\\nRemark 29(Working in Latent Space)\\nA common problem for large-scale applications is that the data is so high-dimensional that it consumes too\\nmuch memory. For example, we might want to generate a high resolution image of1000 × 10000 pixels leading\\nto 1 million (!) dimensions. To reduce memory usage, a common design pattern is to work in alatent space\\nthat can be considered a compressed version of our data at lower resolution. Specifically, the usual approach\\nis to combine a flow or diffusion model with a (variational)autoencoder [24]. In this case, one first encodes\\nthe training dataset in thelatent space via an autoencoder, and then training the flow or diffusion model\\nin the latent space. Sampling is performed by first sampling in the latent space using the trained flow or\\ndiffusion model, and then decoding of the output via the decoder. Intuitively, a well-trained autoencoder can\\nbe thought of as filtering out semantically meaningless details, allowing the generative model to “focus” on\\nimportant, perceptually relevant features [24]. By now, nearly all state-of-the-art approaches to image and\\nvideo generation involve training a flow or diffusion model in the latent space of an autoencoder - so called\\nlatent diffusion models[24, 34]. However, it is important to note: one also needs to train the autoencoder\\nbefore training the diffusion models. Crucially, performance now depends also on how good the autoencoder\\ncompresses images into latent space and recovers aesthetically pleasing images.\\n5.2.2 Encoding the Guiding Variable.\\nUp until this point, we have glossed over how exactly the guiding (conditioning) variabley is fed into the neural\\nnetwork uθ\\nt (x|y). Broadly, this process can be decomposed into two steps: embedding the raw inputyraw (e.g., the\\ntext prompt “a cat playing a trumpet, photorealistic”) into some vector-valued inputy, and feeding the resultingy\\ninto the actual model. We now proceed to describe each step in greater detail.\\nEmbedding Raw Input. Here, we’ll consider two cases: (1) whereyraw is a discrete class-label, and (2) where\\nyraw is a text-prompt. When yraw ∈ Y≜ {0, . . . , N} is just a class label, then it is often easiest to simply learn\\na separate embedding vector for each of theN + 1 possible values ofyraw, and set y to this embedding vector.\\nOne would consider the parameters of these embeddings to be included in the parameters ofuθ\\nt (x|y), and would\\ntherefore learn these during training. Whenyraw is a text-prompt, the situation is more complex, and approaches\\nlargely rely on frozen, pre-trained models. Such models are trained to embed a discrete text input into a continuous\\nvector that captures the relevant information. One such model is known asCLIP(Contrastive Language-Image Pre-\\ntraining). CLIP is trained to learn a shared embedding space for both images and text-prompts, using a training\\nloss designed to encourage image embeddings to be close to their corresponding prompts, while being farther from\\nthe embeddings of other images and prompts [22]. We might therefore takey = CLIP(yraw) ∈ RdCLIP to be the\\nembedding produced by a frozen, pre-trained CLIP model. In certain cases, it may be undesirable to compress the\\nentire sequence into a single representation. In this case, one might additionally consider embedding the prompt\\nusing a pre-trained transformer so as to obtain a sequence of embeddings. It is also common to combine multiple\\nsuch pretrained embeddings when conditioning so as to simultaneously reap the benefits of each model [7, 21].\\n45'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 45, 'page_label': '46', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='5.3 A Survey of Large-Scale Image and Video Models\\nFigure 14: Left: An overview of the diffusion transformer architecture, taken from [19]. Right: A schematic of the\\ncontrastive CLIP loss, in which a shared image-text embedding space is learned, taken from [22].\\nFeeding in the Embedding. Suppose now that we have obtained our embedding vectory ∈ Rdy . Now what? The\\nanswer varies, but usually it is some variant of the following: feed it individually into every sub-component of the\\narchitecture for images. Let us briefly describe how this is accomplished in the U-Net implementation used in lab\\nthree, as depicted in fig. 13. At some intermediate point within the network, we would like to inject information\\nfrom y ∈ Rdy into the current activationxintermediate\\nt ∈ RC×H×W . We might do so using the procedure below,\\ngiven in PyTorch-esque pseudocode.\\ny = MLP(y) ∈ RC ▶ Map y from Rdy to RC.\\ny = reshape(y) ∈ RC×1×1 ▶ Reshape y to “look” like an image.\\nxintermediate\\nt = broadcast_add(xintermediate\\nt , y) ∈ RC×H×W ▶ Add y to xintermediate\\nt pointwise.\\nOne exception to this simple-pointwise conditioning scheme is when we have a sequence of embeddings as\\nproduced by some pretrained language model. In this case, we might consider using some sort of cross-attention\\nscheme between our image (suitably patchified) and the tokens of the embedded sequence. We will see multiple\\nexamples of this in section 5.3.\\n5.3 A Survey of Large-Scale Image and Video Models\\nWe conclude this section by briefly examining two large-scale generative models:Stable Diffusion 3 for image\\ngeneration and Meta’s Movie Gen Video for video generation [7, 21]. As you will see, these models use the\\ntechniques we have described in this work along with additional architectural enhancements to both scale and\\naccommodate richly structured conditioning modalities, such as text-based input.\\n46'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 46, 'page_label': '47', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='5.3 A Survey of Large-Scale Image and Video Models\\n5.3.1 Stable Diffusion 3\\nStable Diffusion is a series of state-of-the-art image generation models. These models were among the first to use\\nlarge-scale latent diffusion models for image generation. If you have not done so, we highly recommend testing it\\nfor yourself online (https://stability.ai/news/stable-diffusion-3).\\nStable Diffusion 3 uses the same conditional flow matching objective that we study in this work (see algo-\\nrithm 5).3 As outlined in their paper, they extensively tested various flow and diffusion alternatives and found\\nflow matching to perform best. For training, it uses classifier-free guidance training (with dropping class labels)\\nas outlined above. Further, Stable Diffusion 3 follows the approach outlined in section 5.2 by training within the\\nlatent space of a pre-trained autoencoder. Training a good autoencoder was a big contribution of the first stable\\ndiffusion papers.\\nToenhancetextconditioning, StableDiffusion3makesuseofboth3differenttypesoftextembeddings(including\\nCLIP embeddings as well as the sequential outputs produced by a pretrained instance of the encoder of Google’s\\nT5-XXL [23], and similar to approaches taken in [3, 26]). Whereas CLIP embeddings provide a coarse, overarching\\nembedding of the input text, the T5 embeddings provide a more granular level of context, allowing for the possibility\\nof the model attending to particular elements of the conditioning text. To accommodate these sequential context\\nembeddings, the authors then propose to extend the diffusion transformer to attend not just to patches of the\\nimage, but to the text embeddings as well, thereby extending the conditioning capacity from the class-based\\nscheme originally proposed for DiT to sequential context embeddings. This proposed modified DiT is referred to\\nas amulti-modal DiT(MM-DiT), and is depicted in fig. 15. Their final, largest model has8 billion parameters.\\nFor sampling, they use50 steps (i.e. they have to evaluate the network50 times) using a Euler simulation scheme\\nand a classifier-free guidance weight between2.0-5.0.\\n5.3.2 Meta Movie Gen Video\\nNext, we discuss Meta’s video generator,Movie Gen Video (https://ai.meta.com/research/movie-gen/). As the\\ndata are not images butvideos, the data x lie in the spaceRT×C×H×W where T represents the new temporal\\ndimension (i.e. the number of frames). As we shall see, many of the design choices made in this video setting can\\nbe seen as adapting existing techniques (e.g., autoencoders, diffusion transformers, etc.) from the image setting to\\nhandle this extra temporal dimension.\\nMovie Gen Video utilizes the conditional flow matching objective with the same CondOT path (see algorithm 5).\\nLike Stable Diffusion 3, Movie Gen Video also operates in the latent space of frozen, pretrained autoencoder. Note\\nthat the autoencoder to reduce memory consumption is even more important for videos than for images - which\\nis why most video generators right now are pretty limited in the length of the video they generate. Specifically,\\nthe authors propose to handle the added time dimension by introducing atemporal autoencoder (TAE) which\\nmaps a raw videox′\\nt ∈ RT′×3×H×W to a latentxt ∈ RT×C×H×W , with T′\\nT = H′\\nH = W′\\nW = 8 [21]. To accomodate\\nlong videos, a temporal tiling procedure is proposed by which the video is chopped up into pieces, each piece is\\nencoder separately, and the latents are sticthed together [21]. The model itself - that is,uθ\\nt (xt) - is given by a\\n3In their work, they use a different convention to condition on the noise. But this is only notation and the algorithm is the same.\\n47'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 47, 'page_label': '48', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='5.3 A Survey of Large-Scale Image and Video Models\\nFigure 15: The architecture of the multi-modal diffusion transformer (MM-DiT) proposed in [7]. Figure also taken\\nfrom [7].\\nDiT-like backbone in whichxt is patchified along the time and space dimensions. The image patches are then\\npassed through a transformer employing both self-attention among the image patches, and cross-attention with\\nlanguage model embeddings, similar to the MM-DiT employed by Stable Diffusion 3. For text conditioning, Movie\\nGen Video employs three types of text embeddings: UL2 embeddings, for granular, text-based reasoning [33],\\nByT5 embeddings, for attending to character-level details (for e.g., prompts explicitly requesting specific text to\\nbe present) [36], and MetaCLIP embeddings, trained in a shared text-image embedding space [13, 21]. Their final,\\nlargest model has30 billion parameters. For a significantly more detailed and expansive treatment, we encourage\\nthe reader to check out the Movie Gen technical report itself [21].\\n48'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 48, 'page_label': '49', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='6 Acknowledgements\\nThis course would not have been possible without the generous support of many others. We would like to thank\\nTommi Jaakkola for serving as the advisor and faculty sponsor for this course, and for thoughtful feedback through-\\nout the process. We would like to thank Christian Fiedler, Tim Griesbach, Benedikt Geiger, and Albrecht Holder-\\nrieth for value feedback on the lecture notes. Further, we thank Elaine Mello from MIT Open Learning for support\\nwith lecture recordings and Ashay Athalye from Students for Open and Universal Learning for helping to cut and\\nprocess the videos. We would additionally like to thank Cameron Diao, Tally Portnoi, Andi Qu, Roger Trullo,\\nÁdám Burián, Zewen Yang, and many others for their invaluable contributions to the labs. We would also like to\\nthank Lisa Bella, Ellen Reid, and everyone else at MIT EECS for their generous support. Finally, we would like to\\nthank all participants in the original course offering (MIT 6.S184/6.S975, taught over IAP 2025), as well as readers\\nlike you for your interest in this class. Thanks!\\n7 References\\n[1] Michael S Albergo, Nicholas M Boffi, and Eric Vanden-Eijnden. “Stochastic interpolants: A unifying frame-\\nwork for flows and diffusions”. In:arXiv preprint arXiv:2303.08797(2023).\\n[2] Brian DO Anderson. “Reverse-time diffusion equation models”. In:Stochastic Processes and their Applications\\n12.3 (1982), pp. 313–326.\\n[3] Yogesh Balaji et al. eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers. 2023.\\narXiv: 2211.01324[cs.CV]. url: https://arxiv.org/abs/2211.01324.\\n[4] Earl A Coddington, Norman Levinson, and T Teichmann.Theory of ordinary differential equations. 1956.\\n[5] Prafulla Dhariwal and Alex Nichol.Diffusion Models Beat GANs on Image Synthesis. 2021. arXiv: 2105.05233\\n[cs.LG]. url: https://arxiv.org/abs/2105.05233.\\n[6] Alexey Dosovitskiy et al. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.\\n2021. arXiv: 2010.11929[cs.CV]. url: https://arxiv.org/abs/2010.11929.\\n[7] Patrick Esser et al. Scaling Rectified Flow Transformers for High-Resolution Image Synthesis. 2024. arXiv:\\n2403.03206 [cs.CV]. url: https://arxiv.org/abs/2403.03206.\\n[8] Lawrence C Evans. Partial differential equations. Vol. 19. American Mathematical Society, 2022.\\n[9] Jonathan Ho, Ajay Jain, and Pieter Abbeel. “Denoising diffusion probabilistic models”. In:Advances in neural\\ninformation processing systems33 (2020), pp. 6840–6851.\\n[10] Jonathan Ho and Tim Salimans.Classifier-Free Diffusion Guidance. 2022. arXiv: 2207.12598[cs.LG]. url:\\nhttps://arxiv.org/abs/2207.12598.\\n[11] Arieh Iserles. A first course in the numerical analysis of differential equations. Cambridge university press,\\n2009.\\n[12] Tero Karras et al. “Elucidating the design space of diffusion-based generative models”. In:Advances in Neural\\nInformation Processing Systems35 (2022), pp. 26565–26577.\\n49'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 49, 'page_label': '50', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='[13] Samuel Lavoie et al. Modeling Caption Diversity in Contrastive Vision-Language Pretraining. 2024. arXiv:\\n2405.00740 [cs.CV]. url: https://arxiv.org/abs/2405.00740.\\n[14] Yaron Lipman et al. “Flow matching for generative modeling”. In:arXiv preprint arXiv:2210.02747(2022).\\n[15] Yaron Lipman et al. “Flow Matching Guide and Code”. In:arXiv preprint arXiv:2412.06264(2024).\\n[16] Xingchao Liu, Chengyue Gong, and Qiang Liu. “Flow straight and fast: Learning to generate and transfer\\ndata with rectified flow”. In:arXiv preprint arXiv:2209.03003(2022).\\n[17] Nanye Ma et al. “Sit: Exploring flow and diffusion-based generative models with scalable interpolant trans-\\nformers”. In:arXiv preprint arXiv:2401.08740(2024).\\n[18] Xuerong Mao. Stochastic differential equations and applications. Elsevier, 2007.\\n[19] William Peebles and Saining Xie. Scalable Diffusion Models with Transformers. 2023. arXiv: 2212.09748\\n[cs.CV]. url: https://arxiv.org/abs/2212.09748.\\n[20] Lawrence Perko. Differential equations and dynamical systems. Vol. 7. Springer Science & Business Media,\\n2013.\\n[21] Adam Polyak et al.Movie Gen: A Cast of Media Foundation Models. 2024. arXiv: 2410.13720[cs.CV]. url:\\nhttps://arxiv.org/abs/2410.13720.\\n[22] Alec Radford et al. Learning Transferable Visual Models From Natural Language Supervision. 2021. arXiv:\\n2103.00020 [cs.CV]. url: https://arxiv.org/abs/2103.00020.\\n[23] Colin Raffel et al. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. 2023.\\narXiv: 1910.10683[cs.LG]. url: https://arxiv.org/abs/1910.10683.\\n[24] Robin Rombach et al.High-Resolution Image Synthesis with Latent Diffusion Models. 2022. arXiv: 2112.10752\\n[cs.CV]. url: https://arxiv.org/abs/2112.10752.\\n[25] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. “U-net: Convolutional networks for biomedical image\\nsegmentation”. In:Medical image computing and computer-assisted intervention–MICCAI 2015: 18th inter-\\nnational conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18. Springer. 2015, pp. 234–\\n241.\\n[26] Chitwan Saharia et al. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.\\n2022. arXiv: 2205.11487[cs.CV]. url: https://arxiv.org/abs/2205.11487.\\n[27] Simo Särkkä and Arno Solin.Applied stochastic differential equations. Vol. 10. Cambridge University Press,\\n2019.\\n[28] Jascha Sohl-Dickstein et al. “Deep unsupervised learning using nonequilibrium thermodynamics”. In:Inter-\\nnational conference on machine learning. PMLR. 2015, pp. 2256–2265.\\n[29] Yang Song and Stefano Ermon. “Generative modeling by estimating gradients of the data distribution”. In:\\nAdvances in neural information processing systems32 (2019).\\n[30] Yang Song et al. Score-Based Generative Modeling through Stochastic Differential Equations. 2021. arXiv:\\n2011.13456 [cs.LG]. url: https://arxiv.org/abs/2011.13456.\\n50'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 50, 'page_label': '51', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='[31] Yang Song et al. “Score-Based Generative Modeling through Stochastic Differential Equations”. In:Interna-\\ntional Conference on Learning Representations (ICLR). 2021.\\n[32] Yang Song et al. “Score-based generative modeling through stochastic differential equations”. In: arXiv\\npreprint arXiv:2011.13456 (2020).\\n[33] Yi Tay et al. UL2: Unifying Language Learning Paradigms. 2023. arXiv: 2205.05131[cs.CL]. url: https:\\n//arxiv.org/abs/2205.05131.\\n[34] Arash Vahdat, Karsten Kreis, and Jan Kautz. “Score-based generative modeling in latent space”. In:Advances\\nin neural information processing systems34 (2021), pp. 11287–11302.\\n[35] Ashish Vaswani et al. Attention Is All You Need. 2023. arXiv: 1706.03762[cs.CL]. url: https://arxiv.org/\\nabs/1706.03762.\\n[36] Linting Xue et al.ByT5: Towards a token-free future with pre-trained byte-to-byte models. 2022. arXiv: 2105.\\n13626 [cs.CL]. url: https://arxiv.org/abs/2105.13626.\\n51'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 51, 'page_label': '52', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='A A Reminder on Probability Theory\\nWe present a brief overview of basic concepts from probability theory. This section was partially taken from [15].\\nA.1 Random vectors\\nConsider data in the d-dimensional Euclidean space x = ( x1, . . . , xd) ∈ Rd with the standard Euclidean inner\\nproduct ⟨x, y⟩ = Pd\\ni=1 xiyi and norm ∥x∥ =\\np\\n⟨x, x⟩. We will consider random variables (RVs)X ∈ Rd with\\ncontinuous probability density function (PDF), defined as acontinuous function pX : Rd → R≥0 providing event\\nA with probability\\nP(X ∈ A) =\\nZ\\nA\\npX(x)dx, (80)\\nwhere\\nR\\npX(x)dx = 1 . By convention, we omit the integration interval when integrating over the whole space\\n(\\nR\\n≡\\nR\\nRd). To keep notation concise, we will refer to the PDFpXt of RVXt as simplypt. We will use the notation\\nX ∼ p or X ∼ p(X) to indicate thatX is distributed according top. One common PDF in generative modeling is\\nthe d-dimensional isotropic Gaussian:\\nN(x; µ, σ2I) = (2πσ2)−d\\n2 exp\\n \\n−∥x − µ∥2\\n2\\n2σ2\\n!\\n, (81)\\nwhere µ ∈ Rd and σ ∈ R>0 stand for the mean and the standard deviation of the distribution, respectively.\\nThe expectation of a RV is the constant vector closest toX in the least-squares sense:\\nE[X] = arg min\\nz∈Rd\\nZ\\n∥x − z∥2 pX(x)dx =\\nZ\\nxpX(x)dx. (82)\\nOne useful tool to compute the expectation offunctions of RVsis thelaw of the unconscious statistician:\\nE[f(X)] =\\nZ\\nf(x)pX(x)dx. (83)\\nWhen necessary, we will indicate the random variables under expectation asEXf(X).\\nA.2 Conditional densities and expectations\\nFigure 16: Joint PDF pX,Y\\n(in shades) and its marginals\\npX and pY (in black lines).\\nFigure from [15]\\n.\\nGiven two random variablesX, Y∈ Rd, their joint PDFpX,Y (x, y) has marginals\\nZ\\npX,Y (x, y)dy = pX(x) and\\nZ\\npX,Y (x, y)dx = pY (y). (84)\\nSee fig. 16 for an illustration of the joint PDF of two RVs inR (d = 1 ). The\\nconditional PDFpX|Y describesthePDFoftherandomvariable X whenconditioned\\non an eventY = y with densitypY (y) > 0:\\npX|Y (x|y) := pX,Y (x, y)\\npY (y) , (85)\\n52'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 52, 'page_label': '53', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='A.2 Conditional densities and expectations\\nand similarly for the conditional PDFpY |X. Bayes’ rule expresses the conditional\\nPDF pY |X with pX|Y by\\npY |X(y|x) = pX|Y (x|y)pY (y)\\npX(x) , (86)\\nfor pX(x) > 0.\\nThe conditional expectationE[X|Y ] is the best approximatingfunction g⋆(Y ) to X in the least-squares sense:\\ng⋆ := arg min\\ng:Rd→Rd\\nE\\nh\\n∥X − g(Y )∥2\\ni\\n= arg min\\ng:Rd→Rd\\nZ\\n∥x − g(y)∥2 pX,Y (x, y)dxdy\\n= arg min\\ng:Rd→Rd\\nZ \\x14Z\\n∥x − g(y)∥2 pX|Y (x|y)dx\\n\\x15\\npY (y)dy. (87)\\nFory ∈ Rd such thatpY (y) > 0 the conditional expectation function is therefore\\nE[X|Y = y] := g⋆(y) =\\nZ\\nxpX|Y (x|y)dx, (88)\\nwhere the second equality follows from taking the minimizer of the inner brackets in eq. (87) forY = y, similarly\\nto eq. (82). Composingg⋆ with the random variableY , we get\\nE[X|Y ] := g⋆(Y ), (89)\\nwhich is a random variable inRd. Rather confusingly, bothE[X|Y = y] and E[X|Y ] are often calledconditional\\nexpectation, but these are different objects. In particular,E[X|Y = y] is a functionRd → Rd, whileE[X|Y ] is a\\nrandom variable assuming values inRd. To disambiguate these two terms, our discussions will employ the notations\\nintroduced here.\\nThe tower propertyis an useful property that helps simplify derivations involving conditional expectations of\\ntwo RVsX and Y :\\nE[E[X|Y ]] = E[X] (90)\\nBecause E[X|Y ] is a RV, itself a function of the RVY , the outer expectation computes the expectation ofE[X|Y ].\\nThe tower property can be verified by using some of the definitions above:\\nE[E[X|Y ]] =\\nZ \\x12Z\\nxpX|Y (x|y)dx\\n\\x13\\npY (y)dy\\n(85)\\n=\\nZ Z\\nxpX,Y (x, y)dxdy\\n(84)\\n=\\nZ\\nxpX(x)dx\\n= E[X] .\\nFinally, consider a helpful property involving two RVsf(X, Y) and Y , whereX and Y are two arbitrary RVs.\\nThen, by using the Law of the Unconscious Statistician with (88), we obtain the identity\\nE[f(X, Y)|Y = y] =\\nZ\\nf(x, y)pX|Y (x|y)dx. (91)\\n53'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 53, 'page_label': '54', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='B A Proof of the Fokker-Planck equation\\nIn this section, we give here a self-contained proof of the Fokker-Planck equation (theorem 15) which includes the\\ncontinuity equation as a special case (theorem 12). We stress thatthis section is not necessary to understand\\nthe remainder of this documentand is mathematically more advanced. If you desire to understand where the\\nFokker-Planck equation comes from, then this section is for you.\\nWe start by showing that the Fokker-Planck is a necessary condition, i.e. ifXt ∼ pt, then the Fokker-Planck\\nequation is fulfilled. The trick for the proof is to usetest functionsf, i.e. functionsf : Rd → R that are infinitely\\ndifferentiable (\"smooth\") and are only non-zero within a bounded domain (compact support). We use the fact that\\nfor arbitrary integrable functionsg1, g2 : Rd → R it holds that\\ng1(x) = g2(x) for allx ∈ Rd ⇔\\nZ\\nf(x)g1(x)dx =\\nZ\\nf(x)g2(x)dx for all test functionsf (92)\\nIn other words, we can express the pointwise equality as equality of taking integrals. The useful thing about test\\nfunctions is that they are smooth, i.e. we can take gradients and higher-order derivatives. In particular, we can\\nuse integration by partsfor arbitrary test functionsf1, f2:\\nZ\\nf1(x) ∂\\n∂xi\\nf2(x)dx = −\\nZ\\nf2(x) ∂\\n∂xi\\nf1(x)dx (93)\\nBy using this together with the definition of the divergence and Laplacian (see eq. (23)), we get the identities:\\nZ\\n∇fT\\n1 (x)f2(x)dx = −\\nZ\\nf1(x)div(f2)(x)dx (f1 : Rd → R, f2 : Rd → Rd) (94)\\nZ\\nf1(x)∆f2(x)dx =\\nZ\\nf2(x)∆f1(x)dx (f1 : Rd → R, f2 : Rd → R) (95)\\nNow let’s proceed to the proof. We use the stochastic update of SDE trajectories as in eq. (6):\\nXt+h =Xt + hut(Xt) + σt(Wt+h − Wt) + hRt(h) (96)\\n≈Xt + hut(Xt) + σt(Wt+h − Wt) (97)\\nwhere for now we simply ignore the error termRt(h) for readability as we will takeh → 0 anyway. We can then\\nmake the following calculation:\\nf(Xt+h) − f(Xt)\\n(97)\\n= f(Xt + hut(Xt) + σt(Wt+h − Wt)) − f(Xt)\\n(i)\\n=∇f(Xt)T (hut(Xt) + σt(Wt+h − Wt)))\\n+ 1\\n2 (hut(Xt) + σt(Wt+h − Wt)))T ∇2f(Xt) (hut(Xt) + σt(Wt+h − Wt)))\\n(ii)\\n= h∇f(Xt)T ut(Xt) + σt∇f(Xt)T (Wt+h − Wt)\\n+ 1\\n2h2ut(Xt)T ∇2f(Xt)ut(Xt) + hσtut(Xt)T ∇2f(Xt)(Wt+h − Wt) + 1\\n2σ2\\nt (Wt+h − Wt)T ∇2f(Xt)(Wt+h − Wt)\\n54'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 54, 'page_label': '55', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='where in (i) we used a 2nd Taylor approximation off around Xt and in (ii) we used the fact that the Hessian∇2f\\nis a symmetric matrix. Note thatE[Wt+h − Wt|Xt] = 0 and Wt+h − Wt|Xt ∼ N(0, hId). Therefore\\nE[f(Xt+h) − f(Xt)|Xt]\\n=h∇f(Xt)T ut(Xt) + 1\\n2h2ut(Xt)T ∇2f(Xt)ut(Xt) + h\\n2 σ2\\nt Eϵt∼N(0,Id)[ϵT\\nt ∇2f(Xt)ϵt]\\n(i)\\n=h∇f(Xt)T ut(Xt) + 1\\n2h2ut(Xt)T ∇2f(Xt)ut(Xt) + h\\n2 σ2\\nt trace(∇2f(Xt))\\n(ii)\\n= h∇f(Xt)T ut(Xt) + 1\\n2h2ut(Xt)T ∇2f(Xt)ut(Xt) + h\\n2 σ2\\nt ∆f(Xt)\\nwhere in(i) we used the fact thatEϵt∼N(0,Id)[ϵT\\nt Aϵt] = trace(A) and in(ii) we used the definition of the Laplacian\\nand the Hessian matrix. With this, we get that\\n∂tE[f(Xt)]\\n= lim\\nh→0\\n1\\nhE[f(Xt+h) − f(Xt)]\\n= lim\\nh→0\\n1\\nhE[E[f(Xt+h) − f(Xt)|Xt]]\\n=E[ lim\\nh→0\\n1\\nh\\n\\x12\\nh∇f(Xt)T ut(Xt) + 1\\n2h2ut(Xt)T ∇2f(Xt)ut(Xt) + h\\n2 σ2\\nt ∆f(Xt)\\n\\x13\\n]\\n=E[∇f(Xt)T ut(Xt) + 1\\n2σ2\\nt ∆f(Xt)]\\n(i)\\n=\\nZ\\n∇f(x)T ut(x)pt(x)dx +\\nZ 1\\n2σ2\\nt ∆f(x)pt(x)dx\\n(ii)\\n= −\\nZ\\nf(x)div(utpt)(x)dx +\\nZ 1\\n2σ2\\nt f(x)∆pt(x)dx\\n=\\nZ\\nf(x)\\n\\x12\\n−div(utpt)(x) + 1\\n2σ2\\nt ∆pt(x)\\n\\x13\\ndx\\nwhere in (i) we used the assumption thatpt as the distribution ofXt and in (ii) we used eq. (94) and eq. (95).\\nTherefore, it holds that\\n∂tE[f(Xt)] =\\nZ\\nf(x)\\n\\x12\\n−div(ptut)(x) + σ2\\nt\\n2 ∆pt(x)\\n\\x13\\ndx (for allf and 0 ≤ t ≤ 1) (98)\\n(i)\\n⇔ ∂t\\nZ\\nf(x)pt(x)dx =\\nZ\\nf(x)\\n\\x12\\n−div(ptut)(x) + σ2\\nt\\n2 ∆pt(x)\\n\\x13\\ndx (for allf and 0 ≤ t ≤ 1) (99)\\n(ii)\\n⇔\\nZ\\nf(x)∂tpt(x)dx =\\nZ\\nf(x)\\n\\x12\\n−div(ptut)(x) + σ2\\nt\\n2 ∆pt(x)\\n\\x13\\ndx (for allf and 0 ≤ t ≤ 1) (100)\\n(iii)\\n⇔ ∂tpt(x) = − div(ptut)(x) + σ2\\nt\\n2 ∆pt(x) ( for allx ∈ Rd, 0 ≤ t ≤ 1) (101)\\nwhere in (i) we used the assumption thatXt ∼ pt, in (ii) we swapped the derivative with the integral and (iii) we\\nused eq. (92) . This completes the proof that the Fokker-Planck equation is a necessary condition.\\nFinally, we explain why it is also a sufficient condition. The Fokker-Planck equation is a partial differential\\nequation (PDE). More specifically, it is a so-calledparabolic partial differential equation. Similar to theorem 3,\\n55'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 55, 'page_label': '56', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}, page_content='such differential equations have a unique solution given fixed initial conditions (see e.g. [8, Chapter 7]). Now, if\\neq. (30) holds forpt, we just shown above that it must also hold for true distributionqt of Xt (i.e. Xt ∼ qt) - in\\nother words, bothpt and qt are solutions to the parabolic PDE. Further, we know that the initial conditions are\\nthe same, i.e.p0 = q0 = pinit by construction of an interpolating probability path (see??). Hence, by uniqueness\\nof the solution of the differential equation, we know thatpt = qt for all0 ≤ t ≤ 1 - which meansXt ∼ qt = pt and\\nwhich is what we wanted to show.\\n56'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': '20250122_Lecture_2', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'page': 0, 'page_label': '1', 'source_document': WindowsPath('../data/PDFs/Lecturenotes.pdf'), 'file_type': 'pdf'}, page_content='Lecture 2\\nConstructing a Training Target for Flow and Diffusion Models\\nMIT IAP 2025 | Jan 22, 2025\\nPeter Holderrieth and Ezra Erives\\nSponsor: Tommi Jaakkola'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': '20250122_Lecture_2', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'page': 1, 'page_label': '2', 'source_document': WindowsPath('../data/PDFs/Lecturenotes.pdf'), 'file_type': 'pdf'}, page_content='Reminder: Flow and Diffusion Models \\nNeural network\\nvector field\\nInitialize: ODE:\\nInitialize: SDE:\\nDiffusion coeff.\\nFlow \\nModel\\nDiffusion \\nModel\\nE.g. Gaussian\\nTo get samples, simulate ODE/SDE from t=0 to t=1 and return'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': '20250122_Lecture_2', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'page': 2, 'page_label': '3', 'source_document': WindowsPath('../data/PDFs/Lecturenotes.pdf'), 'file_type': 'pdf'}, page_content='Next step: Training the model\\nWithout training, the model produces “non-sense”→ We need to train \\nGoal of lecture 2 (today) and lecture 3 (tomorrow): \\nDerive training algorithm\\nImplies\\nTraining = Finding parameters         such that \\nStart with initial \\ndistribution\\nFollow along \\nthe vector field\\nThe distribution of \\nthe final point = data \\ndistribution'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': '20250122_Lecture_2', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'page': 3, 'page_label': '4', 'source_document': WindowsPath('../data/PDFs/Lecturenotes.pdf'), 'file_type': 'pdf'}, page_content='Today’s goal: Derive a Training Target\\n- Typically, we train the model by minimizing a mean squared \\nerror:\\nToday: Derive a formula for the training target:\\nTomorrow: Training algorithm using \\n- In regression or classification, the training target is the label.\\n- Here: No label :( → We have to derive a training target\\nTraining target'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': '20250122_Lecture_2', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'page': 4, 'page_label': '5', 'source_document': WindowsPath('../data/PDFs/Lecturenotes.pdf'), 'file_type': 'pdf'}, page_content='Marginal Vector Field\\nMarginal Score Function\\nFlow Matching\\nScore Matching\\nToday: Training target Tomorrow: Training algorithm'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': '20250122_Lecture_2', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'page': 5, 'page_label': '6', 'source_document': WindowsPath('../data/PDFs/Lecturenotes.pdf'), 'file_type': 'pdf'}, page_content='Section 2:\\nConstructing a Training Target\\nGoal: Derive a formula for a training target for training our \\nmodels'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': '20250122_Lecture_2', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'page': 6, 'page_label': '7', 'source_document': WindowsPath('../data/PDFs/Lecturenotes.pdf'), 'file_type': 'pdf'}, page_content='Today will be the technically most challenging lecture! \\nThe next lectures will be much much easier!\\nYou do not have to understand the derivations. \\nMake sure you understand the formulas for:\\nConditional \\nProbability Path\\nMarginal \\nProbability Path\\nConditional \\nVector Field\\nMarginal \\nVector Field\\nConditional \\nScore Function\\nMarginal \\nScore Function'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': '20250122_Lecture_2', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'page': 7, 'page_label': '8', 'source_document': WindowsPath('../data/PDFs/Lecturenotes.pdf'), 'file_type': 'pdf'}, page_content='Key terminology:  \\n“Conditional” = “Per single data point”\\n“Marginal” = “Across distribution of data points”'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': '20250122_Lecture_2', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'page': 8, 'page_label': '9', 'source_document': WindowsPath('../data/PDFs/Lecturenotes.pdf'), 'file_type': 'pdf'}, page_content='Probability Paths: The Path from Noise to Data'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': '20250122_Lecture_2', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'page': 9, 'page_label': '10', 'source_document': WindowsPath('../data/PDFs/Lecturenotes.pdf'), 'file_type': 'pdf'}, page_content='Conditional Probability Path\\nMarginal Probability Path\\nt=0 t=1'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': '20250122_Lecture_2', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'page': 10, 'page_label': '11', 'source_document': WindowsPath('../data/PDFs/Lecturenotes.pdf'), 'file_type': 'pdf'}, page_content='Conditional Probability Path\\nMarginal Probability Path'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': '20250122_Lecture_2', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'page': 11, 'page_label': '12', 'source_document': WindowsPath('../data/PDFs/Lecturenotes.pdf'), 'file_type': 'pdf'}, page_content='Conditional Probability Path\\nMarginal Probability Path'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': '20250122_Lecture_2', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'page': 12, 'page_label': '13', 'source_document': WindowsPath('../data/PDFs/Lecturenotes.pdf'), 'file_type': 'pdf'}, page_content='Conditional Prob. Path\\nConditional \\nProbability Path\\nGaussian example\\nConditional \\nVector Field\\nConditional \\nScore \\nFunction\\nNotation Key property\\nODE follows \\nconditional path \\nGradient of \\nlog-likelihood\\nInterpolates     \\nand a data point z'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': '20250122_Lecture_2', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'page': 13, 'page_label': '14', 'source_document': WindowsPath('../data/PDFs/Lecturenotes.pdf'), 'file_type': 'pdf'}, page_content='Marginal Prob. Path\\nMarginal \\nProbability   \\nPath\\nFormula\\nMarginal \\nVector \\nField\\nMarginal \\nScore \\nFunction\\nNotation Key property\\nODE follows \\nmarginal path \\nGradient of \\nlog-likelihood \\nof marginal \\npath\\nInterpolates     \\nand'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': '20250122_Lecture_2', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'page': 14, 'page_label': '15', 'source_document': WindowsPath('../data/PDFs/Lecturenotes.pdf'), 'file_type': 'pdf'}, page_content='Simulating ODE \\nwith Conditional \\nVector Field for \\nConditional \\nProbability Path\\nFigure credit: \\nYaron Lipman\\nNOTE: This is an \\nanimated gif and is \\nstatic in a PDF'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': '20250122_Lecture_2', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'page': 15, 'page_label': '16', 'source_document': WindowsPath('../data/PDFs/Lecturenotes.pdf'), 'file_type': 'pdf'}, page_content='Ground truth ODE samples ODE Trajectories'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': '20250122_Lecture_2', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'page': 16, 'page_label': '17', 'source_document': WindowsPath('../data/PDFs/Lecturenotes.pdf'), 'file_type': 'pdf'}, page_content='Continuity Equation\\nGiven:\\nRandomly initialized ODE\\nContinuity equation holds\\nFollow probability path:\\nequivalent\\nMarginals are \\np_t\\nPDE holds'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': '20250122_Lecture_2', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'page': 17, 'page_label': '18', 'source_document': WindowsPath('../data/PDFs/Lecturenotes.pdf'), 'file_type': 'pdf'}, page_content='Continuity Equation\\nOutflow\\nInflow\\nChange of \\nprobability \\nmass at x\\nOutflow - inflow\\nof probability \\nmass from u'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': '20250122_Lecture_2', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'page': 18, 'page_label': '19', 'source_document': WindowsPath('../data/PDFs/Lecturenotes.pdf'), 'file_type': 'pdf'}, page_content='Gaussian\\nConditional\\nProbability Path\\nAnd\\nConditional\\nVector Field\\nFigure credit: \\nYaron Lipman'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': '20250122_Lecture_2', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'page': 19, 'page_label': '20', 'source_document': WindowsPath('../data/PDFs/Lecturenotes.pdf'), 'file_type': 'pdf'}, page_content='Toy example\\nFigure credit: \\nYaron Lipman\\nNOTE: This is an \\nanimated gif and \\nis static in a PDF'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': '20250122_Lecture_2', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'page': 20, 'page_label': '21', 'source_document': WindowsPath('../data/PDFs/Lecturenotes.pdf'), 'file_type': 'pdf'}, page_content='Simulating ODE \\nwith Marginal \\nVector Field for \\nGaussian \\nProbability Path\\nFigure credit: \\nYaron Lipman'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': '20250122_Lecture_2', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'page': 21, 'page_label': '22', 'source_document': WindowsPath('../data/PDFs/Lecturenotes.pdf'), 'file_type': 'pdf'}, page_content='Conditional Prob. Path, Vector Field, and Score\\nConditional \\nProbability Path\\nGaussian example\\nConditional \\nVector Field\\nConditional \\nScore \\nFunction\\nNotation Key property\\nODE follows \\nconditional path \\nGradient of \\nlog-likelihood\\nInterpolates     \\nand a data point z'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': '20250122_Lecture_2', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'page': 22, 'page_label': '23', 'source_document': WindowsPath('../data/PDFs/Lecturenotes.pdf'), 'file_type': 'pdf'}, page_content='Marginal Prob. Path, Vector Field, and Score\\nMarginal \\nProbability   \\nPath\\nFormula\\nMarginal \\nVector \\nField\\nMarginal \\nScore \\nFunction\\nNotation Key property\\nODE follows \\nmarginal path \\nGradient of \\nlog-likelihood \\nof marginal \\npath\\nInterpolates     \\nand'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': '20250122_Lecture_2', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'page': 23, 'page_label': '24', 'source_document': WindowsPath('../data/PDFs/Lecturenotes.pdf'), 'file_type': 'pdf'}, page_content='Outlook (Next class) - Flow Matching Loss\\nThe Flow Matching loss is a mean squared error between the neural \\nnetwork and the marginal vector field: \\nTraining a Flow Model Consists of Learning the Marginal \\nVector Field (How? Next lecture!)'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': '20250122_Lecture_2', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'page': 24, 'page_label': '25', 'source_document': WindowsPath('../data/PDFs/Lecturenotes.pdf'), 'file_type': 'pdf'}, page_content='Example marginal vector field - Meta MovieGen\\nThese videos are generated by simulating the ODE with \\nthe (learnt) marginal vector field'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': '20250122_Lecture_2', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'page': 25, 'page_label': '26', 'source_document': WindowsPath('../data/PDFs/Lecturenotes.pdf'), 'file_type': 'pdf'}, page_content='Ground truth SDE samples SDE Trajectories'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': '20250122_Lecture_2', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'page': 26, 'page_label': '27', 'source_document': WindowsPath('../data/PDFs/Lecturenotes.pdf'), 'file_type': 'pdf'}, page_content='Fokker-Planck equation\\nGiven:\\nRandomly initialized SDE\\nFokker-Planck equation holds\\nFollow probability path:\\nequivalent\\nMarginals are \\np_t\\nHeat  equ.Continuity equ.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': '20250122_Lecture_2', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'page': 27, 'page_label': '28', 'source_document': WindowsPath('../data/PDFs/Lecturenotes.pdf'), 'file_type': 'pdf'}, page_content='Continuity Equation\\nOutflow\\nInflow\\nChange of \\nprobability \\nmass at x\\nOutflow - inflow\\nof probability \\nmass from u'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': '20250122_Lecture_2', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'page': 28, 'page_label': '29', 'source_document': WindowsPath('../data/PDFs/Lecturenotes.pdf'), 'file_type': 'pdf'}, page_content='Fokker-Planck Equation\\nDispersionChange of \\nprobability \\nmass at x Heat dispersion'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': '20250122_Lecture_2', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'page': 29, 'page_label': '30', 'source_document': WindowsPath('../data/PDFs/Lecturenotes.pdf'), 'file_type': 'pdf'}, page_content='Outlook (Next class) - Score Matching Loss\\nThe Score Matching loss is a mean squared error between the neural \\nnetwork and the marginal score function:\\nTo train a diffusion model, we need to train the score network \\nby minimizing the score matching loss (How? Next class!)'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': '20250122_Lecture_2', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'page': 30, 'page_label': '31', 'source_document': WindowsPath('../data/PDFs/Lecturenotes.pdf'), 'file_type': 'pdf'}, page_content='Marginal VF\\n Marginal VF + Score'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': '20250122_Lecture_2', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'page': 31, 'page_label': '32', 'source_document': WindowsPath('../data/PDFs/Lecturenotes.pdf'), 'file_type': 'pdf'}, page_content='Training a Diffusion Model = Learning the Score Function\\nConversion of \\nof noise into \\nprotein \\nstructure by \\nmarginal \\nvector field \\nSlide credit: \\nJason Yim\\nNOTE: This is \\nan animated \\ngif and is \\nstatic in a \\nPDF'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': '20250122_Lecture_2', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'page': 32, 'page_label': '33', 'source_document': WindowsPath('../data/PDFs/Lecturenotes.pdf'), 'file_type': 'pdf'}, page_content='Conditional Prob. Path, Vector Field, and Score\\nConditional \\nProbability Path\\nGaussian example\\nConditional \\nVector Field\\nConditional \\nScore \\nFunction\\nNotation Key property\\nODE follows \\nconditional path \\nGradient of \\nlog-likelihood\\nInterpolates     \\nand a data point z'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': '20250122_Lecture_2', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'page': 33, 'page_label': '34', 'source_document': WindowsPath('../data/PDFs/Lecturenotes.pdf'), 'file_type': 'pdf'}, page_content='Marginal Prob. Path, Vector Field, and Score\\nMarginal \\nProbability   \\nPath\\nFormula\\nMarginal \\nVector \\nField\\nMarginal \\nScore \\nFunction\\nNotation Key property\\nODE follows \\nmarginal path \\nCan be used \\nto convert \\nODE target \\nto SDE\\nInterpolates     \\nand'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': '20250122_Lecture_2', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'page': 34, 'page_label': '35', 'source_document': WindowsPath('../data/PDFs/Lecturenotes.pdf'), 'file_type': 'pdf'}, page_content='Today was the technically most challenging lecture! \\nThe next lectures will be much much easier!\\nMake sure you understand the formulas for:\\nThese 6 formulas is all we need for training!\\nConditional \\nProbability Path\\nMarginal \\nProbability Path\\nConditional \\nVector Field\\nMarginal \\nVector Field\\nConditional \\nScore Function\\nMarginal \\nScore Function'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': '20250122_Lecture_2', 'source': '..\\\\data\\\\PDFs\\\\Lecturenotes.pdf', 'total_pages': 36, 'page': 35, 'page_label': '36', 'source_document': WindowsPath('../data/PDFs/Lecturenotes.pdf'), 'file_type': 'pdf'}, page_content='Next class:\\nThursday (Tomorrow), 11am-12:30pm\\nTraining algorithm!\\nE25-111 (same room)\\nOffice hours: Today, 3pm-4:30pm in 37-212'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-09T02:33:09+00:00', 'author': '', 'keywords': '', 'moddate': '2024-02-09T02:33:09+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Transformers.pdf', 'total_pages': 10, 'page': 0, 'page_label': '1', 'source_document': WindowsPath('../data/PDFs/Transformers.pdf'), 'file_type': 'pdf'}, page_content='An Introduction to Transformers\\nRichard E. Turner\\nDepartment of Engineering, University of Cambridge, UK\\nMicrosoft Research, Cambridge, UK\\nret26@cam.ac.uk\\nAbstract. The transformer is a neural network component that can be used to learn useful represen-\\ntations of sequences or sets of data-points [Vaswani et al., 2017]. The transformer has driven recent\\nadvances in natural language processing [Devlin et al., 2019], computer vision [Dosovitskiy et al., 2021],\\nand spatio-temporal modelling [Bi et al., 2022]. There are many introductions to transformers, but most\\ndo not contain precise mathematical descriptions of the architecture and the intuitions behind the design\\nchoices are often also missing.1 Moreover, as research takes a winding path, the explanations for the\\ncomponents of the transformer can be idiosyncratic. In this note we aim for a mathematically precise,\\nintuitive, and clean description of the transformer architecture. We will not discuss training as this is\\nrather standard. We assume that the reader is familiar with fundamental topics in machine learning\\nincluding multi-layer perceptrons, linear transformations, softmax functions and basic probability.\\n1See Phuong and Hutter [2022] for an exception to this.\\narXiv:2304.10557v5  [cs.LG]  8 Feb 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-09T02:33:09+00:00', 'author': '', 'keywords': '', 'moddate': '2024-02-09T02:33:09+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Transformers.pdf', 'total_pages': 10, 'page': 1, 'page_label': '1', 'source_document': WindowsPath('../data/PDFs/Transformers.pdf'), 'file_type': 'pdf'}, page_content='Figure 1: The input to a transformer isN vectors\\nx(0)\\nn which are eachD dimensional. These can\\nbe collected together into an arrayX(0).\\n1 Strictly speaking, the collection of tokens does\\nnot need to have an order and the transformer\\ncan handle them as a set (where order does not\\nmatter), rather than a sequence. See section 3.\\n2 Note that much of the literature uses the trans-\\nposednotationwherebythedatamatrixis N×D,\\nbut I want sequences to run across the page and\\nfeatures down it in the schematics (a convention\\nI use in other lecture notes).\\nFigure 2: Encoding an image: an example [Doso-\\nvitskiy et al., 2021]. An image is split into N\\npatches. Each patch is reshaped into a vector by\\nthe vec operator. This vector is acted upon by a\\nmatrix W which maps the patch to aD dimen-\\nsional vector x(0)\\nn . These vectors are collected\\ntogether into the input X(0). The matrix W\\ncan be learned with the rest of the transformer’s\\nparameters.\\n3 The idea of interleaving processing across the\\nsequence and across features is a common motif\\nof many machine learning architectures includ-\\ning graph neural networks (interleaves processing\\nacross nodes and across features), Fourier neu-\\nral operators (interleaves processing across space\\nand across features), and bottleneck blocks in\\nResNets (interleaves processing across pixels and\\nacross features).\\n1 Preliminaries\\nLet’s start by talking about the form of the data that is input into a transformer,\\nthe goal of the transformer, and the form of its output.\\n1.1 Input data format: sets or sequences of tokens\\nIn order to apply a transformer, data must be converted into a set or sequence1\\nof N tokens x(0)\\nn of dimension D (see figure 1). The tokens can be collected\\ninto a matrixX(0) which isD×N.2 To give two concrete examples\\n1. a passage of text can be broken up into a sequence of words or sub-words,\\nwith each word being represented by a single unique vector,\\n2. an image can be broken up into a set of patches and each patch can be\\nmapped into a vector.\\nThe embeddings can be fixed or they can be learned with the rest of the pa-\\nrameters of the model e.g. the vectors representing words can be optimised or\\na learned linear transform can be used to embed image patches (see figure 2).\\nA sequence of tokens is a generic representation to use as an input – many\\ndifferent types of data can be “tokenised” and transformers are then immediately\\napplicable rather than requiring a bespoke architectures for each modality as\\nwas previously the case (CNNs for images, RNNs for sequences, deepsets for\\nsets etc.). Moreover, this means that you don’t need bespoke handcrafted\\narchitectures for mixing data of different modalities — you can just throw them\\nall into a big set of tokens.\\n1.2 Goal: representations of sequences\\nThe transformer will ingest the input dataX(0) and return a representation of\\nthe sequence in terms of another matrixX(M) which is also of sizeD×N.\\nThe slicexn = X(M)\\n:,n will be a vector of features representing the sequence at\\nthe location of tokenn. These representations can be used for auto-regressive\\nprediction of the next (n+1)th token, global classification of the entire sequence\\n(by pooling across the whole representation), sequence-to-sequence or image-\\nto-image prediction problems, etc. HereM denotes the number of layers in the\\ntransformer.\\n2 The transformer block\\nThe representation of the input sequence will be produced by iteratively applying\\na transformer block\\nX(m) = transformer-block(X(m−1)).\\nTheblockitselfcomprisestwostages: oneoperatingacrossthesequenceandone\\noperating across the features. The first stage refines each feature independently\\naccording to relationships between tokens across the sequence e.g. how much\\na word in a sequence at positionn depends on previous words at positionn′,\\nor how much two different patches from an image are related to one another.\\nThis stage acts horizontally across rows ofX(m−1). The second stage refines\\nthe features representing each token. This stage acts vertically across a column\\nof X(m−1). By repeatedly applying the transformer block the representation at\\ntoken nand featuredcan be shaped by information at tokenn′and featured′.3\\n1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-09T02:33:09+00:00', 'author': '', 'keywords': '', 'moddate': '2024-02-09T02:33:09+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Transformers.pdf', 'total_pages': 10, 'page': 2, 'page_label': '2', 'source_document': WindowsPath('../data/PDFs/Transformers.pdf'), 'file_type': 'pdf'}, page_content='4 Relationship to Convolutional Neural Net-\\nworks (CNNs). The attention mechanism can\\nrecover convolutional filtering as a special case\\ne.g. ifx(0)\\nn is a 1D regularly sampled time-series\\nand A(m)\\nn′,n = A(m)\\nn′−n then the attention mecha-\\nnism in eq. 1 becomes a convolution. Unlike nor-\\nmal CNNs, these filters have full temporal sup-\\nport. Later we will see that the filters themselves\\ndynamically depend on the input, another differ-\\nence from standard CNNs. We will also see a\\nsimilarity: transformers will use multiple atten-\\ntion maps in each layer in the same way that\\nCNNs use multiple filters (though typically trans-\\nformers have fewer attention maps than CNNs\\nhave channels).\\n5 The need for transformers to store and com-\\npute N ×N attention arrays can be a major com-\\nputational bottleneck, which makes processing of\\nlong sequences challenging.\\n6 When training transformers to perform auto-\\nregressive prediction, e.g. predicting the next\\nword in a sequence based on the previous ones, a\\nclever modification to the model can be used to\\naccelerate training and inference. This involves\\napplying the transformer to the whole sequence,\\nand using masking in the attention mechanism\\n(A(m) becomes an upper triangular matrix) to\\npreventfuturetokensaffectingtherepresentation\\nat earlier tokens. Causal predictions can then be\\nmade for the entire sequence in one forward pass\\nthrough the transformer. See section 4 for more\\ninformation.\\n7 We temporarily suppress the superscripts here\\nto ease the notation so A(m)\\nn,n′ becomes An,n′\\nand similarlyx(m)\\nn becomes xn.\\n2.1 Stage 1: self-attention across the sequence\\nThe output of the first stage of the transformer block is anotherD×N array,\\nY(m). The output is produced by aggregating information across the sequence\\nindependently for each feature using an operation calledattention.\\nAttention. Specifically, the output vector at locationn, denotedy(m)\\nn , is pro-\\nduced by a simple weighted average of the input features at locationn′ =\\n1 ...N , denotedx(m−1)\\nn′ , that is4\\ny(m)\\nn =\\nN∑\\nn′=1\\nx(m−1)\\nn′ A(m)\\nn′,n. (1)\\nHere the weighting is given by a so-calledattention matrix A(m)\\nn′,n which is of\\nsize5 N ×N and normalises over its columns∑N\\nn′=1 A(m)\\nn′,n = 1 . Intuitively\\nspeaking A(m)\\nn′,n will take a high value for locations in the sequencen′which are\\nof high relevance for locationn. For irrelevant locations, it will take the value\\n0. For example, all patches of a visual scene coming from a single object might\\nhave high corresponding attention values.\\nWe can compactly write the relationship as a matrix multiplication,\\nY(m) = X(m−1)A(m), (2)\\nand we illustrate it below in figure 3.6\\nFigure 3: The output of an element of the attention mechanism, Y(m)\\nd,n , is\\nproduced by the dot product of the input horizontally sliced through timeX(m)\\nd,:\\nwith a vertical slice from the attention matrixA(m)\\n:,n . Here the shading in the\\nattention matrix represent the elements with a high value in white and those\\nwith a low value, near to 0, in black.\\nSelf-attention. So far, so simple. But where does the attention matrix come\\nfrom? The neat idea in the first stage of the transformer is that the attention\\nmatrix is generated from the input sequence itself – so-calledself-attention.\\nA simple way of generating the attention matrix from the input would be to\\nmeasure the similarity between two locations by the dot product between the\\nfeatures at those two locations and then use a softmax function to handle the\\nnormalisation i.e.7\\nAn,n′ = exp(x⊤\\nnxn′ )∑N\\nn′′=1 exp(x⊤\\nn′′ xn′ )\\n.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-09T02:33:09+00:00', 'author': '', 'keywords': '', 'moddate': '2024-02-09T02:33:09+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Transformers.pdf', 'total_pages': 10, 'page': 3, 'page_label': '3', 'source_document': WindowsPath('../data/PDFs/Transformers.pdf'), 'file_type': 'pdf'}, page_content='8 Often you will see attention parameterised as\\nAn,n′ = exp(x⊤\\nn U⊤Uxn′ /\\n√\\nD)\\n∑N\\nn′′=1 exp(x⊤\\nn′′ U⊤Uxn′ /\\n√\\nD)\\n.\\nDividing the exponents by the square-root of the\\ndimensionality of the projected vector helps nu-\\nmerical stability, but in this presentation we ab-\\nsorb this term intoU to improve clarity.\\n9 Some of this effect could be handled by the\\nnormalisation in the denominator, but asymmet-\\nric similarity allows more flexibility. However, I\\ndo not know of experimental evidence to support\\nusing Uq ̸= Uk.\\n10 Relationship to Recurrent Neural Net-\\nworks (RNNs). It is illuminating to compare\\nthe temporal processing in the transformer to\\nthat of RNNs which recursively update a hid-\\nden state feature representation ( x(1)\\nn ) based\\non the current observation (x(0)\\nn ) and the pre-\\nvious hidden state x(1)\\nn = f(x(1)\\nn−1; x(0)\\nn ) =\\nf(f(x(1)\\nn−2; x(0)\\nn−1); x(0)\\nn ). Here we’ve unrolled\\nthe RNN one step to show that observations\\nwhich are nearby to the hidden state (e.g.x(0)\\nn )\\nare treated differently from observations that\\nare further away (e.g.x(0)\\nn−1), as information is\\npropagated by recurrent application of the func-\\ntion f(·). In contrast, in the transformer, self-\\nattentiontreatsallobservationsatalltime-points\\nin an identical manner, no matter how far away\\nthey are. This is one reason why they find it\\nsimpler to learn long-range relationships.\\n11 If attention matrices are viewed as a data-\\ndriven version of filters in a CNN, then the need\\nfor more filters / channels is clear. Typical\\nchoices for the number of headsH is 8 or 16,\\nlowerthantypicalnumbersofchannelsinaCNN.\\n12 The computational cost of multi-head self-\\nattentionisusuallydominatedbythematrixmul-\\ntiplication involving the attention matrix and is\\ntherefore O(HDN 2).\\n13 The product of the matrices V (m)\\nh X(m−1)\\nis related to the so-calledvalues which are nor-\\nmally introduced in descriptions of self-attention\\nalong side queries and keys. In the usual presen-\\ntation, there is a redundancy between the linear\\ntransformusedtocomputethevaluesandthelin-\\near projection at the end of the multi-head self-\\nattention, so we have not explicitly introduced\\nthem here. The standard presentation can be re-\\ncovered by setting Vh to be a low-rank matrix\\nVh = UhUv,h where Uh is DxK and Uv,h is\\nKxD. Typically K is set toK = D/H so that\\nchanging the number of heads leads to models\\nwith similar numbers of parameters and compu-\\ntational demands.\\nHowever, this naïve approach entangles information about the similarity between\\nlocations in the sequence with the content of the sequence itself.\\nAn alternative is to perform the same operation on a linear transformation of\\nthe sequence,Uxn, so that8\\nAn,n′ = exp(x⊤\\nnU⊤Uxn′ )∑N\\nn′′=1 exp(x⊤\\nn′′ U⊤Uxn′ )\\nTypically,U will project to a lower dimensional space i.e.U isK×Ddimensional\\nwith K <D. In this way only some of the features in the input sequence need\\nbe used to compute the similarity, the others being projected out, thereby de-\\ncoupling the attention computation from the content. However, the numerator\\nin this construction is symmetric. This could be a disadvantage. For example,\\nwe might want the word ‘caulking iron’ to be strongly associated with the word\\n‘tool’ (as it is a type of tool), but have the word ‘tool’ more weakly associated\\nwith the word ‘caulking iron’ (because most of us rarely encounter it).9\\nFortunately, itissimpletogeneralisetheattentionmechanismabovetobeasym-\\nmetric by applying two different linear transformations to the original sequence,\\nAn,n′ = exp\\n(\\nx⊤\\nnU⊤\\nk Uqxn′\\n)\\n∑N\\nn′′=1 exp\\n(\\nx⊤\\nn′′ U⊤\\nk Uqxn′\\n). (3)\\nThe two quantities that are dot-producted together hereqn = Uqxn and kn =\\nUkxn are typically known as thequeries and thekeys, respectively.\\nTogether equations 2 and 3 define the self-attention mechanism. Notice that\\nthe K×D matrices Uq and Uk are the only parameters of this mechanism.10\\nMulti-head self-attention (MHSA). In the self-attention mechanisms de-\\nscribed above, there is one attention matrix which describes the similarity of\\ntwo locations within the sequence. This can act as a bottleneck in the architec-\\nture – it would be useful for pairs of points to be similar in some ‘dimensions’\\nand different in others.11\\nIn order to increase capacity of the first self-attention stage, the transformer\\nblock applies H sets of self-attention in parallel12 (termed H heads) and then\\nlinearly projects the results down to theD×N array required for further pro-\\ncessing. This slight generalisation is calledmulti-head self-attention.\\nY(m) = MHSAθ(X(m−1)) =\\nH∑\\nh=1\\nV(m)\\nh X(m−1)A(m)\\nh , where (4)\\n[A(m)\\nh ]n,n′ =\\nexp\\n((\\nk(m)\\nh,n\\n)⊤\\nq(m)\\nh,n′\\n)\\n∑N\\nn′′=1 exp\\n((\\nk(m)\\nh,n′′\\n)⊤\\nq(m)\\nh,n′\\n) (5)\\nq(m)\\nh,n = U(m)\\nq,h x(m−1)\\nn and k(m)\\nh,n = U(m)\\nk,h x(m−1)\\nn . (6)\\nHere theH matrices V(m)\\nh which areD×Dproject theH self-attention stages\\ndown to the required output dimensionalityD.13\\nThe addition of the matricesV(m)\\nh , and the fact that retaining just the diagonal\\nelements of the attention matrixA(m) will interact the signal instantaneously\\nwith itself, does mean there is some cross-feature processing in multi-head self-\\nattention, as opposed to it containing purely cross-sequence processing. How-\\never, the stage has limited capacity for this type of processing and it is the job\\nof the second stage to address this.\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-09T02:33:09+00:00', 'author': '', 'keywords': '', 'moddate': '2024-02-09T02:33:09+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Transformers.pdf', 'total_pages': 10, 'page': 4, 'page_label': '4', 'source_document': WindowsPath('../data/PDFs/Transformers.pdf'), 'file_type': 'pdf'}, page_content='14 The MLPs used typically have one or two\\nhidden-layers with dimension equal to the num-\\nber of featuresD (or larger). The computational\\ncostofthisstepisthereforeroughly N×D×D. If\\nthefeatureembeddingsizeapproachesthelength\\nof the sequenceD ≈N, the MLPs can start to\\ndominatethecomputationalcomplexity(e.g.this\\ncan be the case for vision transformers which em-\\nbed large patches).\\n15 Relationship to Graph Neural Networks\\n(GNNs). At a high level, graph neural networks\\ninterleave two steps. First, a message passing\\nstep where each node receives messages from its\\nneighbours which are then aggregated together.\\nSecond, a feature processing step where the in-\\ncoming aggregated messages are used to update\\neach node’s features. Through this lens, the\\ntransformer can be viewed as an unrolled GNN\\nwith each token corresponding to an edge of a\\nfully connected graph. MHSA forms the mes-\\nsage passing step, and the MLPs forming the\\nfeature update step. Each transformer block cor-\\nresponds to one update of the GNN. Moreover,\\nmany methods for scaling transformers introduce\\nsparse forms of attention where each token at-\\ntends to only a restricted set of other tokens,\\nthat is they specify a sparse graph connectivity\\nstructure. Arguably, in this way transformers are\\nmore general as they can use different graphs at\\ndifferent layers in the transformer.\\n16 This is also known as z-scoring in some fields\\nand is related to whitening.\\nFigure 4 shows multi-head self-attention schematically. Multi-head attention\\ncomprises the following parametersθ = {Uq,h,Uk,h,Vh}H\\nh=1 i.e. 3H matrices\\nof sizeK×D, K×D, andD×D respectively.\\nFigure 4:Multi-head self-attention appliesH self-\\nattention operations in parallel and then linearly\\nprojects theHD ×N dimensional output down to\\nD ×N by applying a linear transform, implemented\\nhere by theH matrices Vh.\\n2.2 Stage 2: multi-layer perceptron across features\\nThe second stage of processing in the transformer block operates across features,\\nrefining the representation using a non-linear transform. To do this, we simply\\napply a multi-layer perceptron (MLP) to the vector of features at each location\\nn in the sequence,\\nx(m)\\nn = MLPθ(y(m)\\nn ).\\nNotice that the parameters of the MLP,θ, are the same for each locationn.14\\n15\\n2.3 The transformer block: Putting it all together with residual con-\\nnections and layer normalisation\\nWe can now stack MHSA and MLP layers to produce the transformer block.\\nRather than doing this directly, we make use of two ubiquitous transformations\\nto produce a more stable model that trains more easily: residual connections\\nand normalisation.\\nResidual connections. The use of residual connections is widespread across\\nmachinelearningastheymakeinitialisationsimple, haveasensibleinductivebias\\ntowards simple functions, and stabilise learning [Szegedy et al., 2017]. Instead\\nof directly specifying a functionx(m) = fθ(x(m−1)), the idea is to parameterise\\nit in terms of an identity mapping and a residual term\\nx(m) = x(m−1) + resθ(x(m−1)).\\nEquivalently, this can be viewed as modelling the differences between the repre-\\nsentation x(m) −x(m−1) = resθ(x(m−1)) and will work well when the function\\nthat is being modelled is close to identity. This type of parameterisation is used\\nfor both the MHSA and MLP stages in the transformer, with the idea that each\\napplies a mild non-linear transformation to the representation. Over many layers,\\nthese mild non-linear transformations compose to form large transformations.\\nToken normalisation.The use of normalisation, such as LayerNorm and Batch-\\nNorm, is also widespread across the deep learning community as a means to\\nstabilise learning. There are many potential choices for how to compute nor-\\nmalisation statistics (see figure 5 for a discussion), but the standard approach\\nis use LayerNorm [Ba et al., 2016] which normalises each token separately, re-\\nmoving the mean and dividing by the standard deviation,16\\n¯xd,n = 1√\\nvar(xn)\\n(xd,n −mean(xn)) γd + βd = LayerNorm(X)d,n\\nwhere mean(xn) = 1\\nD\\n∑D\\nd=1 xd,n and var(xn) = 1\\nD\\n∑D\\nd=1(xd,n −mean(xn))2.\\nThe two parametersγd and βd are a learned scale and shift.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-09T02:33:09+00:00', 'author': '', 'keywords': '', 'moddate': '2024-02-09T02:33:09+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Transformers.pdf', 'total_pages': 10, 'page': 5, 'page_label': '5', 'source_document': WindowsPath('../data/PDFs/Transformers.pdf'), 'file_type': 'pdf'}, page_content='batch\\nfeature\\nsequenced=D\\nd=1\\nn=Nn=1 batch\\nfeature\\nsequenced=D\\nd=1\\nn=Nn=1\\nLayerNorm for transformers(TokenNorm) BatchNorm for transformers\\nFigure 5: Transformers perform layer normali-\\nsation (left hand schematic) which normalises the\\nmean and standard deviation of each individual to-\\nken in each sequence in the batch. Batch normal-\\nisation (right hand schematic), which normalises\\nover the featureand batch dimension together, is\\nfound to be far less stable [Shen et al., 2020].\\nOther flavours of normalisation are possible and po-\\ntentially under-explored e.g. instance normalisation\\nwould normalise across the sequence dimension in-\\nstead.\\n17 Whilst it is possible to control the non-\\nlinearities and weights in neural networks to pre-\\nvent explosion of the representation, the con-\\nstraints this places on the activation functions\\ncan adversely affect learning. The LayerNorm\\napproach is arguably simpler and simpler to train.\\nbatch\\nfeature\\nimage height\\nand width\\nd=D\\nd=1\\nbatch\\nfeature\\nsequence\\nd=D\\nd=1\\nn=Nn=1\\nLayerNorm for CNNs BatchNorm for CNNs\\nFigure 6: In CNNs LayerNorm is conventionally\\napplied to both the featuresand across the fea-\\nture maps (i.e. across the height and width of the\\nimages) (left hand schematic). As the height and\\nwidth dimension in CNNs corresponds to the se-\\nquence dimension, 1 . . . Nof transformers, the\\nterm ’LayerNorm’ is arguably used inconsistently\\n(compare to figure 5). I would prefer to call the\\nnormalisation used in transformers ’token nor-\\nmalisation’ instead to avoid confusion. Batch\\nnormalisation (right hand schematic) is consis-\\ntently defined.\\n18 The exact configuration of the normalisation\\nand residual layers can differ, but here we show\\na standard setup [Xiong et al., 2020].\\nAs this transform normalises each token individually and as LayerNorm is ap-\\nplied differently in CNNs, see figure 6, I would prefer to call this normalisation\\nTokenNorm.\\nThis transform stops feature representations blowing up in magnitude as non-\\nlinearities are repeatedly applied through neural networks.17 In transformers,\\nLayerNorm is usually applied in the residual terms of both the MHSA and MLP\\nstages.\\nPuttingthisalltogether, wehavethestandardtransformerblockshownschemat-\\nically in figure 7.18\\nFigure 7: The transformer block. Residual connections are added to the multi-\\nhead self-attention (MHSA) stage and the multi-layer perceptron (MLP) stage.\\nLayer normalisation is also applied to the inputs of both the MHSA and the\\nMLP. They are then stacked. This block can then be repeatedM times.\\n3 Position encoding\\nThe transformer treats the data as a set — if you permute the columns ofX(0)\\n(i.e. re-order the tokens in the input sequence) you permute all the represen-\\ntations throughout the networkX(m) in the same way. This is key for many\\napplications since there may not be a natural way to order the original data into\\na sequence of tokens. For example, there is no single ‘correct’ order to map\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-09T02:33:09+00:00', 'author': '', 'keywords': '', 'moddate': '2024-02-09T02:33:09+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Transformers.pdf', 'total_pages': 10, 'page': 6, 'page_label': '6', 'source_document': WindowsPath('../data/PDFs/Transformers.pdf'), 'file_type': 'pdf'}, page_content='19 Vision transformers [Dosovitskiy et al., 2021]\\nuse x(0)\\nn = Wpn + en where pn is the nth\\nvectorised patch,en is the learned position em-\\nbedding, andW is the patch embedding matrix.\\nArguably it would be more intuitive to append\\nthe position embedding to the patch embedding.\\nHowever, if we use the concatenation approach\\nandconsiderwhathappensafterapplyingalinear\\ntransform,\\nV\\n[Wpn\\nen\\n]\\n=\\n[V11 V12\\nV21 V22\\n][Wpn\\nen\\n]\\n=\\n[V11Wpn + V12en\\nV21Wpn + V22en\\n]\\n= W′pn + e′\\nn\\nwe recover the additive construction, which is\\none hint as to why the additive construction\\nworks.\\n20 Note that I’m overloading the notation here:\\npreviously superscripts denoted layers in the\\ntransformer, but here I’m using them to denote\\nthe number of items in the input sequence.\\nimage patches into a one dimensional sequence.\\nHowever, this presents a problem since positional information is key in many\\nproblems and the transformer has thrown it out. The sequence ‘herbivores\\neat plants’ should not have the same representation (up to permutation) as\\n‘plants eat herbivores’. Nor should an image have the same representation as\\none comprising the same patches randomly permuted. Thankfully, there is a\\nsimple fix for this: the location of each token within the original dataset should\\nbe included in the token itself, or through the way it is processed. There are\\nseveral options how to do this, one is to include this information directly into the\\nembedding X(0). E.g.bysimplyaddingthepositionembedding(surprisinglythis\\nworks19) or concatenating. The position information can be fixed e.g. adding a\\nvector of sinusoids of different frequencies and phases to encode position of a\\nword in a sentence [Vaswani et al., 2017], or it can be a free parameter which is\\nlearned [Devlin et al., 2019], as it often done in image transformers. There are\\nalso approaches to include relative distance information between pairs of tokens\\nby modifying the self-attention mechanism [Wu et al., 2021] which connects to\\nequivariant transformers.\\n4 Application specific transformer variants\\nFor completeness we will give some simple examples for how the standard trans-\\nformer architecture above is used and modified for specific applications. This\\nincludes adding a head to the transformer blocks to carry out the desired pre-\\ndiction task, but also modifications to the standard construction of the body.\\n4.1 Auto-regressive language modelling\\nIn auto-regressive language modelling the goal is to predict the next wordwn\\nin the sequence given the previous wordsw1:n−1, that is to return p(wn =\\nw|w1:n−1). Two modifications are required to use the transformer for this task\\n— a change to the body to make the architecture efficient and the addition of\\na head to make the predictions for the next word.\\nModification to the body: auto-regressive masking.Applying the version\\nof the transformer we have covered so far to auto-regressive prediction is compu-\\ntationally expensive, both during training and testing. To see this, note that AR\\nprediction requires making a sequence of predictions: you start by predicting the\\nfirst wordp(w1 = w), then you predict the second given the firstp(w2 = w|w1),\\nthen the third word given the first twop(w2 = w|w1,w2), and so on until you\\npredict the last item in the sequencep(wN = w|w1:N−1). This requires apply-\\ning the transformerN −1 times with input sequences that grow by one word\\neach time: w1,w1:2,...,w 1:N−1. This is very costly at both training-time and\\ntest-time.\\nFortunately, there is a neat way around this by enabling the transformer to\\nsupport incremental updates whereby if you add a new token to an existing\\nsequence, you do not change the representation for the old tokens. To make this\\nproperty clear, I will define it mathematically: let the output of the incremental\\ntransformer applied to the firstn words be denoted20\\nX(n) = transformer-incremental(w1:n).\\nThen the output of the incremental transformer when applied ton+ 1 words is\\nX(n+1) = transformer-incremental(w1:n+1).\\nIn the incremental transformerX(n) = X(n+1)\\n1:D,1:n i.e. the representation of the\\nold tokens has not changed by adding the new one. If we have this property\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-09T02:33:09+00:00', 'author': '', 'keywords': '', 'moddate': '2024-02-09T02:33:09+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Transformers.pdf', 'total_pages': 10, 'page': 7, 'page_label': '7', 'source_document': WindowsPath('../data/PDFs/Transformers.pdf'), 'file_type': 'pdf'}, page_content='21 Notice that this masking operation also en-\\ncodes position information since you can infer\\nthe order of the tokens from the mask.\\n22 This restriction to the attention will cause a\\nloss of representational power. It’s an open ques-\\ntion as to how significant this is and whether in-\\ncreasing the capacity of the model can mitigate\\nit e.g. by using higher dimensional tokens, i.e. in-\\ncreasing D.\\nthen 1. at test-time auto-regressive generation can use incremental updates to\\ncompute the new representation efficiently, 2. at training time we can make\\nthe N auto-regressive predictions for the whole sequencep(w1 = w)p(w2 =\\nw|w1)p(w2 = w|w1,w2) ...p (wN = w|w1:N−1) in a single forwards pass.\\nUnfortunately, the standard transformer introduced above does not have this\\nproperty due to the form of the attention used. Every token attends to every\\nother token, so if we add a new token to the sequence then the representation\\nfor every token changes throughout the transformer. However, if we mask the\\nattention matrix so that it is upper-triangularAn,n′ = 0 when n > n′ then\\nthe representation of each wordonly depends on the previous words.21 This\\nthen gives us the incremental property as none of the other operations in the\\ntransformer operate across the sequence.22\\nAdding a head. We’re now almost set to perform auto-regressive language\\nmodelling. We apply the masked transformer blockM times to the input se-\\nquence of words. We then take the representation at tokenn−1, that isx(M)\\nn−1\\nwhich captures causal information in the sequence at this point, and generate\\nthe probability of the next word through a softmax operation\\np(wn = w|w1:n−1) = p(wn = w|x(M)\\nn−1) = exp(g⊤\\nwx(M)\\nn−1)\\n∑W\\nw=1 exp(g⊤wx(M)\\nn−1)\\n.\\nHere W is the vocabulary size, the wth word isw and {gw}W\\nw=1 are softmax\\nweights that will be learned.\\n4.2 Image classification\\nFor image classification the goal is to predict the labely given the input image\\nwhich has been tokenised into the sequenceX(0), that isp(y|X(0)). One way\\nof computing this distribution would be to apply the standard transformer body\\nM times to the tokenised image patches before aggregating the final layer of the\\ntransformer, X(M), across the sequence e.g. by spatial poolingh = ∑N\\nn=1 x(M)\\nn\\nin order to form a feature representation for the entire image. The representation\\nh could then be used to perform softmax classification. An alternative approach\\nis found to perform better [Dosovitskiy et al., 2021]. Instead we introduce a\\nnew fixed (learned) token at the startn = 0 of the input sequencex(0)\\n0 . At\\nthe head we use then= 0 vector, x(M)\\n0 , to perform the softmax classification.\\nThis approach has the advantage that the transformer maintains and refines a\\nglobal representation of the sequence at each layermof the transformer that is\\nappropriate for classification.\\n4.3 More complex uses\\nThe transformer block can also be used as part of more complicated systems\\ne.g. in encoder-decoder architectures for sequence-to-sequence modelling for\\ntranslation [Devlin et al., 2019, Vaswani et al., 2017] or in masked auto-encoders\\nfor self-supervised vision systems [He et al., 2021].\\n5 Conclusion\\nThis concludes this basic introduction to transformers which aspired to be math-\\nematically precise and to provide intuitions behind the design decisions.\\nWe have not talked about loss functions or training in any detail, but this is\\nbecause rather standard deep learning approaches are used for these. Briefly,\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-09T02:33:09+00:00', 'author': '', 'keywords': '', 'moddate': '2024-02-09T02:33:09+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Transformers.pdf', 'total_pages': 10, 'page': 8, 'page_label': '8', 'source_document': WindowsPath('../data/PDFs/Transformers.pdf'), 'file_type': 'pdf'}, page_content='transformers are typically trained using the Adam optimiser. They are often\\nslow to train compared to other architectures and typically get more unstable\\nas training progresses. Gradient clipping, decaying learning rate schedules, and\\nincreasing batch sizes through training help to mitigate these instabilities, but\\noften they still persist.\\nAcknowledgements. We thank Dr. Max Patacchiola, Sasha Shysheya, John\\nBronskill, Runa Eschenhagen and Jess Riedel for feedback on previous versions\\nof this note. Richard E. Turner is supported by Microsoft, Google, Amazon,\\nARM, Improbable and EPSRC grant EP/T005386/1.\\nReferences\\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization.\\narXiv preprint arXiv:1607.06450, 2016.\\nKaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian.\\nPangu-weather: A 3d high-resolution model for fast and accurate global\\nweather forecast, 2022.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT:\\nPre-training of deep bidirectional transformers for language understanding.\\nIn Proceedings of the 2019 Conference of the North American Chapter\\nof the Association for Computational Linguistics: Human Language Tech-\\nnologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapo-\\nlis, Minnesota, June 2019. Association for Computational Linguistics. doi:\\n10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423.\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\\naohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\\nGeorg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An im-\\nage is worth 16x16 words: Transformers for image recognition at scale. In\\n9th International Conference on Learning Representations, ICLR 2021, Vir-\\ntual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https:\\n//openreview.net/forum?id=YicbFdNTTy.\\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Gir-\\nshick. Masked autoencoders are scalable vision learners, 2021.\\nMary Phuong and Marcus Hutter. Formal algorithms for transformers.arXiv\\npreprint arXiv:2207.09238, 2022.\\nSheng Shen, Zhewei Yao, Amir Gholami, Michael Mahoney, and Kurt Keutzer.\\nPowerNorm: Rethinking batch normalization in transformers. In Hal Daumé\\nIII and Aarti Singh, editors, Proceedings of the 37th International Confer-\\nence on Machine Learning, volume 119 of Proceedings of Machine Learn-\\ning Research, pages 8741–8751. PMLR, 13–18 Jul 2020. URL https:\\n//proceedings.mlr.press/v119/shen20e.html.\\nChristian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander Alemi.\\nInception-v4, inception-resnetandtheimpactofresidualconnectionsonlearn-\\ning. In Proceedings of the AAAI conference on artificial intelligence, vol-\\nume 31, 2017.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\\nAidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all\\nyou need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach,\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-09T02:33:09+00:00', 'author': '', 'keywords': '', 'moddate': '2024-02-09T02:33:09+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Transformers.pdf', 'total_pages': 10, 'page': 9, 'page_label': '9', 'source_document': WindowsPath('../data/PDFs/Transformers.pdf'), 'file_type': 'pdf'}, page_content='R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neu-\\nral Information Processing Systems, volume 30. Curran Associates, Inc.,\\n2017. URL https://proceedings.neurips.cc/paper_files/paper/\\n2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\\nK. Wu, H. Peng, M. Chen, J. Fu, and H. Chao. Rethinking and improv-\\ning relative position encoding for vision transformer. In 2021 IEEE/CVF\\nInternational Conference on Computer Vision (ICCV), pages 10013–10021,\\nLos Alamitos, CA, USA, oct 2021. IEEE Computer Society. doi:10.1109/\\nICCV48922.2021.00988. URL https://doi.ieeecomputersociety.org/\\n10.1109/ICCV48922.2021.00988.\\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing,\\nHuishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normal-\\nization in the transformer architecture. In Hal Daumé III and Aarti Singh,\\neditors, Proceedings of the 37th International Conference on Machine Learn-\\ning, volume 119 ofProceedings of Machine Learning Research, pages 10524–\\n10533. PMLR, 13–18 Jul 2020. URLhttps://proceedings.mlr.press/\\nv119/xiong20b.html.\\n9')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97604962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents, chunk_size = 1000, chunk_overlap = 200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function = len,\n",
    "        separators=[\"\\n\\n\",\"\\n\",\" \",\"\"]\n",
    "        )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} no. of documents into {len(split_docs)} chunks\")\n",
    "\n",
    "    if split_docs:\n",
    "        print(f\"Sample chunk document: {split_docs[0].page_content[:200]}\")\n",
    "        print(f\"Metadata {split_docs[0].metadata}\")\n",
    "\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5374bb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 102 no. of documents into 251 chunks\n",
      "Sample chunk document: MIT Class 6.S184:Generative AI With Stochastic Differential Equations, 2025\n",
      "An Introduction to Flow Matching and Diffusion Models\n",
      "Peter Holderrieth and Ezra Erives\n",
      "Website: https://diffusion.csail.mit\n",
      "Metadata {'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-03T02:14:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-03T02:14:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\PDFs\\\\Diffusion Model.pdf', 'total_pages': 56, 'page': 0, 'page_label': '1', 'source_document': WindowsPath('../data/PDFs/Diffusion Model.pdf'), 'file_type': 'pdf'}\n"
     ]
    }
   ],
   "source": [
    "chunks = split_documents(all_pdf_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b059890b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model all-MiniLM-L6-v2\n",
      " Successfully loaded the model . Embedding dimension 384\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using sentence transformer\"\"\"\n",
    "\n",
    "    def __init__(self, model_name : str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"Initialize the embedding manager\"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the Sentence Transformer Model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading the model {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\" Successfully loaded the model . Embedding dimension {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading the model {self.model_name} : {e}\")\n",
    "\n",
    "    def generate_embeddings(self, texts:List[str]) -> np.ndarray:\n",
    "        \"\"\"Generates the embeddings for a list of texts\"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        else:\n",
    "            embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "            print(f\"Generated Embeddings with the sahpe {embeddings.shape}\")\n",
    "            return embeddings\n",
    "        \n",
    "## Initialize the embedding manager    \n",
    "embedding_manager=EmbeddingManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02bc279e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error initializing Vector Store {e}\n"
     ]
    }
   ],
   "source": [
    "class VectorStore:\n",
    "\n",
    "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
    "\n",
    "    def __init__(self, collection_name: str = \"PDF Store\", persist_dir:str = \"../data/vector_store\" ):\n",
    "        \"\"\"Initialize the Vector Store\"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_dir = persist_dir\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self.initialize_store()\n",
    "\n",
    "    def initialize_store(self):\n",
    "        \"Initialize the chroma db client and collection\"\n",
    "        try:\n",
    "            os.makedirs(self.persist_dir, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_dir)\n",
    "            self.collection = self.client.get_or_create_collection(name=self.collection_name,\n",
    "            metadata={\"description\": \"PDF document embeddings for RAG\"}) \n",
    "\n",
    "            print(f\"Vector Store initialized successfully {self.collection_name}\")\n",
    "            print(f\"Number of documents in collection: {self.collection.count}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error initializing Vector Store {e}\")\n",
    "\n",
    "    def add_documents(self, documents: list[Any], embeddings:np.ndarray):\n",
    "        \"\"\"Add documents and their embeddings to the vector store\"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i, (doc,embedding) in enumerate (zip(documents,embeddings)):\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # prepare meatadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_lengh'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embeddings\n",
    "            embeddings_list.append(embeddings.tolist())\n",
    "\n",
    "        # Add to collection\n",
    "\n",
    "        try:\n",
    "            self.collection.add(\n",
    "             ids =  ids,\n",
    "             metadatas=metadatas,\n",
    "             documents=documents_text,\n",
    "             embeddings=embeddings_list   \n",
    "            )\n",
    "\n",
    "            print(f\"Added documents to vector store {self.collection_name}\")\n",
    "            print(f\"Number of documents added to the store {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to the store {e}\")\n",
    "            \n",
    "VectorStore = VectorStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ebc5d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 8/8 [00:10<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Embeddings with the sahpe (251, 384)\n",
      "Error adding documents to the store 'NoneType' object has no attribute 'add'\n"
     ]
    }
   ],
   "source": [
    "## Generate the embeddings\n",
    "texts = [doc.page_content for doc in chunks]\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "## store the embeddings in Vector Store\n",
    "VectorStore.add_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd93893",
   "metadata": {},
   "source": [
    "Retriever Pipeline from VectoreStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8fc6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "\n",
    "     \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "\n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        self.vector_store = vectore_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(query: str, top_k:int = 5, threshold_score:float = 0.0) -> List[Dict[str, Any]]:\n",
    "       print(f\"Retrieving documents for {query}\")\n",
    "       print(f\"Score Threshold is: {threshold_score}\")\n",
    "    \n",
    "        # Generate query embedding\n",
    "       query_embedding = embedding_manager.generate_embeddings([query])[0]\n",
    "\n",
    "       # retrieve from VectorStore\n",
    "       try:\n",
    "        results = vector_store.collection.query(\n",
    "                querry_embeddings = [query_embedding.tolist()],\n",
    "                n_results =  top_k)\n",
    "        \n",
    "        retrieved_docs = []\n",
    "\n",
    "        if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "\n",
    "                for (i, (doc_id, document, metadata, distance)) in enumerate(zip(ids, documents, metadatas,distances)):\n",
    "                        similarity_score = 1 - distance\n",
    "\n",
    "                        if similarity_score > = threshold_score:\n",
    "                            retrieved_docs.append({\n",
    "                                'Id': doc_id,\n",
    "                                \"content\": document,\n",
    "                                \"metadata\": metadata,\n",
    "                                \"distance\": distance,\n",
    "                                'similarity_Score': similarity_score \n",
    "                                'rank': i+1\n",
    "                            })  \n",
    "\n",
    "                    print (f\"Retrieved {len(retrieved_docs)} documents (after filtering)\") \n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "        \n",
    "            return retrieved_docs\n",
    "       except Exception as e:\n",
    "             print(f\" Error during retrieval: {e}\")\n",
    "             return []\n",
    "\n",
    "rag_retriever = RAGRetriever(VectorStore, embedding_manager) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acd92ce",
   "metadata": {},
   "source": [
    "RAG Pipeline - Vector DB to LLM Output Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e677414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b3159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroqLM:\n",
    "    def __init__(self, model_name: str = \"gemma2-9b-it\", api_key: None):\n",
    "        self.model_name = model_name\n",
    "        self.api_key = api_key\n",
    "\n",
    "        If not self.api_key:\n",
    "            raise ValueError(\"Groq API Key is not fund. Please initialize the API Key\")\n",
    "    \n",
    "        self.llm = ChatGroq(\n",
    "            groq_api_key = self.api_key,\n",
    "            model_name = self.model_name,\n",
    "            temparature = 0.1,\n",
    "            max_tokens = 1024\n",
    "        )\n",
    "\n",
    "        print(f\"Initialized Groq LLM with model: {self.model_name}\")\n",
    "\n",
    "    def generate_response(self, query:str , context:str, max_length:int = 500) -> str:\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables = [\"context\", \"Question\"]\n",
    "            template = \"\"\"You are a helpful AI assistant .  Use the following context to answer the question accurately and precisely\n",
    "                 Context: {context}\n",
    "            Question: {query}\n",
    "            Anser: Provide a clear and informative answer based on the context above. If the context doesn't contain enough information to answer the question, say so\"\"\" \n",
    "             )\n",
    "        formatted_prompt = prompt_template.format(context=context, question = query)\n",
    "        try:\n",
    "            # generate responses\n",
    "            messages = HumanMessage(content = formatted_prompt)\n",
    "            response = self.llm.invoke(messages)\n",
    "            return response.content\n",
    "        \n",
    "        except Exception as e:\n",
    "            return f\"Error generating message {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d97277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NetworkSecurity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
